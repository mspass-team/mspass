language: cpp

os: linux

dist: bionic

jobs:
  include:
    - name: "Docker Compose"
      language: shell
      services: 
        - docker
      before_install: 
        - pwd
      install:
        - docker build -t wangyinz/mspass .
      script:
        - docker-compose up -d
        - docker exec mspass-worker /usr/local/spark/bin/run-example --master spark://mspass-master:7077 SparkPi 10
      if: type = cron

    - name: "Ubuntu Bionic Default GCC"
      addons:
        apt:
          packages:
            - gfortran
      compiler: gcc
      if: type = cron
      
    - name: "Ubuntu Bionic Boost LAPACK GCC"
      addons:
        apt:
          packages:
            - gfortran
            - libboost-dev
            - libboost-serialization-dev
            - liblapack-dev
      compiler: gcc
      
    - name: "Ubuntu Bionic Default CLANG"
      addons:
        apt:
          packages:
            - gfortran
      compiler: clang
      if: type = cron
      
    - name: "Ubuntu Bionic Boost LAPACK CLANG"
      addons:
        apt:
          packages:
            - gfortran
            - libboost-dev
            - libboost-serialization-dev
            - liblapack-dev
      compiler: clang
      
    - name: "OSX XC9.4 Boost"
      os: osx
      osx_image: xcode9.4
      addons:
        homebrew:
          packages:
            - gcc
            - boost
          update: true
      if: type = cron
    
    - stage: deploy
      name: "Deploy to GitHub Pages"
      if: (NOT type IN (pull_request)) AND (branch = master) # only deploy if merging on master
      addons:
        apt:
          packages:
            - gfortran
            - libboost-dev
            - libboost-serialization-dev
            - liblapack-dev
            - doxygen
            - pandoc
      compiler: gcc
      before_install:
        - |
          pyenv versions
          pyenv global 3.7
        - pip install -U pip
        - pip install pymongo
        - pip install numpy 
        - pip install obspy
        - pip install -r docs/requirements.txt
      install:
        - pip3 install ./ -vvv
      script:
        - export MSPASS_HOME=$(pwd)
        - cd docs 
        - ls
        - make html
      deploy:
        provider: pages # deploy on github pages
        skip_cleanup: true
        token: "$token" # defined via the Travis interface
        local_dir: docs/build/html
        on:
          branch: master
    
    - stage: test
      name: "Test Python Code"
      addons:
        apt:
          packages:
            - gfortran
            - libboost-dev
            - libboost-serialization-dev
            - liblapack-dev
      compiler: gcc
      before_install:
        - |
          pyenv versions
          pyenv global 3.7
        - pip install -U pip
        - pip install pymongo
        - pip install numpy 
        - pip install obspy
        - pip install pytest
        - pip install mongomock
        - pip install pytest-cov
        - pip install codecov
        - pip install "dask[complete]"
        - mkdir -p /opt
        - wget -q -O /opt/spark.tgz https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz
        - tar xzf /opt/spark.tgz -C /opt/
        - rm /opt/spark.tgz
        - export SPARK_HOME=/opt/spark-3.0.0-bin-hadoop2.7
        - export PATH=$PATH:/opt/spark-3.0.0-bin-hadoop2.7/bin
        - pip install pyspark
        - pip install pytest-spark
      install:
        - python setup.py build --debug install
      script:
        - export MSPASS_HOME=$(pwd)
        - pytest --cov=mspasspy python/tests/
        - make test -C build/temp.*/
      # - gcov -b -n build/temp.*/python/CMakeFiles/ccore.dir/mspass_wrapper.gcno > /dev/null
      after_success:
        - codecov
        - bash <(curl -s https://codecov.io/bash)
          
before_install:
  - |
    if [ $TRAVIS_OS_NAME == "linux" ]; then
      pyenv versions
      pyenv global 3.7
    fi
  - cd cxx 
  - mkdir build 
  - cd build
install:
  - cmake ..
  - make -j 8
script:
  - ctest --output-on-failure
      

