

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>CRUD Operations in MsPASS &mdash; MsPASS 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Handling Errors" href="handling_errors.html" />
    <link rel="prev" title="Database Concepts" href="database_concepts.html" />  

  <style>
    .wy-nav-content { max-width: 1600px; }
  </style>

  
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> MsPASS
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
    
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../user_manual.html">User’s Manual</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_object_design_concepts.html">Data Object Design Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="time_standard_constraints.html">Time Standard Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="database_concepts.html">Database Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="database_concepts.html#id5">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="database_concepts.html#advanced-topics">Advanced Topics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CRUD Operations in MsPASS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create">Create</a></li>
<li class="toctree-l3"><a class="reference internal" href="#read">Read</a></li>
<li class="toctree-l3"><a class="reference internal" href="#update">Update</a></li>
<li class="toctree-l3"><a class="reference internal" href="#delete">Delete</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-io-concepts">Key IO Concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mspass-chemistry">MsPASS Chemistry</a></li>
<li class="toctree-l4"><a class="reference internal" href="#save-concepts">Save Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#read-concepts">Read concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#update-concepts">Update Concepts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="handling_errors.html">Handling Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="processing_history_concepts.html">Processing History Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="adapting_algorithms.html">Adapting an Existing Algorithm to MsPASS</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference_manual.html">Reference Manual</a></li>
</ul>

            
          
    <a href= "../genindex.html">Index</a>
  
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MsPASS</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../user_manual.html">User’s Manual</a> &raquo;</li>
        
      <li>CRUD Operations in MsPASS</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/user_manual/CRUD_operations.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="crud-operations-in-mspass">
<span id="crud-operations"></span><h1>CRUD Operations in MsPASS<a class="headerlink" href="#crud-operations-in-mspass" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The acronymn CRUD is often used as a mnemonic in books and online tutorials
teaching database concepts.  CRUD stands for Create-Read-Update-Delete.
It is usful because it summarizes the four main things any database system
must accomplish cleanly.  This section is organized into subsections on
each of the topics defined by CRUD.  At the end of this section we
summarize some common options in CRUD operations at the end of this section.</p>
<p>The most common database operations are defined as methods in a class
we give the obvious name Database.  Most MsPASS jobs need to the have following
incantation a the top of the python job script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="s1">&#39;database_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>where the second argument on Database, <code class="code docutils literal notranslate"><span class="pre">'database_name'</span></code>,
is a name you chose for the dataset you are working with.
For the remainder of this section we will use the symbol “db” as
defined, but as in any programming language you need to recognize the
symbol can be anything you as the user find sensible. “db” is just our
choice.</p>
<p>The default schema for a Database assumes the data set is a set of
TimeSeries object.  That implies the dataset is defined in the
collection we call <code class="code docutils literal notranslate"><span class="pre">wf_TimeSeries</span></code>.   If the dataset is already
assembled into three component bundles (<code class="code docutils literal notranslate"><span class="pre">Seismogram</span></code> objects)
the Database constructor needs to be informed of that through this
alternative construct:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="n">db</span><span class="o">=</span><span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="s1">&#39;database_name&#39;</span><span class="p">,</span> <span class="n">db_schema</span><span class="o">=</span><span class="s1">&#39;wf_Seismogram&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If your workflow requires reading both TimeSeries and Seismogram
data, you should create two handles with one using the wf_TimeSeries
and the other using the wf_Seismogram schema.</p>
</div>
<div class="section" id="create">
<h2>Create<a class="headerlink" href="#create" title="Permalink to this headline">¶</a></h2>
<p>We tag all methods of the Database class that do “Create” operations with
the synonymous word “save”.   Here we list all save methods with a brief
description of each method.  We refer to the docstring pages for detailed
(and most up to date) usage:</p>
<ol class="arabic">
<li><p><code class="code docutils literal notranslate"><span class="pre">save_data</span></code> is probably the most common method you will use.  The
first argument is one of the atomic objects defined in MsPASS
(Seismogram or TimeSeries) that you wish to save.  Options are
described in the docstring.  Here is an example usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example assumes filename contains one channel of data in</span>
<span class="c1"># miniseed format</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;testfile.msd&#39;</span>
<span class="n">dstrm</span> <span class="o">=</span> <span class="n">read</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;MSEED&#39;</span><span class="p">)</span>
<span class="c1"># A stream is a list of obspy Trace objects - get the first</span>
<span class="n">dtr</span> <span class="o">=</span> <span class="n">dstrm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># This mspass function converts a Trace to our native TimeSeries</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">Trace2TimeSeries</span><span class="p">(</span><span class="n">dtr</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_data</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>By default <code class="code docutils literal notranslate"><span class="pre">save_data</span></code> stores all Metadata except those linked to
normalized collections (<code class="code docutils literal notranslate"><span class="pre">source</span></code>, <code class="code docutils literal notranslate"><span class="pre">channel</span></code>, and <code class="code docutils literal notranslate"><span class="pre">site</span></code>) with no
safety checks.  We discuss the alternatives in a later section
so the reader can first get a broader perspective.</p>
</li>
<li><p><code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code>  is similar to <code class="code docutils literal notranslate"><span class="pre">save_data</span></code> except the first argument
is an Ensemble object.  There are currently two of them:  (1) TimeSeriesEnsemble
and (2) SeismogramEnsemble.   As discussed in detail elsewhere an Ensemble
is a generalization of the idea of a “gather” in seismic reflection processing.
The <code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code> method is a convenience class for saving Ensembles.
Ensembles are containers of atomic objects.  <code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code>
is mostly a loop over the container saving the atomic objects it contains
to the wf_TimeSeries (for TimeSeriesEnsembles) or wf_Seismogram
(for Seismogram objects).</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> should be viewed mostly as a convenience method to build
the <code class="code docutils literal notranslate"><span class="pre">source</span></code> collection from QUAKEML data downloaded from FDSN data
centers via obspy’s web services functions.   <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> can be
thought of as a converter that translates the contents of a QUAKEML
file or string for storage as a set of MongoDB documents in the <code class="code docutils literal notranslate"><span class="pre">source</span></code>
collection.  We use obspy’s <code class="code docutils literal notranslate"><span class="pre">Catalog</span></code> object as an intermediary to
avoid the need to write our own QUAKEML parser.   As with save_data
the easiest way to the usage would be this example derived from
our <em>getting_started</em> tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;IRIS&quot;</span><span class="p">)</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">UTCDateTime</span><span class="p">(</span><span class="s1">&#39;2011-03-11T05:46:24.0&#39;</span><span class="p">)</span>
<span class="n">starttime</span> <span class="o">=</span> <span class="n">t0</span><span class="o">-</span><span class="mf">3600.0</span>
<span class="n">endtime</span> <span class="o">=</span> <span class="n">t0</span><span class="o">+</span><span class="p">(</span><span class="mf">7.0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">24.0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">3600.0</span><span class="p">)</span>
<span class="n">lat0</span> <span class="o">=</span> <span class="mf">38.3</span>
<span class="n">lon0</span> <span class="o">=</span> <span class="mf">142.5</span>
<span class="n">minlat</span> <span class="o">=</span> <span class="n">lat0</span><span class="o">-</span><span class="mf">3.0</span>
<span class="n">maxlat</span> <span class="o">=</span> <span class="n">lat0</span><span class="o">+</span><span class="mf">3.0</span>
<span class="n">minlon</span> <span class="o">=</span> <span class="n">lon0</span><span class="o">-</span><span class="mf">3.0</span>
<span class="n">maxlon</span> <span class="o">=</span> <span class="n">lon0</span><span class="o">+</span><span class="mf">3.0</span>
<span class="n">minmag</span> <span class="o">=</span> <span class="mf">6.5</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_events</span><span class="p">(</span><span class="n">starttime</span><span class="o">=</span><span class="n">starttime</span><span class="p">,</span> <span class="n">endtime</span><span class="o">=</span><span class="n">endtime</span><span class="p">,</span>
                        <span class="n">minlatitude</span><span class="o">=</span><span class="n">minlat</span><span class="p">,</span> <span class="n">minlongitude</span><span class="o">=</span><span class="n">minlon</span><span class="p">,</span>
                        <span class="n">maxlatitude</span><span class="o">=</span><span class="n">maxlat</span><span class="p">,</span> <span class="n">maxlongitude</span><span class="o">=</span><span class="n">maxlon</span><span class="p">,</span>
                                <span class="n">minmagnitude</span><span class="o">=</span><span class="n">minmag</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_catalog</span><span class="p">(</span><span class="n">cat</span><span class="p">)</span>
</pre></div>
</div>
<p>This particular example pulls 11 large aftershocks of the 2011 Tohoku
Earthquake.</p>
</li>
<li><p><code class="code docutils literal notranslate"><span class="pre">save_inventory</span></code> is similar in concept to <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code>, but instead of
translating data for source information it translates information to
MsPASS for station metadata.  The station information problem is slightly
more complicated than the source problem because of an implementation
choice we made in MsPASS.   That is, because a primary goal of MsPASS
was to support three-component seismograms as a core data type, there
is a disconnect in what metadata is required to support a TimeSeries
versus a Seismogram object.   We handle this by defining two different,
but similar MongoDB collections:  <code class="code docutils literal notranslate"><span class="pre">channel</span></code> for TimeSeries data and
<code class="code docutils literal notranslate"><span class="pre">site</span></code> for Seismogram objects.  The name for this method contains the
keyword “inventory” because like <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> we use an obspy
python class as an intermediary.  The reasons is similar; obspy had
already solved the problem of downloading station metadata from
FDSN web services with their
<a class="reference external" href="https://docs.obspy.org/packages/obspy.core.inventory.html">read_inventory function</a>.
As with <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> <code class="code docutils literal notranslate"><span class="pre">save_inventory</span></code> can be thought of as a translator
from data downloaded with web services to the form needed in MsPASS.
It may be helpful to realize that Obspy’s Inventory object is actually
a python translation of the data structure defined by the
<a class="reference external" href="https://www.fdsn.org/xml/station/">FDSN StationXML</a>
standardized format defined for web service requests for station metadata.
Like <code class="code docutils literal notranslate"><span class="pre">save_source</span></code> an example from the getting started tutorial
should be instructive:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inv</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_stations</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="s1">&#39;TA&#39;</span><span class="p">,</span> <span class="n">starttime</span><span class="o">=</span><span class="n">starttime</span><span class="p">,</span> <span class="n">endtime</span><span class="o">=</span><span class="n">endtime</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;xml&#39;</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_inventory</span><span class="p">(</span><span class="n">inv</span><span class="p">)</span>
</pre></div>
</div>
<p>This example extracts all stations with the “network code” of “TA”
(the Earthscope transportable array).  A complication of station
metadata that differs from source data is that station metatdata is
time variable.  The reason is that sensors change, three-component sensors
are reoriented, digitizers change, etc.  That means station metadata
have a time span for which they are valid that has to be handled to
assure we associate the right metadata with any piece of data.</p>
<p>In MsPASS we translate the StationXML data to documents stored in two
collections:  <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code>.  Both collections contain the
attributes <code class="code docutils literal notranslate"><span class="pre">starttime</span></code> and <code class="code docutils literal notranslate"><span class="pre">endtime</span></code> that define the time interval for which
that document’s data are valid.  <code class="code docutils literal notranslate"><span class="pre">site</span></code> is simpler.  It mainly contains
station location data defined with three standard attribute keys:
<code class="code docutils literal notranslate"><span class="pre">lat</span></code>, <code class="code docutils literal notranslate"><span class="pre">lon</span></code>, and <code class="code docutils literal notranslate"><span class="pre">elev</span></code>.  We store all geographic coordinates (i.e. lat and lon)
as decimal degrees and elevation (elev) in km.   The <code class="code docutils literal notranslate"><span class="pre">channel</span></code> collection
contains station location information but it also contains two additional
important pieces of metadata:  (1) orientation information defined by
the keys <code class="code docutils literal notranslate"><span class="pre">hang</span></code> and <code class="code docutils literal notranslate"><span class="pre">vang</span></code>, and (2) full response information.
We store response data in MongoDB as a pickle image of the data stored
in the StationXML data as translated by obspy.   In the read section
below we describe how to retrieve response data from <code class="code docutils literal notranslate"><span class="pre">channel</span></code>.</p>
</li>
</ol>
</div>
<div class="section" id="read">
<h2>Read<a class="headerlink" href="#read" title="Permalink to this headline">¶</a></h2>
<p>Read operation is the inverse of save (create).  The core readers were
design to simplify the process of reading the core data types of MsPASS:  TimeSeries
and Seismogram.  There are also convenience functions for reading ensembles.
As with the save operators we discuss here the key methods, but refer the
reader to the sphinx documentation for full usage.</p>
<ol class="arabic">
<li><p><code class="code docutils literal notranslate"><span class="pre">read_data</span></code> is the core method to read atomic data.  The method has
one required argument.  That argument can be either a MongoDB object
id or a MongoDB document retrieved from one of the “wf” collections.
The document version is actually only a convenience.  The actual
database operation always uses the unique ObjectID for the document used
to define the read operation.  (i.e. the first thing read_data does if
if finds arg 0 is a document is to extract the “_id” attribute and use
it to query MongoDB.  If arg 0 is an ObjectID the parsing of the doc is bypassed and
the query is performed immediately with id passed.)</p>
<p><code class="code docutils literal notranslate"><span class="pre">read_data</span></code> will use the waveform collection defined when the Database
handle was constructed.  That is, a given Database handle should only
be used for the same type of atomic object (TimeSeries or Seismogram).
If a workflow uses both data types we recommend creating two handles with
different names.  For interactive work one can also use the
<code class="code docutils literal notranslate"><span class="pre">set_database_schema</span></code> to switch.   Avoid frequent calls to
<code class="code docutils literal notranslate"><span class="pre">set_database_schema</span></code> for handling mixed data as it is a relatively
expensive (in time) operation that invokes reading of a file defining
the schema.</p>
<p>The data objects in MsPASS are stored internally as C++ objects with
multiple elements illustrated in the figure below.   Although these
objects can conceptually be thought of as a single entity the individual
parts are handled differently because they define different concepts
and are subject to different read, write, and storage rules.
<a class="reference internal" href="#crud-operations-figure1"><span class="std std-numref">Fig. 1</span></a> illustrates this fragmentation:</p>
<div class="figure align-center" id="id1">
<span id="crud-operations-figure1"></span><a class="reference internal image-reference" href="../_images/CRUD_operations_figure1.png"><img alt="../_images/CRUD_operations_figure1.png" src="../_images/CRUD_operations_figure1.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text">A schematic diagram of how different parts of a data object is handled.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>The key point of this figure is that the waveform data is treated differently
from the Metadata and two auxiliary items we call ProcessingHistory and the
error log (elog).  Waveform data is currently stored either internally in
MongoDB’s gridfs storage or in external files.  The wf collection for
the data type being read (wf_TimeSeries or wf_Seismogram) stores only
data we store as Metadata.  A more extensive discussion of Metadata and
how we use it can be found <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">here</span></a>.
That section also gives details about ProcessingHistory and the error
log and the reasons they are part of MsPASS.</p>
<p>By default <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> reads Metadata in what we call “promiscuous” mode.
That means it takes in all metadata stored in the wf collection arg 0
defines with no type checking or filtering.  Alternatives are “cautious”
and “pedantic”.   Both of the later enforce the type and name constraints defined
by the schema.   The difference is that in “pedantic” mode any
conflicts in data type stored versus what is expected will cause the
return to be marked dead.  In “cautious” mode the reader will attempt
to convert any mismatched types and mark the return dead only if the
conversion is not possible (e.g. a string like “xyz” cannot normally
be converted to an integer and a python list cannot be converted to
a float.)  Guidelines for how to use these different modes are:</p>
<ol class="arabic simple">
<li><p>Use “promiscuous” mode when the wf collection to be read is known
to be clean.  That mode is the default because it is faster to
run because all the safeties are bypassed.  The potential cost is that
some members of the data set could be killed on input.
That potential problem can normally be eliminated by running the
<code class="code docutils literal notranslate"><span class="pre">clean</span></code> method described in a section below.</p></li>
<li><p>Use “cautious” for data saved without an intervening <code class="code docutils literal notranslate"><span class="pre">clean</span></code>
operation, especially if the workflow contains an experimental
algorithm.</p></li>
<li><p>The “pedantic” mode is mainly of use for data export where a
type mismatch could produce invalid data required by another package.</p></li>
</ol>
</li>
<li><p>A closely related function to <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> is <code class="code docutils literal notranslate"><span class="pre">read_ensemble_data</span></code>.  Like
<code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code> it is mostly a loop to assemble an ensemble of
atomic data using a sequence of calls to <code class="code docutils literal notranslate"><span class="pre">read_data</span></code>.  The sequence of
what to read is defined by arg 0 that can be one of two things:  (a) a
python list of ObjectIds of documents in a wf collection that define
the ensemble, or (b) a MongoDB cursor object returned by a query that
defines the ensemble.  The second option should be clearer with this
example showing how to read a TimeSeriesEnsemble that is a generalized
shot gather.  This fragment assumes the symbol source_id was set above
by some other mechanism.  e.g. it could be extracted from a loop over
the <code class="code docutils literal notranslate"><span class="pre">source</span></code> collection (or query the source collection) aimed at
processing all (or a subset) of the sources defined there.  Notice we
also include a size check with the MongoDB function count_documents
to impose constraints on the query. That is always good practice.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;source_id&quot;</span><span class="p">:</span> <span class="n">source_id</span><span class="p">}</span>
<span class="n">ndocs</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">count_documents</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">if</span> <span class="n">ndocs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No data found for source_id = &quot;</span><span class="p">,</span> <span class="n">source_id</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">ndocs</span> <span class="o">&gt;</span> <span class="n">TOOBIG</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents matching source_id=&quot;</span><span class="p">,</span> <span class="n">source_id</span><span class="p">,</span> <span class="s2">&quot; is &quot;</span><span class="p">,</span> <span class="n">ndocs</span><span class="p">,</span>
        <span class="s2">&quot;Exceeds the internal limit on the ensemble size=&quot;</span><span class="p">,</span> <span class="n">TOBIG</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">ens</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">read_ensemble_data</span><span class="p">(</span><span class="n">cursor</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>A workflow that needs to read and process a large data sets in
a parallel environment should use
the parallel equivalent of <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> and <code class="code docutils literal notranslate"><span class="pre">read_ensemble_data</span></code> called
<code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code>.  MsPASS supports two parallel frameworks called
SPARK and DASK.   Both abstract the concept of the parallel data set in
a container they call an RDD and Bag respectively.   Both are best thought
of as a handle to the entire data set that can be passed between
processing functions.   The <code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> method is critical
to improve performance of a parallel workflow.  The use of storage
in MongoDB’s gridfs in combination with SPARK or DASK
are known to help reduce io bottlenecks
in a parallel environment.  SPARK and DASK have internal mechanisms to schedule
IO to optimize throughput, particularly with reads made through the gridfs
mechanism we use as the default data storage.  <code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code>
provides the mechanism to accomplish that.</p>
<p><code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> has a very different call structure than the
other seismic data readers.  It is not a method of Database, but a
separate function call.  The input to be read by this function is
defined by arg 2 (C counting starting at 0).  It expects to be passed a
MongoDB cursor object, which is the standard return from the database
find operation.   As with the other functions discussed in this section
a block of example code should make this clearer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span><span class="p">,</span><span class="n">read_distributed_data</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="c1"># This is the name used to acccess the database of interest assumed</span>
<span class="c1"># to contain data loaded previously.  Name used would change for user</span>
<span class="n">dbname</span> <span class="o">=</span> <span class="s1">&#39;distributed_data_example&#39;</span>  <span class="c1"># name of db set in MongoDB - example</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span><span class="n">dbname</span><span class="p">)</span>
<span class="c1"># This example reads all the data currently stored in this database</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">find</span><span class="p">({})</span>
<span class="n">rdd0</span> <span class="o">=</span> <span class="n">read_distributed_data</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="n">dbname</span><span class="p">,</span> <span class="n">cursor</span><span class="p">)</span>
</pre></div>
</div>
<p>The output of the read is the SPARK RDD that we assign the symbol rdd0.
If you are using DASK instead of SPARK you would add the optional
argument <code class="code docutils literal notranslate"><span class="pre">format='dask'</span></code>.</p>
</li>
</ol>
</div>
<div class="section" id="update">
<h2>Update<a class="headerlink" href="#update" title="Permalink to this headline">¶</a></h2>
<p>Because of the way we stored seismic data in MsPASS (see figure above)
the concept of an update makes sense only for Metadata.
The update concept makes no sense for ProcessingHistory and error log data.
Hence, the history and elog collections, that hold that data, should never
be updated.   No MsPASS supported algorithms will do that, but we
emphasize that constraint because you as the owner of the dataset could
(incorrectly) modify history or elog with calls to MongoDB’s api.</p>
<p>As noted elsewhere Metadata loaded with data objects in MsPASS can come
from one of two places:  (1) attributes loaded directly with the atomic data from
the unique document in a wf collection with which that data is associated,
and (2) “normalized” data loaded through a cross reference ID from one of the
three standardized collection in MsPASS (<code class="code docutils literal notranslate"><span class="pre">site</span></code>, <code class="code docutils literal notranslate"><span class="pre">channel</span></code>, and <code class="code docutils literal notranslate"><span class="pre">source</span></code>).
In a waveform processing job (i.e. python driver script) the metadata
extracted from normalized collections should be treated as immutable.
In fact, when schema validation tests are enabled for save operations
(see above) any accidental changes to any normalized attributes will not be
saved but will be flagged with error log entries during the save.
In most cases regular attributes from normalized data (e.g. source_lat and
source_lon used for an earthquake epicenter) are silently ignored in an
update.  Trying to alter a normalization id field (i.e. source_id, site_id,
or channel_id) is always treated as a serious error that invalidates the
data.  The following two rules summarize these idea in a more concise form:</p>
<ul class="simple">
<li><p><strong>Update Rule 1</strong>:  Processing workflows should never alter any database
attribute marked readonly or loaded from a normalization collection.</p></li>
<li><p><strong>Update Rule 2</strong>:  Processing workflows must never alter a cross-referencing
id field.   Any changes to cross-referencing ids defined in the schema will
cause the related data to be marked dead.</p></li>
</ul>
<p>These rules apply to both updates and writes.  How violations of the rules
are treated on writes or updates depends on the setting of the <code class="code docutils literal notranslate"><span class="pre">mode</span></code> argument
common to all update and write methods described in more detail in a section
below.</p>
</div>
<div class="section" id="delete">
<h2>Delete<a class="headerlink" href="#delete" title="Permalink to this headline">¶</a></h2>
<p>A delete operation is much more complicated in MsPASS that what you would
find as a type example in any textbook an database theory.  In a
relational environment delete normally means removing a single tuple.
In MongoDB delete is more complicated because it is
common to delete only a subset of the contents of a given document (the equivalent
of a relational tuple).  The figure above shows that with MsPASS we have
the added complexity of needing to handle data spread across multiple MongoDB
collections and (sometimes) external files.  The problem with connected
collections is the same as that a relational system has to handle with
multiple relations that are commonly cross-referenced to build a
relational join.  The external file problem is familiar to any user
that has worked with a CSS3.0 relational database schema like Antelope.</p>
<p>In MsPASS we adopt these rules to keep delete operations under control.</p>
<ul class="simple">
<li><p><strong>Delete Rule 1</strong>:  Normalization collection documents should never be
deleted during a processing run.  Creation of these collections should
always be treated as a preprocessing step.</p></li>
<li><p><strong>Delete Rule 2</strong>:  Any deletions of documents in normalization collections should
be done through one of the MongoDB APIs.  If such housecleaning is
needed it is the user’s responsibility to assure this does not leaving
unresolved cross-references to waveform data.</p></li>
<li><p><strong>Delete Rule 3</strong>:  Deletes of waveform data, wf collections, history,
and error log data are best done through the mspass Database
handle.  Custom cleanup is an advanced topic that must be handled
with caution.</p></li>
</ul>
<p>We trust rules 1 and 2 require no further comment.  Rule 3, however,
needs some clarification to understand how we handle deletes.
A good starting point is to look at the signature of the simple delete
method of the Database class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="nb">id</span><span class="p">,</span><span class="n">remove_unreferenced_files</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">clear_history</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">clear_elog</span><span class="o">=</span><span class="kc">True</span><span class="p">):)</span>
</pre></div>
</div>
<p>As with the read methods id can be either an ObjectID referencing a
document in one of the relevant wf collection or a document that contains
that ObjectID.  Similarly, the idea of the <code class="code docutils literal notranslate"><span class="pre">clear_history</span></code> and <code class="code docutils literal notranslate"><span class="pre">clear_elog</span></code>
may be apparent from the name.  When true all documents linked to the
waveform data being deleted in the history and elog collections (respectively)
will also be deleted.  If either are false debris can be left behind
in the elog and history collections.</p>
<p>The main complexity in this method is behind the boolean argument with the name
<code class="code docutils literal notranslate"><span class="pre">remove_unreferenced_files</span></code>.  First, recognize this argument is completely
ignored if the waveform data being referenced is stored internally in
MongoDB (the default) in the gridfs file system.  In that situation delete
will remove the sample data as well as the document in wf that id defines.
The complexity enters if the data are stored as external files.  The
simple delete method of Database is an expensive operation that should be
avoided within a workflow or on large datasets.  The reason is that
each call for deleting an atomic object (defined by id) requires a
second query to the wf collection involved to search for any other
data with an exact match to two attributes we used to define a
single data file:  <code class="code docutils literal notranslate"><span class="pre">dir</span></code> which is a directory name and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code> which is the
name of the file at leaf node of the file system tree.  (CSS3.0 users
are familiar with these attribute names.  We use the same names as the concept here
is identical to the CSS3.0’s use.)  Only when the secondary query finds
no matching values for <code class="code docutils literal notranslate"><span class="pre">dir</span></code> and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code> will the file be deleted.</p>
<p>For large datasets stored in external files we provide a secondary cleanup
mechanism through a special class we call the <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code>.
The constructor for a <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code> requires the Database handle
that it should use to manage file data.  The delete method for
the <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code> runs a four step algorithm:</p>
<ol class="arabic simple">
<li><p>Extract the <code class="code docutils literal notranslate"><span class="pre">dir</span></code> and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code> fields from the document from wf to
be staged for file deletion.</p></li>
<li><p>Build the path description for the referenced file as dir+”/”+dfile.</p></li>
<li><p>Push the result to a set container cached in the <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code>
class instance.  A set only adds a new element if the string that path
defines differs from an existing element.</p></li>
<li><p>Delete the document in the wf collection defined by the input id.</p></li>
</ol>
<p>To use this feature you will need to write a custom cleanup function in
python.  That cleanup function should call the delete method of <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code>
in a loop.   When the function should call
the <code class="code docutils literal notranslate"><span class="pre">delete_unreferenced_files</span></code> method of <code class="code docutils literal notranslate"><span class="pre">staged_file_manager</span></code>.   That method
queries for any residual matches for <code class="code docutils literal notranslate"><span class="pre">dir</span></code> and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code>.  Only if there are
no remaining matches will the referenced file be deleted.   Once removed the
entry for that file is removed from the internal set container.
This algorithm allows for interwoven calls to the <code class="code docutils literal notranslate"><span class="pre">delete</span></code> method and
the <code class="code docutils literal notranslate"><span class="pre">deleted_unreferenced_files</span></code> method.  That allows for multiple cleanup
stages during a workflow to help manage memory.</p>
</div>
<div class="section" id="key-io-concepts">
<h2>Key IO Concepts<a class="headerlink" href="#key-io-concepts" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mspass-chemistry">
<h3>MsPASS Chemistry<a class="headerlink" href="#mspass-chemistry" title="Permalink to this headline">¶</a></h3>
<p>In this section we expand on some concepts the user needs to understand
in interacting with the io system in MsPASS.  If we repeat things it means
they are important, not that we were careless in writing this document.</p>
<p>It might be useful to think of data in MsPASS with an analogy from
chemistry:  Ensemble data are analogous to molecules make up of a
chain of atoms, the atoms are our “Atomic” data objects (TimeSeries or
Seismogram objects), and each atom can be broken into a set of subatomic
particles.  The figure above illustrates the subatomic idea visually.
We call these “subatomic particles”
Metadata, sample data, error log, and (processing) history.  The subatomic
particle have very different properties.</p>
<ol class="arabic simple">
<li><p><em>Metadata</em> are generalized header data.  Our Metadata concept maps closely
to the concepts of a python dict.  There are minor differences described
elsewhere.  For database interaction the most important concept is that
Metadata, like a dict, is a way to index a piece of data with a name-value
pair.   A fundamental reason MongoDB was chosen for data management in
MsPASS is that a MongoDB document maps almost exactly into a python dict
and by analogy our Metadata container.</p></li>
<li><p><em>waveform data</em> are the primary data MsPASS was designed to support.
Waveform data is the largest volume of information, but is different in
that it has a more rigid structure;  TimeSeries wavformdata are universally
stored as a vector, and Seismogram data are most rationally (although not
universally) stored as a matrix.  All modern computer systems have
very efficient means of moving contiguous blocks of data from storage to
memory so reading waveform data is a very different problem than
reading Metadata when they are fragmented as in MsPASS. Note that
traditional waveform handling uses a fixed format with a header and
data section to exploit the efficiency of reading contiguous memory blocks.
That is why traditional formats like SAC and SEGY have a fixed header/data
sections that define “the data”.   To make MsPASS generic that paradigm
had to be broken so it is important to recognize in MsPASS
waveform data are completely disaggreted from the other data components
we use for defining TimeSeries and Seismogram objects.</p></li>
<li><p><em>error log</em> data has yet another fundamentally different structure.
First of all, our normal goal in any processing system is to minimize
the number of data objects that have any error log entries at all.
After all, an error log entry means something may be wrong that
invalidated the data or make the results questionable.  We structure
error logs internally as a linked list.   There is an order because
multiple errors define a chain in the order they were posted.   The order,
however, is of limited use.  What is important in a processing workflow is
that nonfatal errors can be posted to the error log and are accumulated
as the data move through a processing chain.  That means all log entries
must make it clear what algorithm posted the error.  We handle that
by having all MsPASS supported processing functions post error messages
that have a unique tag back to the process that generated them.</p></li>
<li><p><em>processing history</em> is an optional component of MsPASS processing that
is designed to preserve the sequence of data processing steps required to
produce a processed waveform saved by the system.  The details of the
data structures used to preserve that history is a lengthy topic best
discussed elsewhere.  For this section the key point is that preserving
the history chain is an optional save parameter.  Whenever a save operation
for history is initiated the accumulated history chain is dumped to
the database, the history chain container is cleared, and then redefined
with a hook back to the data that was just saved.</p></li>
</ol>
<p>In MsPASS Metadata are further subdivided into three additional subsets
that are handled differently:</p>
<ol class="arabic simple">
<li><p>An attribute can be marked read-only in the schema.   As the
name implies that means they are never expected to be altered in a
workflow.</p></li>
<li><p>A special form of read-only attributes are attributes loaded by
readers from normalized collections.  Such attributes are never saved
by atomic object writers and the normalized collection (i.e. source, site,
and channel) are always treated as strinctly read only.</p></li>
<li><p>Normalization requires a cross-referencing method.   In MsPASS we
always uses the ObjectID of the document in the normalizing collection
and store that attribute with a key with a common structure:
<code class="code docutils literal notranslate"><span class="pre">collection_id</span></code> where “collection” is a variable and “_id” is literal.
(e.g. the linking key for the source collection is “source_id”).
We use that approach because in MongoDB an ObjectID is guaranteed to
provide a unique index.   That allows the system be more generic.
Hence, unlike FSDS data centers that depend upon the SEED format in
MsPASS net, sta, chan, loc are baggage for joining the site or channel
collections to a set of waveform data.  We have functions for
linking seed data with net, sta, chan, and loc keys to build links
but the links are still defined by ObjectIDs.   An example of why this
is more generic is to contrast SEED data to something like a CMP
reflection data set.  In a CMP survey geophone locations are never
given names but are indexed by something else like a survey flag
position.   We support CMP data with the same machinery as SEED
data because the link is through the ObjectID.  The process of
defining the geometry (site and/or channel) just requires a different
cross-referencing algorithm. Because of their central role in
providing such cross references a normalization id is treated
as absolutely immutable in a workflow.  If a writer detects a linking
id was altered the datum with which it is associated will be marked
bad (dead) and the waveform data will not be saved.</p></li>
</ol>
</div>
<div class="section" id="save-concepts">
<h3>Save Concepts<a class="headerlink" href="#save-concepts" title="Permalink to this headline">¶</a></h3>
<p>Waveform save methods begin with this axiom:  a save operation should
never abort for anything but a system error.   That means the definition of
success is not black and white.  There are a range of known and probably
as yet unknown ways data can acquire inconsistencies that are problems of
varying levels of severity.  Here is the range of outcomes in increasing
order severity:</p>
<ol class="arabic simple">
<li><p>No problems equal to complete success.</p></li>
<li><p>Problems that could be repaired automatically.  Such errors always
generate error log entries, but the errors are judged to
be harmless.   A good example is automatic type conversion from an
integer to a floating point number.</p></li>
<li><p>Errors that are recoverable but leave anomalies in the database.
An example is the way read_only data and normalized attributes are handled if
the writer detects that they have changed in the workflow.  When that
happens the revised data are saved to the related wf collection with a
an altered key and a more serious error is logged.</p></li>
<li><p>Unrecoverable MsPASS errors that might be called an unforgivable sin.
At present the only unforgivable sin is changing a cross-referencing id.
If a writer detects that cross-referencing ObjectID has been altered the
data will be marked dead and the Metadata document will be written to
a special collection called “tombstone”.</p></li>
</ol>
<ol class="arabic simple" start="4">
<li><p>Unrecoverable (fatal) errors will abort a workflow.   At present that
should only happen from system generated errors that throw an
unexpected exception in python.   If you encounter any errors that
causes a job to abort, the standard python handlers should post an
informative error.  If you find the error should be recoverable, you
can and should write a python error handler by surrounding the problem
section with a <em>try-except</em> block.</p></li>
</ol>
<p>Save operations by default apply only limited safeties defined by items 3-4
above.  Those are all required because if they were ignored the database
could be corrupted.   Safeties defined by item 2 are optional to make save
operations faster, although users are warned we may change that option
as we acquire more timing data.</p>
<p>In a save operation error log data is always saved.   The log entries are
linked to wf collections with another ObjectID with the standard naming
convention for cross-reference keys.  That is, wf_TimeSeries_id and
wf_Seismogram_id for TimeSeries and Seismogram data respectively.</p>
<p>Data marked dead are handled specially.  For such data the sample will be
throw away.  The Metadata will be written to a separate collection called
“tombstone”.  Error log data linked to dead data are written to the elog
collection along with living data, but the cross-referencing id is
tagged as “tombstone_id”.  That provides a simple query mechanism to
show only the most serious errors from a processing run.</p>
<p>Saving history data is optional.  When enabled the history chain contents
are dumped to this history collection, the history container is cleared, and
then initialized with a reference to the saved entry and the data
redefined as what we call an “ORIGIN”.  The clear process is done because of
a concern that history data could, in some instances, potentially cause
a memory bloat with iterative processing.</p>
</div>
<div class="section" id="read-concepts">
<h3>Read concepts<a class="headerlink" href="#read-concepts" title="Permalink to this headline">¶</a></h3>
<p>Reads have to construct a valid data object and are subject to different
constraints.  We believe it is most instructive to describe these in the order
they are handled during construction.</p>
<ol class="arabic simple">
<li><p>Construction of TimeSeries or Seismogram objects are driven by
document data read from the wf_TimeSeries or wf_Seismogram collection
respectively.   By default the entire contents of each document
are loaded into Metadata with no safety checks (defined
above as “promiscuous mode”).  Options allow Metadata type checks to be enabled
against the schema.  In addition, one can list a set of keys that should
be dropped in the read.</p></li>
<li><p>By default normalized Metadata can only be loaded through cross-referencing id
keys (currently source_id, site_id, and/or channel_id but more may be added).
The set of which collections are to be loaded are controlled by optional
parameters in each reader.  An important constraint is that for all
normalized collections defined as required, if the cross-referencing
key is no defined a reader will ignore that datum.  <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> silently
signals that condition by returning a None.  <code class="code docutils literal notranslate"><span class="pre">read_ensemble_data</span></code> and
<code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> normally silently skip such data.   That model
is intentional because it allows initial loading of a large data set with
unresolvable anomalies that prevent one or more of the cross-referencing
ids from being defined.</p></li>
<li><p>The waveform data is read and loaded.  If that process fails the data
will be marked dead and an error log posted with the reason (e.g. a
file not found message).</p></li>
<li><p>If the sample date read is successful the error log will normally be empty
after any read.</p></li>
<li><p>If processing history is desired the <code class="code docutils literal notranslate"><span class="pre">load_history</span></code> option needs to be
set true.  On read the only action this creates is initialization of the
ProcessingHistory component of the data with a record providing a unique
link back to the data just read.</p></li>
</ol>
<p>We reiterate that the overall behavior of all readers are controlled by the
<code class="code docutils literal notranslate"><span class="pre">mode=</span></code> argument common to all.  The current options are: <code class="code docutils literal notranslate"><span class="pre">promiscuous</span></code>,
<code class="code docutils literal notranslate"><span class="pre">cautious</span></code>, and <code class="code docutils literal notranslate"><span class="pre">pedantic</span></code>.   Detailed descriptions of what each mean are
give above and in the sphynx documentation generated from docstrings.</p>
</div>
<div class="section" id="update-concepts">
<h3>Update Concepts<a class="headerlink" href="#update-concepts" title="Permalink to this headline">¶</a></h3>
<p>As noted above an update is an operation that can be made only to
Metadata saving the image to the related wf collection.  We know of
two common needs for a pure Metadata update without an associated
save of the waveform data.</p>
<ol class="arabic simple">
<li><p>A processing step that computes something that can be conveniently
stored as Metadata.  Examples are automated phase pickers and
amplitude measurements.</p></li>
<li><p>Pure Metadata operations.  e.g. most reflection processing systems
have some form of generic metadata calculator of various levels of
sophistication.  The most flexible can take multiple Metadata (header)
values and use them to compute a set a different value.   Such
operations do not alter the waveform data but may require a
database update to preserve the calculation.   An example is an
active source experiment where receiver coordinates can often be
computed from survey flag numbers or some other independent counter.</p></li>
</ol>
<p>Many database updates are standalone operations such as preprocessing to
create entries for attributes like the source_id cross-reference to
define a link to the right source for each waveform.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="handling_errors.html" class="btn btn-neutral float-right" title="Handling Errors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="database_concepts.html" class="btn btn-neutral float-left" title="Database Concepts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Ian Wang.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>