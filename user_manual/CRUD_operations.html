<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CRUD Operations in MsPASS &mdash; MsPASS 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Importing Data" href="importing_data.html" />
    <link rel="prev" title="Database Concepts" href="database_concepts.html" />  

  <style>
    .wy-nav-content { max-width: 1600px; }
  </style>

  
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MsPASS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
    
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/run_mspass_with_docker.html">Run MsPASS with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/deploy_mspass_with_docker_compose.html">Deploy MsPASS with Docker Compose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/deploy_mspass_on_HPC.html">Deploy MsPASS on HPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started_overview.html">MsPASS Setup In-Depth Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Manual</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_object_design_concepts.html">Data Object Design Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="time_standard_constraints.html">Time Standard Constraints</a></li>
<li class="toctree-l1"><a class="reference internal" href="obspy_interface.html">Using ObsPy with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="database_concepts.html">Database Concepts</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">CRUD Operations in MsPASS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create">Create</a></li>
<li class="toctree-l2"><a class="reference internal" href="#read">Read</a></li>
<li class="toctree-l2"><a class="reference internal" href="#update">Update</a></li>
<li class="toctree-l2"><a class="reference internal" href="#delete">Delete</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-io-concepts">Key IO Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mspass-chemistry">MsPASS Chemistry</a></li>
<li class="toctree-l3"><a class="reference internal" href="#save-concepts">Save Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#read-concepts">Read concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#update-concepts">Update Concepts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="importing_data.html">Importing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="handling_errors.html">Handling Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_editing.html">Data Editing</a></li>
<li class="toctree-l1"><a class="reference internal" href="header_math.html">Header (Metadata) Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphics.html">Graphics in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="processing_history_concepts.html">Processing History Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_processing.html">Parallel Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="adapting_algorithms.html">Adapting an Existing Algorithm to MsPASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cxx_api/index.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mspass_schema/mspass_schema.html">MsPASS Schema</a></li>
</ul>

    <a href= "../genindex.html">Index</a>
  
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MsPASS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>CRUD Operations in MsPASS</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mspass-team/mspass/blob/master/docs/source/user_manual/CRUD_operations.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="crud-operations-in-mspass">
<span id="crud-operations"></span><h1>CRUD Operations in MsPASS<a class="headerlink" href="#crud-operations-in-mspass" title="Permalink to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>The acronymn CRUD is often used as a mnemonic in books and online tutorials
teaching database concepts.  CRUD stands for Create-Read-Update-Delete.
It is a useful mnemonic because it summarizes the four main things any database system
must accomplish cleanly.  This section is organized into subsections on
each of the topics defined by CRUD.  At the end of this section we
summarize some common options in CRUD operations.</p>
<p>The most common database operations are defined as methods in a class
we give the obvious name Database.  Most MsPASS jobs need to the have following
incantation at the top of the python job script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">DBClient</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">DBClient</span><span class="p">()</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="s1">&#39;database_name&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>where the second argument on Database, <code class="code docutils literal notranslate"><span class="pre">'database_name'</span></code>,
is a name you chose for the dataset you are working with.
For the remainder of this section we will use the symbol “db” as
defined, but as in any programming language you need to recognize the
symbol can be anything you as the user find sensible. “db” is just our
choice.</p>
<p>Unlike relational database systems, a schema is not required by
MongoDB.   However, for reasons outlined in the section
<a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">Data Object Design Concepts</span></a> a schema feature
was added as a component of MsPASS.  We emphasize the
schema in this design, however, can be thought of as more
like guidelines than rigid rules.
The default schema for a Database assumes the data set is a set of
TimeSeries object.  That implies the dataset is defined in the
collection we call <code class="code docutils literal notranslate"><span class="pre">wf_TimeSeries</span></code>.   If the dataset is already
assembled into three component bundles (<code class="code docutils literal notranslate"><span class="pre">Seismogram</span></code> objects)
the Database constructor needs to be informed of that through this
alternative construct:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="s1">&#39;database_name&#39;</span><span class="p">,</span> <span class="n">db_schema</span><span class="o">=</span><span class="s1">&#39;wf_Seismogram&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If your workflow requires reading both TimeSeries and Seismogram
data, best practice (i.e. it isn’t required but a good idea)
would be to create two handles with one using the wf_TimeSeries
and the other using the wf_Seismogram schema.</p>
</section>
<section id="create">
<h2>Create<a class="headerlink" href="#create" title="Permalink to this heading"></a></h2>
<p>We tag all methods of the Database class that do “Create” operations with
the synonymous word “save”.   Here we list all save methods with a brief
description of each method.  Consult the docstring pages for detailed
and most up to date usage:</p>
<ol class="arabic">
<li><p><a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data" title="mspasspy.db.database.Database.save_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_data</span></code></a> is probably the most common method you will use.  The
first argument is one of the atomic objects defined in MsPASS
(Seismogram or TimeSeries) that you wish to save.  Options are
described in the docstring.  Here is an example usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example assumes filename contains one channel of data in</span>
<span class="c1"># miniseed format</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;testfile.msd&#39;</span>
<span class="n">dstrm</span> <span class="o">=</span> <span class="n">read</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;MSEED&#39;</span><span class="p">)</span>
<span class="c1"># A stream is a list of obspy Trace objects - get the first</span>
<span class="n">dtr</span> <span class="o">=</span> <span class="n">dstrm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># This mspass function converts a Trace to our native TimeSeries</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">Trace2TimeSeries</span><span class="p">(</span><span class="n">dtr</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_data</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>By default <code class="code docutils literal notranslate"><span class="pre">save_data</span></code> stores all Metadata except those linked to
normalized collections (<code class="code docutils literal notranslate"><span class="pre">source</span></code>, <code class="code docutils literal notranslate"><span class="pre">channel</span></code>, and/or <code class="code docutils literal notranslate"><span class="pre">site</span></code>) with no
safety checks.  We discuss additional common options in a later section.</p>
</li>
<li><p><a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.save_ensemble_data" title="mspasspy.db.database.Database.save_ensemble_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_ensemble_data</span></code></a> is similar to <code class="code docutils literal notranslate"><span class="pre">save_data</span></code> except the first argument
is an Ensemble object.  There are currently two of them:  (1) TimeSeriesEnsemble
and (2) SeismogramEnsemble.   As discussed in the section
<a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">Data Object Design Concepts</span></a> an Ensemble
is a generalization of the idea of a “gather” in seismic reflection processing.
The <code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code> method is a convenience function for saving Ensembles.
Ensembles are containers of atomic objects.  <code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code>
is mostly a loop over the container saving the atomic objects it contains
to the wf_TimeSeries (for TimeSeriesEnsembles) or wf_Seismogram
(for Seismogram objects).  The method has one feature that differs form
<code class="code docutils literal notranslate"><span class="pre">save_data</span></code>; Ensemble objects may and often do contain attributes
common to the entire group in a separate Metadata container linked to the
ensemble as a whole.  Prior to entering the loop for saving the atomic
members of the ensemble the contents of the Ensemble’s Metadata container
are copied verbatim to each member.  If previous values existed in any
of the members they will be silently replaced by the ensemble groups version.</p></li>
<li><p><a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.save_catalog" title="mspasspy.db.database.Database.save_catalog"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_catalog</span></code></a> should be viewed mostly as a convenience method to build
the <code class="code docutils literal notranslate"><span class="pre">source</span></code> collection from QUAKEML data downloaded from FDSN data
centers via obspy’s web services functions.   <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> can be
thought of as a converter that translates the contents of a QUAKEML
file or string for storage as a set of MongoDB documents in the <code class="code docutils literal notranslate"><span class="pre">source</span></code>
collection.  We used obspy’s <code class="code docutils literal notranslate"><span class="pre">Catalog</span></code> object as an intermediary to
avoid the need to write our own QUAKEML parser.   As with save_data
the easiest way to understand the usage would be this example derived from
our <em>getting_started</em> tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;IRIS&quot;</span><span class="p">)</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">UTCDateTime</span><span class="p">(</span><span class="s1">&#39;2011-03-11T05:46:24.0&#39;</span><span class="p">)</span>
<span class="n">starttime</span> <span class="o">=</span> <span class="n">t0</span><span class="o">-</span><span class="mf">3600.0</span>
<span class="n">endtime</span> <span class="o">=</span> <span class="n">t0</span><span class="o">+</span><span class="p">(</span><span class="mf">7.0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">24.0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">3600.0</span><span class="p">)</span>
<span class="n">lat0</span> <span class="o">=</span> <span class="mf">38.3</span>
<span class="n">lon0</span> <span class="o">=</span> <span class="mf">142.5</span>
<span class="n">minlat</span> <span class="o">=</span> <span class="n">lat0</span><span class="o">-</span><span class="mf">3.0</span>
<span class="n">maxlat</span> <span class="o">=</span> <span class="n">lat0</span><span class="o">+</span><span class="mf">3.0</span>
<span class="n">minlon</span> <span class="o">=</span> <span class="n">lon0</span><span class="o">-</span><span class="mf">3.0</span>
<span class="n">maxlon</span> <span class="o">=</span> <span class="n">lon0</span><span class="o">+</span><span class="mf">3.0</span>
<span class="n">minmag</span> <span class="o">=</span> <span class="mf">6.5</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_events</span><span class="p">(</span><span class="n">starttime</span><span class="o">=</span><span class="n">starttime</span><span class="p">,</span> <span class="n">endtime</span><span class="o">=</span><span class="n">endtime</span><span class="p">,</span>
                        <span class="n">minlatitude</span><span class="o">=</span><span class="n">minlat</span><span class="p">,</span> <span class="n">minlongitude</span><span class="o">=</span><span class="n">minlon</span><span class="p">,</span>
                        <span class="n">maxlatitude</span><span class="o">=</span><span class="n">maxlat</span><span class="p">,</span> <span class="n">maxlongitude</span><span class="o">=</span><span class="n">maxlon</span><span class="p">,</span>
                                <span class="n">minmagnitude</span><span class="o">=</span><span class="n">minmag</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_catalog</span><span class="p">(</span><span class="n">cat</span><span class="p">)</span>
</pre></div>
</div>
<p>This particular example pulls 11 large aftershocks of the 2011 Tohoku
Earthquake.</p>
</li>
<li><p><a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.save_inventory" title="mspasspy.db.database.Database.save_inventory"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_inventory</span></code></a> is similar in concept to <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code>, but instead of
translating data for source information it translates information to
MsPASS for station metadata.  The station information problem is slightly
more complicated than the source problem because of an implementation
choice we made in MsPASS.   That is, because a primary goal of MsPASS
was to support three-component seismograms as a core data type, there
is a disconnect in what metadata is required to support a TimeSeries
versus a Seismogram object.   We handle this by defining two different,
but similar MongoDB collections:  <code class="code docutils literal notranslate"><span class="pre">channel</span></code> for TimeSeries data and
<code class="code docutils literal notranslate"><span class="pre">site</span></code> for Seismogram objects.  The name for this method contains the
keyword “inventory” because like <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> we use an obspy
python class as an intermediary.  The reasons is similar; obspy had
already solved the problem of downloading station metadata from
FDSN web services with their
<a class="reference external" href="https://docs.obspy.org/packages/obspy.core.inventory.html">read_inventory function</a>.
As with <code class="code docutils literal notranslate"><span class="pre">save_catalog</span></code> <code class="code docutils literal notranslate"><span class="pre">save_inventory</span></code> can be thought of as a translator
from data downloaded with web services to the form needed in MsPASS.
It may be helpful to realize that Obspy’s Inventory object is actually
a python translation of the data structure defined by the
<a class="reference external" href="https://www.fdsn.org/xml/station/">FDSN StationXML</a>
standardized format defined for web service requests for station metadata.
Like <code class="code docutils literal notranslate"><span class="pre">save_source</span></code> an example from the getting started tutorial
should be instructive:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inv</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get_stations</span><span class="p">(</span><span class="n">network</span><span class="o">=</span><span class="s1">&#39;TA&#39;</span><span class="p">,</span> <span class="n">starttime</span><span class="o">=</span><span class="n">starttime</span><span class="p">,</span> <span class="n">endtime</span><span class="o">=</span><span class="n">endtime</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;xml&#39;</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">db</span><span class="o">.</span><span class="n">save_inventory</span><span class="p">(</span><span class="n">inv</span><span class="p">)</span>
</pre></div>
</div>
<p>This example extracts all stations with the “network code” of “TA”
(the Earthscope transportable array).  A complication of station
metadata that differs from source data is that station metatdata is
time variable.  The reason is that sensors change, three-component sensors
are reoriented, digitizers change, etc.  That means station metadata
have a time span for which they are valid that has to be handled to
assure we associate the right metadata with any piece of data.</p>
<p>In MsPASS we translate the StationXML data to documents stored in two
collections:  <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code>.  Both collections contain the
attributes <code class="code docutils literal notranslate"><span class="pre">starttime</span></code> and <code class="code docutils literal notranslate"><span class="pre">endtime</span></code> that define the time interval for which
that document’s data are valid.  <code class="code docutils literal notranslate"><span class="pre">site</span></code> is simpler.  It mainly contains
station location data defined with three standard attribute keys:
<code class="code docutils literal notranslate"><span class="pre">lat</span></code>, <code class="code docutils literal notranslate"><span class="pre">lon</span></code>, and <code class="code docutils literal notranslate"><span class="pre">elev</span></code>.  We store all geographic coordinates (i.e. lat and lon)
as decimal degrees and elevation (elev) in km.   The <code class="code docutils literal notranslate"><span class="pre">channel</span></code> collection
contains station location information but it also contains two additional
important pieces of metadata:  (1) orientation information defined by
the keys <code class="code docutils literal notranslate"><span class="pre">hang</span></code> and <code class="code docutils literal notranslate"><span class="pre">vang</span></code>, and (2) full response information.
We store response data in MongoDB as a pickle image of the data stored
in the StationXML data as translated by obspy.   In the read section
below we describe how to retrieve response data from <code class="code docutils literal notranslate"><span class="pre">channel</span></code>.</p>
<p>Finally, we note a key feature of the <code class="code docutils literal notranslate"><span class="pre">save_inventory</span></code> method:
it enforces a seed convention to avoid saving duplicate documents.
As noted earlier he SEED standard uses the keys we call net, sta, chan,
and loc along with a time interval to define a unique block of
receiver metadata.   The <code class="code docutils literal notranslate"><span class="pre">save_inventory</span></code> method enforces
the unique combination of these keys in a save.  It always will
refuse to add an entry it interprets as a pure duplicate document.
If you need to modify an existing site or channel
collection that has invalid documents you will need to write a custom function to override that
behaviour or rebuild the collection as needed with web services.</p>
</li>
</ol>
</section>
<section id="read">
<h2>Read<a class="headerlink" href="#read" title="Permalink to this heading"></a></h2>
<p>The Read operation is the inverse of save (create).  The core readers were
designed to simplify the process of reading the core data types of MsPASS:  TimeSeries
and Seismogram.  There are also convenience functions for reading ensembles.
As with the save operators we discuss here the key methods, but refer the
reader to the sphinx documentation for full usage.</p>
<ol class="arabic">
<li><p><a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.read_data" title="mspasspy.db.database.Database.read_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">read_data</span></code></a> is the core method for reading atomic data.  The method has
one required argument.  That argument is an ObjectID for the document used
to define the read operation OR a MongoDB document (python dict) that
contains the ObjectID.  The ObjectID is guaranteed to provide a
unique key to one and only one document and is the way this reader
finds one and only one record to fetch per call.  The most common use
is the for with a MongoDB document in a construct like the following
in a serial job:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="p">{</span><span class="o">...</span><span class="n">Some</span> <span class="n">MongoDB</span> <span class="n">query</span> <span class="nb">dict</span> <span class="n">entry</span><span class="o">...</span><span class="p">}</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span> <span class="c1"># Changed to wf_Seismogram for 3D data</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">read_data</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>  <span class="c1"># Add option collection=&#39;wf_Seismogram&#39; for 3C reads</span>
</pre></div>
</div>
<p>By default <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> will use the waveform collection defined
in the schema defined for the handle.  The default for the standard
MsPASS schema is TimeSeries.   As the comment in the example states
if you are trying to read from a different collection (i.e wf_Seismogram
or wf_miniseed) you need to specify that alternative with the collection argument.</p>
<p>The data objects in MsPASS are stored internally as C++ objects with
multiple elements illustrated in the figure below.   Although these
objects should be thought of as a single entity the individual
parts are handled differently in reading because they define different concepts
and are subject to different read, write, and storage rules.
<a class="reference internal" href="#crud-operations-figure1"><span class="std std-numref">Fig. 5</span></a> illustrates this fragmentation:</p>
<figure class="align-center" id="id1">
<span id="crud-operations-figure1"></span><a class="reference internal image-reference" href="../_images/CRUD_operations_figure1.png"><img alt="../_images/CRUD_operations_figure1.png" src="../_images/CRUD_operations_figure1.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Schematic diagram of how different parts of a data object are handled.
The red box around the center of the figure shows a schematic of the
data components when a data object is constructed in memory.  The
boxes in the right-hand (black) box illustrate that the different
pieces of the object are normally stored in different places.
This example shows all the components stored within MongoDB
(the black box), but we note sample data may also be stored as
files in a file system or in cloud containers.</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The key point of this figure is that the waveform data is treated differently
from the Metadata and two auxiliary items we call ProcessingHistory and the
error log (elog).  Waveform data is currently stored either internally in
MongoDB’s gridfs storage or in external files.  Documents in the wf collection for
the data type being read (wf_TimeSeries or wf_Seismogram) contain only
data we store as Metadata.  A more extensive discussion of Metadata and
how we use it can be found <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">here</span></a>.
That section also gives details about ProcessingHistory and the error
log and the reasons they are part of MsPASS.</p>
<p>By default <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> reads Metadata in what we call “promiscuous” mode.
That means it takes in all metadata stored in the wf collection at which
it is is pointed and loads the results into the objects Metadata container
with no type checking or filtering.  Alternatives are “cautious”
and “pedantic”.   Both of the later enforce the type and name constraints defined
by the schema.   The difference is that in “pedantic” mode any
conflicts in data type stored versus what is expected will cause the
return to be marked dead.  In “cautious” mode the reader will attempt
to convert any mismatched types and mark the return dead only if the
conversion is not possible (e.g. a string like “xyz” cannot normally
be converted to an integer and a python list cannot be converted to
a float.)  Guidelines for how to use these different modes are:</p>
<ol class="arabic simple">
<li><p>Use “promiscuous” mode when the wf collection to be read is known
to be clean.  That mode is the default because it is faster to
run because all the safeties are bypassed.  The potential cost is that
some members of the data set could be killed on input.
That potential problem can normally be eliminated by running the
<code class="code docutils literal notranslate"><span class="pre">clean</span></code> method described in a section below.</p></li>
<li><p>Use “cautious” for data saved without an intervening <code class="code docutils literal notranslate"><span class="pre">clean</span></code>
operation, especially if the workflow contains an experimental
algorithm.</p></li>
<li><p>The “pedantic” mode is mainly of use for data export where a
type mismatch could produce invalid data required by another package.</p></li>
</ol>
</li>
<li><p>A closely related function to <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> is <a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.read_ensemble_data" title="mspasspy.db.database.Database.read_ensemble_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">read_ensemble_data</span></code></a>.  Like
<code class="code docutils literal notranslate"><span class="pre">save_ensemble_data</span></code> it is mostly a loop to assemble an ensemble of
atomic data using a sequence of calls to <code class="code docutils literal notranslate"><span class="pre">read_data</span></code>.  The sequence of
what to read is defined by arg 0.   That arg must be one of two things:
(a) a python list of ObjectIDs or (b) a cursor object created by a query
that uniquely defines the ensemble contnts.  The example code below illustrates how this is done.
This code fragment assumes the variable <code class="code docutils literal notranslate"><span class="pre">source_id</span></code> was defined earlier
and defines (a) a valid ObjectId in the source collection, and (b) has
been defined in wf_TimeSeries previously by a cross-referencing function.  Notice we
also include a size check with the MongoDB function count_documents
to impose constraints on the query. That is always good practice.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;source_id&quot;</span><span class="p">:</span> <span class="n">source_id</span><span class="p">}</span>
<span class="n">ndocs</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">count_documents</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">if</span> <span class="n">ndocs</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No data found for source_id = &quot;</span><span class="p">,</span> <span class="n">source_id</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">ndocs</span> <span class="o">&gt;</span> <span class="n">TOOBIG</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of documents matching source_id=&quot;</span><span class="p">,</span> <span class="n">source_id</span><span class="p">,</span> <span class="s2">&quot; is &quot;</span><span class="p">,</span> <span class="n">ndocs</span><span class="p">,</span>
        <span class="s2">&quot;Exceeds the internal limit on the ensemble size=&quot;</span><span class="p">,</span> <span class="n">TOBIG</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">ens</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">read_ensemble_data</span><span class="p">(</span><span class="n">cursoe</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>A workflow that needs to read and process a large data sets in
a parallel environment should use
the parallel equivalent of <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> and <code class="code docutils literal notranslate"><span class="pre">read_ensemble_data</span></code> called
<code class="xref py py-meth docutils literal notranslate"><span class="pre">read_distributed_data</span></code>.  MsPASS supports two parallel frameworks called
SPARK and DASK.   Both abstract the concept of the parallel data set in
a container they call an RDD and Bag respectively.   Both are best thought
of as a handle to the entire data set that can be passed between
processing functions.   The <code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> method is critical
to improve performance of a parallel workflow.  The use of storage
in MongoDB’s gridfs in combination with SPARK or DASK
are known to help reduce io bottlenecks
in a parallel environment.  SPARK and DASK have internal mechanisms to schedule
IO to optimize throughput, particularly with reads made through the gridfs
mechanism we use as the default data storage.  <code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code>
provides the mechanism to accomplish that.</p>
<p><code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> has a very different call structure than the
other seismic data readers.  It is not a method of Database, but a
separate function call.  The input to be read by this function is
defined by arg 2 (C counting starting at 0).  It expects to be passed a
MongoDB cursor object, which is the standard return from the database
find operation.   As with the other functions discussed in this section
a block of example code should make this clearer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="n">Database</span><span class="p">,</span><span class="n">read_distributed_data</span>
<span class="n">dbclient</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>
<span class="c1"># This is the name used to acccess the database of interest assumed</span>
<span class="c1"># to contain data loaded previously.  Name used would change for user</span>
<span class="n">dbname</span> <span class="o">=</span> <span class="s1">&#39;distributed_data_example&#39;</span>  <span class="c1"># name of db set in MongoDB - example</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Database</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span><span class="n">dbname</span><span class="p">)</span>
<span class="c1"># This example reads all the data currently stored in this database</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">wf_TimeSeries</span><span class="o">.</span><span class="n">find</span><span class="p">({})</span>
<span class="n">rdd0</span> <span class="o">=</span> <span class="n">read_distributed_data</span><span class="p">(</span><span class="n">dbclient</span><span class="p">,</span> <span class="n">dbname</span><span class="p">,</span> <span class="n">cursor</span><span class="p">)</span>
</pre></div>
</div>
<p>The output of the read is the SPARK RDD that we assign the symbol rdd0.
If you are using DASK instead of SPARK you would add the optional
argument <code class="code docutils literal notranslate"><span class="pre">format='dask'</span></code>.</p>
</li>
</ol>
</section>
<section id="update">
<h2>Update<a class="headerlink" href="#update" title="Permalink to this heading"></a></h2>
<p>Because of the way we stored seismic data in MsPASS (see figure above)
the concept of an update makes sense only for Metadata.
The update concept makes no sense for ProcessingHistory and error log data.
Hence, the history and elog collections, that hold that data, should never
be updated.   No MsPASS supported algorithms will do that, but we
emphasize that constraint because you as the owner of the dataset could
(incorrectly) modify history or elog with calls to MongoDB’s api.</p>
<p>As noted elsewhere Metadata loaded with data objects in MsPASS can come
from one of two places:  (1) attributes loaded directly with the atomic data from
the unique document in a wf collection with which that data is associated,
and (2) “normalized” data loaded through a cross reference ID from one of the
standardized collection in MsPASS (currently <code class="code docutils literal notranslate"><span class="pre">site</span></code>, <code class="code docutils literal notranslate"><span class="pre">channel</span></code>, and <code class="code docutils literal notranslate"><span class="pre">source</span></code>).
In a waveform processing job (i.e. python driver script) the metadata
extracted from normalized collections should be treated as immutable.
In fact, when schema validation tests are enabled for save operations
(see above) any accidental changes to any normalized attributes will not be
saved but will be flagged with error log entries during the save.
In most cases regular attributes from normalized data (e.g. source_lat and
source_lon used for an earthquake epicenter) are silently ignored in an
update.  Trying to alter a normalization id field (i.e. source_id, site_id,
or channel_id) is always treated as a serious error that invalidates the
data.  The following two rules summarize these idea in a more concise form:</p>
<ul class="simple">
<li><p><strong>Update Rule 1</strong>:  Processing workflows should never alter any database
attribute marked readonly or loaded from a normalization collection.</p></li>
<li><p><strong>Update Rule 2</strong>:  Processing workflows must never alter a cross-referencing
id field.   Any changes to cross-referencing ids defined in the schema will
cause the related data to be marked dead.</p></li>
</ul>
<p>These rules apply to both updates and writes.  How violations of the rules
are treated on writes or updates depends on the setting of the <code class="code docutils literal notranslate"><span class="pre">mode</span></code> argument
common to all update and write methods described in more detail in a section
below.</p>
</section>
<section id="delete">
<h2>Delete<a class="headerlink" href="#delete" title="Permalink to this heading"></a></h2>
<p>A delete operation is much more complicated in MsPASS than what you would
find as a type example in any textbook on database theory.  In a
relational system delete normally means removing a single tuple.
In MongoDB delete is more complicated because it is
common to delete only a subset of the contents of a given document (the equivalent
of a relational tuple).  The figure above shows that with MsPASS we have
the added complexity of needing to handle data spread across multiple MongoDB
collections and (sometimes) external files.  The problem with connected
collections is the same as that a relational system has to handle with
multiple relations that are commonly cross-referenced to build a
relational join.  The external file problem is familiar to any user
that has worked with a CSS3.0 relational database schema like Antelope.</p>
<p>In MsPASS we adopt these rules to keep delete operations under control.</p>
<ul class="simple">
<li><p><strong>Delete Rule 1</strong>:  Normalization collection documents should never be
deleted during a processing run.  Creation of these collections should
always be treated as a preprocessing step.</p></li>
<li><p><strong>Delete Rule 2</strong>:  Any deletions of documents in normalization collections should
be done through one of the MongoDB APIs.  If such housecleaning is
needed it is the user’s responsibility to assure this does not leave
unresolved cross-references to waveform data.</p></li>
<li><p><strong>Delete Rule 3</strong>:  Deletes of waveform data, wf collections, history,
and error log data are best done through the mspass Database
handle.  Custom cleanup is an advanced topic that must be handled
with caution.</p></li>
</ul>
<p>We trust rules 1 and 2 require no further comment.  Rule 3, however,
needs some clarification to understand how we handle deletes.
A good starting point is to look at the signature of the simple core delete
method of the Database class: <a class="reference internal" href="../python_api/mspasspy.db.html#mspasspy.db.database.Database.delete_data" title="mspasspy.db.database.Database.delete_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">delete_data</span></code></a></p>
<p>As with the read methods id is the ObjectID of the wf collection document
that references the data to be deleted.
Similarly, the idea of the <code class="code docutils literal notranslate"><span class="pre">clear_history</span></code> and <code class="code docutils literal notranslate"><span class="pre">clear_elog</span></code>
may be apparent from the name.  When true all documents linked to the
waveform data being deleted in the history and elog collections (respectively)
will also be deleted.  If either are false, debris can be left behind
in the elog and history collections.  On the other hand, setting either
true will result in a loss of information that might be needed to address
problems during processing.  Furthermore, both are only relevant to
fully or partially processed data.   In general, we recommend the default
for any cleanups applied within a workflow.  Set clear_elog true only in
post processing cleanup after you are confident there are not serious
errors that need to be traced.  Set clear_history True only if you have
no interest in retaining the object level history.  The default is True
because we view object level history preservation as a last step to
provide a mechanism for others to reproduce your work.</p>
<p>The main complexity in this method is behind the boolean argument with the name
<code class="code docutils literal notranslate"><span class="pre">remove_unreferenced_files</span></code>.  First, recognize this argument is completely
ignored if the waveform data being referenced is stored internally in
MongoDB in the gridfs file system.  In that situation delete_data
will remove the sample data as well as the document in wf that id defines.
The complexity enters if the data are stored as external files.  The
atomic delete_data method of Database is an expensive operation that should be
avoided within a workflow or on large datasets.  The reason is that
each call for deleting an atomic object (defined by its id) requires a
second query to the wf collection involved to search for any other
data with an exact match to two attributes we used to define a
single data file:  <code class="code docutils literal notranslate"><span class="pre">dir</span></code> which is a directory name and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code> which is the
name of the file at leaf node of the file system tree.  (CSS3.0 users
are familiar with these attribute names.  We use the same names as the concept here
is identical to the CSS3.0’s use.)  Only when the secondary query finds
no matching values for <code class="code docutils literal notranslate"><span class="pre">dir</span></code> and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code> will the file be deleted.
You should recognize that if, as is strongly advised, data are organized in
a smaller number of larger files deletes of this kind can leave a lot of
debris.   For example, it is easy to produce examples where deleting thousands of
files in a dataset with millions of files removes few if any files.  On the
other hand, the common old SAC model of one waveform per file is an abomination
for storing millions of waveforms on any HPC system.   If your application
requires frequent delete operations for cleanup during a workflow
we strongly advise you store all your data with the
gridfs option.</p>
</section>
<section id="key-io-concepts">
<h2>Key IO Concepts<a class="headerlink" href="#key-io-concepts" title="Permalink to this heading"></a></h2>
<section id="mspass-chemistry">
<h3>MsPASS Chemistry<a class="headerlink" href="#mspass-chemistry" title="Permalink to this heading"></a></h3>
<p>In this section we expand on some concepts the user needs to understand
in interacting with the io system in MsPASS.  If we repeat things it means
they are important, not that we were careless in writing this document.</p>
<p>It might be useful to think of data in MsPASS with an analogy from
chemistry:  Ensemble data are analogous to molecules make up of a
chain of atoms, the atoms are our “Atomic” data objects (TimeSeries or
Seismogram objects), and each atom can be broken into a set of subatomic
particles.  The figure above illustrates the subatomic idea visually.
We call these “subatomic particles”
Metadata, waveform data, error log, and (processing) history.  The subatomic
particle have very different properties.</p>
<ol class="arabic simple">
<li><p><em>Metadata</em> are generalized header data.  Our Metadata concept maps closely
to the concepts of a python dict.  There are minor differences described
elsewhere.  For database interaction the most important concept is that
Metadata, like a dict, is a way to index a piece of data with a name-value
pair.   A fundamental reason MongoDB was chosen for data management in
MsPASS is that a MongoDB document maps almost exactly into a python dict
and by analogy our Metadata container.</p></li>
<li><p><em>waveform data</em> are the primary data MsPASS was designed to support.
Waveform data is the largest volume of information, but is different in
that it has a more rigid structure;  TimeSeries waveform data are universally
stored in memory as a vector, and Seismogram data are most rationally (although not
universally) stored as a matrix.  All modern computer systems have
very efficient means of moving contiguous blocks of data from storage to
memory so reading waveform data is a very different problem than
reading Metadata when they are fragmented as in MsPASS. Note that
traditional waveform handling uses a fixed format with a header and
data section to exploit the efficiency of reading contiguous memory blocks.
That is why traditional formats like SAC and SEGY have a fixed header/data
sections that define “the data”.   To make MsPASS generic that paradigm
had to be broken so it is important to recognize in MsPASS
waveform data are completely disaggregated from the other data components
we use for defining our data objects.</p></li>
<li><p><em>error log</em> data has yet another fundamentally different structure.
First of all, our normal goal in any processing system is to minimize
the number of data objects that have any error log entries at all.
After all, an error log entry means something may be wrong that
invalidated the data or make the results questionable.  We structure
error logs internally as a linked list.   There is an order because
multiple errors define a chain in the order they were posted.   The order,
however, is of limited use.  What is important in a processing workflow is
that nonfatal errors can be posted to the error log and are accumulated
as the data move through a processing chain.  That means all log entries
must make it clear what algorithm posted the error.  We handle that
by having all MsPASS supported processing functions post error messages
that have a unique tag back to the process that generated them.</p></li>
<li><p><em>processing history</em> is an optional component of MsPASS processing that
is designed to preserve the sequence of data processing steps required to
produce a processed waveform saved by the system.  The details of the
data structures used to preserve that history is a lengthy topic best
discussed elsewhere.  For this section the key point is that preserving
the history chain is an optional save parameter.  Whenever a save operation
for history is initiated the accumulated history chain is dumped to
the database, the history chain container is cleared, and then redefined
with a hook back to the data that was just saved.</p></li>
</ol>
<p>In MsPASS Metadata are further subdivided into three additional subsets
that are handled differently through the schema definition:</p>
<ol class="arabic simple">
<li><p>An attribute can be marked read-only in the schema.   As the
name implies that means they are never expected to be altered in a
workflow.</p></li>
<li><p>A special form of read-only attributes are attributes loaded by
readers from normalized collections.  Such attributes are never saved
by atomic object writers and the normalized collection (i.e. source, site,
and channel) are always treated as strictly read only.</p></li>
<li><p>Normalization requires a cross-referencing method.   In MsPASS we
normally uses the ObjectID of the document in the normalizing collection
and store that attribute using a key with a common structure:
<code class="code docutils literal notranslate"><span class="pre">collection_id</span></code> where “collection” is a variable and “_id” is literal.
(e.g. the linking key for the source collection is “source_id”).
We use that approach because in MongoDB an ObjectID is guaranteed to
provide a unique index.   That allows the system be more generic.
Hence, unlike FDSN data centers that depend upon the SEED format in
MsPASS net, sta, chan, loc (the core miniseed keys)
are baggage for joining the site or channel
collections to a set of waveform data.  We have functions for
linking seed data with net, sta, chan, and loc keys to build links
but the links are still best defined by ObjectIDs.   An example of why this
is more generic is to contrast SEED data to something like a CMP
reflection data set.  In a CMP survey geophone locations are never
given names but are indexed by something else like a survey flag
position.   We support CMP data with the same machinery as SEED
data because the link is through the ObjectID.  The process of
defining the geometry (site and/or channel) just requires a different
cross-referencing algorithm. Because of their central role in
providing such cross references a normalization id is treated
as absolutely immutable in a workflow.  If a writer detects a linking
id was altered the datum with which it is associated will be marked
bad (dead) and the waveform data will not be saved.</p></li>
</ol>
</section>
<section id="save-concepts">
<h3>Save Concepts<a class="headerlink" href="#save-concepts" title="Permalink to this heading"></a></h3>
<p>Waveform save methods begin with this axiom:  a save operation should
never abort for anything but a system error.   That means the definition of
success is not black and white.  There are a range of known and probably
as yet unknown ways data can acquire inconsistencies that are problems of
varying levels of severity.  Here is the range of outcomes in increasing
order severity:</p>
<ol class="arabic simple">
<li><p>No problems equal to complete success.</p></li>
<li><p>Problems that could be repaired automatically.  Such errors always
generate error log entries, but the errors are judged to
be harmless.   A good example is automatic type conversion from an
integer to a floating point number.</p></li>
<li><p>Errors that are recoverable but leave anomalies in the database.
An example is the way read_only data and normalized attributes are handled if
the writer detects that they have changed in the workflow.  When that
happens the revised data are saved to the related wf collection with a
an altered key and a more serious error is logged.</p></li>
<li><p>Unrecoverable MsPASS errors that might be called an unforgivable sin.
At present the only unforgivable sin is changing a cross-referencing id.
If a writer detects that cross-referencing ObjectID has been altered the
data will be marked dead and the Metadata document will be written to
a special collection called “graveyard”.</p></li>
</ol>
<ol class="arabic simple" start="4">
<li><p>Unrecoverable (fatal) errors will abort a workflow.   At present that
should only happen from system generated errors that throw an
unexpected exception in python.   If you encounter any errors that
causes a job to abort, the standard python handlers should post an
informative error.  If you find the error should be recoverable, you
can and should write a python error handler by surrounding the problem
section with a <em>try-except</em> block.</p></li>
</ol>
<p>Save operations by default apply only limited safeties defined by items 3-4
above.  Those are all required because if they were ignored the database
could be corrupted.   Safeties defined by item 2 are optional to make save
operations faster, although users are warned we may change that option
as we acquire more timing data.</p>
<p>In a save operation error log data is always saved.   The log entries are
linked to wf collections with another ObjectID with the standard naming
convention for cross-reference keys.  That is, wf_TimeSeries_id and
wf_Seismogram_id for TimeSeries and Seismogram data respectively.</p>
<p>Data marked dead are handled specially.  For such data the sample data will be
throw away.  The Metadata for dead data are saved in the elog collection
document associated with the datum as a subdocument accessible with the
key “tombstone”.  That provides a simple query mechanism to
show only the most serious errors from a processing run.   Specifically,
this code fragment will print all error messages associated with
dead data with a crude tag of seed net, sta, starttime before each
list of elog messages:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This needs to be checked for correctness - done while off the grid</span>
<span class="n">query</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;$def&#39;</span> <span class="p">:</span> <span class="s1">&#39;tombstone&#39;</span><span class="p">}</span>
<span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">elog</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
  <span class="n">wfmd</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="s1">&#39;tombstone&#39;</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error log contents for this Seismogram marked dead:&#39;</span><span class="p">,</span>
        <span class="n">wfmd</span><span class="p">[</span><span class="s1">&#39;net&#39;</span><span class="p">],</span> <span class="n">wfmd</span><span class="p">[</span><span class="s1">&#39;sta&#39;</span><span class="p">],</span> <span class="n">UTCDateTime</span><span class="p">(</span><span class="n">wfmd</span><span class="p">[</span><span class="s1">&#39;startime&#39;</span><span class="p">]))</span>
  <span class="n">err</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="s1">&#39;logdata&#39;</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">err</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Note the above is minimal to be concise.  A good report would contain
additional entries from the tombstone contents and additional components of
the container defined the symbol “e”.</p>
<p>Saving history data is optional.  When enabled the history chain contents
are dumped to this history collection, the history container is cleared, and
then initialized with a reference to the saved entry and the data
redefined as what we call an “ORIGIN”.  The clear process is done because of
a concern that history data could, in some instances, potentially cause
a memory bloat with iterative processing.</p>
</section>
<section id="read-concepts">
<h3>Read concepts<a class="headerlink" href="#read-concepts" title="Permalink to this heading"></a></h3>
<p>Reads have to construct a valid data object and are subject to different
constraints.  We believe it is most instructive to describe these in the order
they are handled during construction.</p>
<ol class="arabic simple">
<li><p>Construction of TimeSeries or Seismogram objects are driven by
document data read from the wf_TimeSeries or wf_Seismogram collection
respectively.   By default the entire contents of each document
are loaded into Metadata with no safety checks (defined
above as “promiscuous mode”).  Options allow Metadata type checks to be enabled
against the schema.  In addition, one can list a set of keys that should
be dropped in the read.</p></li>
<li><p>By default normalized Metadata can only be loaded through cross-referencing id
keys (currently source_id, site_id, and/or channel_id but more may be added).
The set of which collections are to be loaded are controlled by optional
parameters in each reader.  An important constraint is that for all
normalized collections defined as required, if the cross-referencing
key is not defined a reader will ignore that datum.  <code class="code docutils literal notranslate"><span class="pre">read_data</span></code> silently
signals that condition by returning a None.  <code class="code docutils literal notranslate"><span class="pre">read_ensemble_data</span></code> and
<code class="code docutils literal notranslate"><span class="pre">read_distributed_data</span></code> normally silently skip such data.   That model
is intentional because it allows initial loading of a large data set with
unresolvable anomalies that prevent one or more of the cross-referencing
ids from being defined.</p></li>
<li><p>The waveform data is read and the data object is constructed.  If that process fails the data
will be marked dead and an error log posted with the reason (e.g. a
file not found message).</p></li>
<li><p>If the sample date read is successful the error log will normally be empty
after any read.</p></li>
<li><p>If processing history is desired the <code class="code docutils literal notranslate"><span class="pre">load_history</span></code> option needs to be
set true.  In a reader the only action this creates is initialization of the
ProcessingHistory component of the data with a record providing a unique
link back to the data just read.</p></li>
</ol>
<p>We reiterate that the overall behavior of all readers are controlled by the
<code class="code docutils literal notranslate"><span class="pre">mode=</span></code> argument common to all.  The current options are: <code class="code docutils literal notranslate"><span class="pre">promiscuous</span></code>,
<code class="code docutils literal notranslate"><span class="pre">cautious</span></code>, and <code class="code docutils literal notranslate"><span class="pre">pedantic</span></code>.   Detailed descriptions of what each mean are
given above and in the sphynx documentation generated from docstrings.</p>
</section>
<section id="update-concepts">
<h3>Update Concepts<a class="headerlink" href="#update-concepts" title="Permalink to this heading"></a></h3>
<p>As noted above an update is an operation that can be made only to
Metadata.  In MsPASS Metadata map directly into MongoDB’s document concept
of name-value pairs, while the waveform data are stored in some version of
a file. We know of two common application for a pure Metadata update
without an associated save of the waveform data.</p>
<ol class="arabic simple">
<li><p>A processing step that computes something that can be conveniently
stored as Metadata.  Examples are automated phase pickers,
amplitude measurements and assorted QC metrics.</p></li>
<li><p>Pure Metadata operations.  e.g. most reflection processing systems
have some form of generic metadata calculator of various levels of
sophistication.  The most flexible can take multiple Metadata (header)
values and use them to compute a set a different value.   Such
operations do not alter the waveform data but may require a
database update to preserve the calculation.   An example is an
active source experiment where receiver coordinates can often be
computed from survey flag numbers or some other independent counter.
In MsPASS Metadata calculations are particularly easy and thus likely
because python is used as the job control language.   (Classical seismic
reflection systems and programs like SAC use a custom interpreter.)</p></li>
</ol>
<p>Updates to data that only involve Metadata changes should obey this rule:</p>
<ul class="simple">
<li><p><strong>Update Rule 3:</strong>  Updates for Seismogram and TimeSeries object Metadata should be done
through the <code class="code docutils literal notranslate"><span class="pre">update_metadata</span></code> method of <code class="code docutils literal notranslate"><span class="pre">Database</span></code>.  Updates to
other collections should use the pymongo API.</p></li>
</ul>
<p>As noted elsewhere numerous online and printed documentation exists for MongoDB
that you should refer to when working directly with database collections.
As the rule states when you need to save the results of a pure Metadata change
within a workflow (e.g. posting a phase pick) use the <code class="code docutils literal notranslate"><span class="pre">update_metadata</span></code>
method of <code class="code docutils literal notranslate"><span class="pre">Database</span></code>.   That method has two standard arguments already
discussed above:   (1) <code class="code docutils literal notranslate"><span class="pre">mode</span></code>, and (2) <code class="code docutils literal notranslate"><span class="pre">collection</span></code>.
Three others are important for controlling the behavior of updates:</p>
<ol class="arabic simple">
<li><p><strong>ignore_metadata_changed_test</strong> is a boolean that is False by default.
We know of no example where setting this argument True in a update would
be advised (it exists as an option only to streamline create operations that
are run through the same method.).  The Metadata container does bookkeeping
that marks which, if any, key-value pairs in the container have been
altered since the data was loaded (constructed).  The <code class="code docutils literal notranslate"><span class="pre">update_metadata</span></code>
normally uses that feature to reduce the size of the update transaction by
only submitting updates for key-value pairs marked changed.   Setting this
argument True would most likely be harmless, but would also add inefficiency.</p></li>
<li><p><strong>exclude_keys</strong> is an optional list of keys for the  Metadata container that the method
should not try to update.   Use of this option is rare.   An example where it
might be useful is if some function altered a Metadata value that is known
to be incorrect.</p></li>
<li><p><strong>data_tag</strong> was discussed above for save/create operations.  When the
entire contents of a TimeSeries or Seismogram object are being saved the
tag serves as a mark for saves to distinguish those data from the
starting data or other intermediate saves.  In a pure update, however, the
meaning is different.  The data_tag argument is used any data updated
will have the associated tag in the database changed to the string
specified in the call to <code class="code docutils literal notranslate"><span class="pre">update_metadata</span></code>.  The default is to
do nothing to any existing tag (i.e. the tag is not updated).</p></li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="database_concepts.html" class="btn btn-neutral float-left" title="Database Concepts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="importing_data.html" class="btn btn-neutral float-right" title="Importing Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2021, Ian Wang.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>