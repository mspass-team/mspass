

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Database Concepts &mdash; MsPASS 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=d45e8c67"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CRUD Operations in MsPASS" href="CRUD_operations.html" />
    <link rel="prev" title="Introduction" href="introduction.html" />  

  <style>
    .wy-nav-content { max-width: 1600px; }
  </style>

  
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MsPASS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
    
              <p class="caption" role="heading"><span class="caption-text">Desktop Operation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/mspass_desktop.html">Running MsPASS on a Desktop Computer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/command_line_desktop.html">Command Line Docker Desktop Operation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/deploy_mspass_with_conda.html">Deploy MsPASS with Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/advanced_setup_considerations.html">Advanced Setup Considerations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cluster Operations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/deploy_mspass_on_HPC.html">Deploying MsPASS on an HPC cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/deploy_mspass_with_conda_and_coiled.html">Deploy MsPASS with Conda and Coiled</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started_overview.html">MsPASS Virtual Cluster Concepts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Management</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Database Concepts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#nonsql-database">NonSQL Database</a></li>
<li class="toctree-l2"><a class="reference internal" href="#schema">Schema</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#design-concepts">Design Concepts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#schema-in-mspass">Schema in MsPASS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#some-key-concepts">Some Key Concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#objectid">ObjectId</a></li>
<li class="toctree-l4"><a class="reference internal" href="#normalized-data">Normalized Data</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#waveform-processing">Waveform Processing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#concepts">Concepts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#waveform-data-storage">Waveform Data Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gridfs-storage">gridfs storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-storage">File storage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#elog">elog</a></li>
<li class="toctree-l3"><a class="reference internal" href="#history">history</a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalized-collections">Normalized collections</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#site-and-channel">site and channel</a></li>
<li class="toctree-l4"><a class="reference internal" href="#source">source</a></li>
<li class="toctree-l4"><a class="reference internal" href="#history-object-and-history-global">history_object and history_global</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id4">Summary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#preprocessing-import-collections">Preprocessing/Import collections</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#seed-and-miniseed">SEED and MiniSEED</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#customizing-the-schema">Customizing the schema</a></li>
<li class="toctree-l3"><a class="reference internal" href="#importing-data-formats-other-than-miniseed">Importing Data Formats other than miniSEED</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="CRUD_operations.html">CRUD Operations in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="mongodb_and_mspass.html">Using MongoDB with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="importing_tabular_data.html">Importing Tabular Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Seismic Data Objects</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_object_design_concepts.html">Data Object Design Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy_scipy_interface.html">Using numpy/scipy with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="obspy_interface.html">Using ObsPy with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="time_standard_constraints.html">Time Standard Constraints</a></li>
<li class="toctree-l1"><a class="reference internal" href="processing_history_concepts.html">Processing History Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="continuous_data.html">Continuous Data Handling with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="schema_choices.html">What database schema should I use?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="importing_data.html">Importing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="handling_errors.html">Handling Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_editing.html">Data Editing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cleaning_metadata.html">Cleaning Inconsistent Metadata</a></li>
<li class="toctree-l1"><a class="reference internal" href="header_math.html">Header (Metadata) Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphics.html">Graphics in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal_to_noise.html">Signal to Noise Ratio Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="arrival_time_measurement.html">Arrival Time Measurement Techniques in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="adapting_algorithms.html">Adapting an Existing Algorithm to MsPASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">System Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="parallel_processing.html">Parallel Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="memory_management.html">Memory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="io.html">I/O in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="parallel_io.html">Parallel IO in MsPASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">Frequency Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="development_strategies.html">How do I develop a new workflow from scratch?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cxx_api/index.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mspass_schema/mspass_schema.html">MsPASS Schema</a></li>
</ul>

    <a href= "../genindex.html">Index</a>
  
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MsPASS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Database Concepts</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mspass-team/mspass/blob/master/docs/source/user_manual/database_concepts.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="database-concepts">
<span id="id1"></span><h1>Database Concepts<a class="headerlink" href="#database-concepts" title="Link to this heading"></a></h1>
<section id="nonsql-database">
<h2>NonSQL Database<a class="headerlink" href="#nonsql-database" title="Link to this heading"></a></h2>
<div class="line-block">
<div class="line">MsPASS uses a NonSQL database called MongoDB.   NonSQL is a generic
name today for a database system that does not utilize the structure
query language (SQL).  SQL is the standard language for interacting
with relational database systems like Oracle, MySQL, PostGRES, etc.
One type of NonSQL database is a “document database”.  MongoDB is
classified as a document database.   Readers unfamiliar with the
concept of a document database are referred to online sources which
are far better than anything we could produce.   A good starting point
is the <a class="reference external" href="https://docs.mongodb.com/manual/introduction/">MongoDB tutorial
introduction</a>.
Another good source is <a class="reference external" href="https://www.tutorialspoint.com/mongodb/index.htm">this
one</a> on
tutorialspoint.</div>
</div>
</section>
<section id="schema">
<h2>Schema<a class="headerlink" href="#schema" title="Link to this heading"></a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h3>
<p>Wikepedia defines a database schema as follow:</p>
<div class="line-block">
<div class="line">The term “<a class="reference external" href="https://en.wiktionary.org/wiki/schema">schema</a>”
refers to the organization of data as a blueprint of how the database
is constructed (divided into database tables in the case of <a class="reference external" href="https://en.wikipedia.org/wiki/Relational_databases">relational
databases</a>)
the schema defines a set of attributes, tables (relations), and how
they can be linked (joined).</div>
</div>
<p>As this definition states, in a relational database like CSS3.0 the
schema defines a set of attributes, tables (relations), and a how they are
linked (joined).   MsPASS uses a “nonSQL database”, which means the interaction
is not with Structured Query Language (SQL).   We use a particular
form of nonSQL database called a “document database” as implemented in
the open source package <a class="reference external" href="https://www.mongodb.com/">MongoDB</a>.
The top-level concept for understanding MongoDB is name-value pairs.
One way of thinking of MongoDB is that it only implements each attribute
as a name-value pair:  the name is the key that defines the concept and
the attribute is the thing that defines a particular instance of that
concept.  The contents can
be something as simple as an integer number or as elaborate as any python
object.  Tables (relations) are synonymous with what is called a <em>collection</em>
in MongoDB.
Although a <em>collection</em> is conceptually similar to a table
it is operationally very different.  In MongoDB a <em>collection</em> contains
one or more <em>documents</em>, which play the same role as a single tuple in
a relational database.  In a relational database an attribute has one
keyword used to define the content that is visualized as a table with
a header line defining the attribute name.  A MongoDB document, in contrast,
effectively has the name tag with each entry as a MongoDB document is made
up of a set of name-value pairs.  For readers already familiar with python
the name-value pairs map exactly into the concept of a python dict.  The
python API (pymongo), in fact, retrieves documents into a data structure
that behaves exactly like a python dict.  One critical point about that
distinction is that a relational database has to define a mechanism to
flag a particular cell in a table as null.   In a MongoDB document a null
is defined a true null;   a key-value pair not defined in the document.</p>
<p>We close this section by noting that a schema is not required by
MongoDB. As we discussed in detail in <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">Data Object Design Concepts</span></a>
MsPASS data objects are implemented in C++.   Strong typing in C++
makes a schema a necessary evil to make the system more robust.
A schema also provides a necessary central location to define the
namespace of what kind of content is expected for a particular key.</p>
</section>
<section id="design-concepts">
<h3>Design Concepts<a class="headerlink" href="#design-concepts" title="Link to this heading"></a></h3>
<p>A properly designed database schema needs to prioritize the problem it
aims to solve.   The schema for MsPASS was aimed to address the
following design goals:</p>
<ol class="arabic simple">
<li><p><em>Efficient flow through Spark and DASK.</em> A key reason MongoDB was chosen as
the database engine for MsPASS was that it is cleanly integrated with
Spark and DASK.   Nonetheless, the design needs to minimize database
transaction within a workflow.   Our aim was to try to limit database
transaction to reading input data, saving intermediate results, and
saving a final result.</p></li>
<li><p><em>KISS (Keep It Simple Stupid).</em> Experience has shown clearly that
complex relational schemas like CSS3.0 have many, sometimes subtle,
issues that confound beginners.  A case in point is that large
organizations commonly have a team of database managers to maintain
the integrity of their database and optimize performance.   An
important objective of our design is to keep it simple so scientists
do not have to become database managers to work with the system.</p></li>
<li><p><em>Efficient and robust handling of three-component seismograms.</em>
Although MsPASS supports <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">scalar seismic
data,</span></a> our view is that the
greater need in the community is an efficient system for handling 3C
data.   In reality, our schema design ended up completely neutral on
this point; scalar and 3C data are handled identically.  The only
differences is what attributes (Metadata) are required for each data type.</p></li>
<li><p><em>Provide a clean mechanism to manage static metadata.</em> MsPASS is a
system designed to process a “data set”, which means the data are
preassembled, validated, and then passed into a processing chain.
The first two steps (assembly and validation) are standalone tasks
that require assembly of waveform data and a heterogenous collection
of metadata from a range of sources.   Much of that problem has been
the focus of extensive development work by IRIS and the FDSN.
Furthermore, obspy already had a well-developed, solid system
for interaction with FDSN web services.  We saw no reason to
“reinvent the wheel” and lean heavily on obspy’s web service tools
for assembling data from FDSN sources.  The MsPASS schema for
receiver metadata can, in fact, be thought of a little more than a
dialect of StationXML.   Similarly, the MsPASS schema for source
metadata can be thought of as a dialect of QuakeML.
Furthermore, because we utilized obspy’s web service tools the
python objects obspy defines for storing source and receiver metadata
are mostly echoed in the schema.</p></li>
<li><p><em>Extensible.</em> A DBMS cannot be too rigid in a research environnment,
or it will create barriers to progress.  This is especially important to MsPASS as our
objective is to produce a system for seismic research, not a
production system for repetitive processing of the similar data.
Seismic reflection processing and seismic network catalog
preparation are two examples of repetitive processing in
seismology.  In both areas traditional relational database management
systems have proven merit. A research system needs greater flexility to
handle unanticipated new ideas and approaches without starting from
scratch.  A goals was to provide a mechanism for users to extend
the database with little to no impact on the core system.</p></li>
</ol>
<div class="line-block">
<div class="line">On the other hand, we have explicitly avoided worrying about problems
we concluded were already solved.  These are:</div>
</div>
<ol class="arabic simple">
<li><p><em>Catalog preparation.</em>   At this time a primary job of most
operational seismic networks of all scales is preparation of a
catalog of seismic events and linking that information to data used
to generate the event location and source parameters.  There are
multiple commercial and government supported systems for solving
this problem.   We thus treat catalog data as an import problem.</p></li>
<li><p><em>Real time processing</em>.   Although there are elements of MsPASS that
are amenable to near real time processing of streaming data, we view
real time processing as another solved problem outside the scope of
this system.</p></li>
</ol>
</section>
</section>
<section id="schema-in-mspass">
<h2>Schema in MsPASS<a class="headerlink" href="#schema-in-mspass" title="Link to this heading"></a></h2>
<section id="id2">
<h3>Overview<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<div class="line-block">
<div class="line">We reiterate the important concept that in
MongoDB a <em>collection</em> is roughly equivalent to a table (relation)
in a relational database.  Each collection holds one or more <em>documents</em>.
A single document is roughly equivalent to a tuple in a relational database.
In this section we describe how we group documents into collections defined
in MsPASS.   These collections and the attributes they contain are the
<em>schema</em> for MsPASS.  In this section we describe how the schema of MsPASS is
defined and used to maintain the integrity of a database.
A useful feature of MsPASS is that the schema is readily
adaptable.  We defer custom schema definitions to a section in “Advanced
Topics”.</div>
</div>
</section>
<section id="some-key-concepts">
<h3>Some Key Concepts<a class="headerlink" href="#some-key-concepts" title="Link to this heading"></a></h3>
<section id="objectid">
<h4>ObjectId<a class="headerlink" href="#objectid" title="Link to this heading"></a></h4>
<p>MongoDB collections always utilize a unique identifier they call an
<code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> to provide a bombproof, unique identifier for a single document
in a collection.  MongoDB automatically generates one id with the special
name <code class="code docutils literal notranslate"><span class="pre">_id</span></code> whenever a new document is added to a collection.   An important
thing to realize is two absolutely identical documents, which can readily
be saved from a python dict or our Metadata container, can be saved to
a collection and they will be treated as different because they will each
get a different <code class="code docutils literal notranslate"><span class="pre">_id</span></code> assigned.   That is good or bad depending on the
perspective.  It can be bad in an application where duplicates
create a problem, but we assert that for most data processing it is
a good thing.  We contrast this with the experience we have had with relational
databases where a job can abort on a write because of a duplicate
database key problem.  That never happens with MongoDB, but the flip side
of the coin is it is very easy to unintentionally save pure duplicates.</p>
<p>Because ObjectIds are guaranteed to be unique we use them extensively inside
MsPASS to provide indices and especially as a tool to create cross-references
to common data like station and source Metadata.</p>
<p>ObjectIds are stored in MongoDB as a binary object we normally store in
its raw form using pymongo.  Users should be aware that a human readable
for can be obtain in python by using the str attribute of ObjectId class.  (i.e. if
<code class="code docutils literal notranslate"><span class="pre">myid</span></code> is an ObjectId loaded from MongoDB, the readable form is <code class="code docutils literal notranslate"><span class="pre">myid.str</span></code>)
For more on ObjectIds the following site is a good <a class="reference external" href="https://www.tutorialspoint.com/mongodb/mongodb_objectid.htm">introduction</a>.</p>
</section>
<section id="normalized-data">
<h4>Normalized Data<a class="headerlink" href="#normalized-data" title="Link to this heading"></a></h4>
<p>When we started this development we planned to create a purely flat
Metadata space through what MongoDB calls an <em>embedded data model</em>.
As we gained experience on the system, however, we realized all seismology
Metadata was better suited to make more use of what MongoDB documentation
calls a <em>normalized data model</em>.  The generic concepts these terms
describe can be found <a class="reference external" href="https://www.tutorialspoint.com/mongodb/mongodb_data_modeling.htm">here</a>.</p>
<p>At this time there are three sets of Metadata we handle by normalization.
They are familiar concepts to anyone familiar with the relational database
schema CSS3.0 used, for example, in Antelope.  The concepts involved are:</p>
<ul class="simple">
<li><p><em>Station (instrument) related Metadata.</em>   These are described below and actually
define two collections with the names <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code>.  The
distinctions are a bit subtle and better left to the more detailed
discussion below.</p></li>
<li><p><em>Source related Metadata.</em>   Any event driven processing needs information
about seismic sources that are associated with the signals to be
analyzed.  That data is stored in this collection.</p></li>
</ul>
<p>A common feature of all “normalized” collection data is that they define a
subset of data that is are shared by many waveforms.  In that situation it
is more efficient in both storage and database maintenance to keep the
related data together.  Readers familiar with relational systems
understand this same concept as our site, channel, and source collections
are similar to the CSS3.0 site, sitechan, and origin tables respectively.</p>
<p>A key feature of normalized data is we need a fast index to link the
normalized data to our waveform data.  In all cases we use the ObjectId of
the normalized collection as the index.   As noted above all documents in
MongoDB automatically are assigned an ObjectId accessible with key
<code class="code docutils literal notranslate"><span class="pre">_id</span></code>.  For all normalized Metadata we use a convention wherein we
store the ObjectId of a related document in another collection using
a composite key name constructed as <code class="code docutils literal notranslate"><span class="pre">collection_id</span></code>, where <code class="code docutils literal notranslate"><span class="pre">collection</span></code>
is the name of the collection and <code class="code docutils literal notranslate"><span class="pre">_id</span></code> is a literal meant to imply
an ObjectId normally accessible through the “_id” key.   For example,
we use <code class="code docutils literal notranslate"><span class="pre">site_id</span></code> to refer to documents in the <code class="code docutils literal notranslate"><span class="pre">site</span></code> collection.
That means that when <code class="code docutils literal notranslate"><span class="pre">site_id</span></code> appears in another collection it is a
reference to the ObjectId (referenced directly with alternate key <code class="code docutils literal notranslate"><span class="pre">_id</span></code>
in the site collection) of the related document in the <code class="code docutils literal notranslate"><span class="pre">site</span></code> collection.</p>
<p>The major motivation for using the normalized data model for handling
source and receiver metadata is the data involved have two important
properties.   First, since MsPASS was designed as a system for efficiently
handling an assembled data set, the data these collections can be treated
as static (immutable) within a workflow.   Waveform data readers must thus do
what is MongoDB’s version of a database join between the waveform collection
and one or more of the normalizing collections.   Second, in every case
we know the source and receiver metadata are small compared to any
data set for which one would need to use the parallel processing machinery
of MsPASS.  That means the time to query the normalizing collections is
always expected to be much smaller than the time to query a waveform collection that often
has millions of documents. Although experience showed that expectation was
true, we also found there are situations where embedded database operations
can be a bottleneck in a workflow.   For that reason we developed a set of
normalization classes in python that cache tables of attributes needed for
normalization.   That idea is described below in the
<a class="reference internal" href="normalization.html#normalization"><span class="std std-ref">Normalization</span></a> section.</p>
</section>
</section>
<section id="waveform-processing">
<h3>Waveform Processing<a class="headerlink" href="#waveform-processing" title="Link to this heading"></a></h3>
<section id="concepts">
<h4>Concepts<a class="headerlink" href="#concepts" title="Link to this heading"></a></h4>
<p>A first-order concept in our database design is that a processing workflows
should driven by one primary collection.  We emphasize that idea by
stating this rule:</p>
<blockquote>
<div><dl class="simple">
<dt>Rule 1:</dt><dd><p>Before running any workflow the input waveform collection
must be populated to define all Metadata required to run the workflow.</p>
</dd>
</dl>
</div></blockquote>
<p>That means there is normally a significant <em>preprocessing</em> effort
required to prep the dataset.  Existing tools to aid this process are
currently available in the modules found under <cite>mspasspy.preprocessing</cite>.
We stress, however, that preparing data for processing gets increasingly
complicated as the size of a dataset grows as the probability of an
unanticipated data problem increase with the size of a dataset.  Never underestimate the
universal concept of <a class="reference external" href="https://www.dictionary.com/browse/murphy-s-law">Murphy’s Law</a>.
Although at this writing the functionality is only planned, an
essential tool is to run a verification tool to validate data before running
a large job.</p>
<p>With that background, there are two collections used to manage waveform data.
They are called <code class="code docutils literal notranslate"><span class="pre">wf_TimeSeries</span></code> and <code class="code docutils literal notranslate"><span class="pre">wf_Seismogram</span></code>.
These two collection are the primary work areas to assemble a working data set.
We elected to keep data describing each of the two atomic data types in MsPASS,
<a class="reference internal" href="../python_api/mspasspy.ccore.html#mspasspy.ccore.seismic.TimeSeries" title="mspasspy.ccore.seismic.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>
and <a class="reference internal" href="../python_api/mspasspy.ccore.html#mspasspy.ccore.seismic.Seismogram" title="mspasspy.ccore.seismic.Seismogram"><code class="xref py py-class docutils literal notranslate"><span class="pre">Seismogram</span></code></a>,
in two different collections.  The
main reason we made the decision to create two collections instead of one
is that there are some minor differences in the Metadata that would
create inefficiencies if we mixed the two data types in one place.
If an algorithm needs to have inputs of both TimeSeries and Seismogram
objects (e.g. array deconvolution where a TimeSeries defines the source
wavelet and the data to be deconvolved are Seismogram object) it can still
be handled, but the queries can actually happen faster because they
can be issue against two smaller sets.</p>
<p>The key point about the use of the wf collections is that all serial processing
can be reduced to this pseudocode logic:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">)</span> <span class="n">Create</span> <span class="n">database</span> <span class="n">handle</span>
<span class="mi">2</span><span class="p">)</span> <span class="n">Point</span> <span class="n">the</span> <span class="n">handle</span> <span class="n">at</span> <span class="n">wf_Seismogram</span> <span class="ow">or</span> <span class="n">wf_TimeSeries</span> <span class="k">as</span> <span class="n">appropriate</span>
<span class="mi">3</span><span class="p">)</span> <span class="n">Create</span> <span class="n">a</span> <span class="n">MongoDB</span> <span class="n">cursor</span> <span class="p">(</span><span class="n">find</span> <span class="nb">all</span> <span class="ow">or</span> <span class="n">issue</span> <span class="n">a</span> <span class="n">query</span><span class="p">)</span>
<span class="mi">4</span><span class="p">)</span> <span class="n">foreach</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
    <span class="mi">1</span><span class="n">i</span><span class="p">)</span>  <span class="n">Run</span> <span class="n">a</span> <span class="n">sequnce</span> <span class="n">of</span> <span class="n">functions</span> <span class="n">on</span> <span class="n">x</span>
    <span class="mi">2</span><span class="n">i</span><span class="p">)</span>  <span class="n">Save</span> <span class="n">the</span> <span class="n">result</span>
</pre></div>
</div>
<p>Parallel jobs are very similar but require creation of an RDD or Dask bag
to drive the processing.  Our parallel api, described elsewhere (LINK)
simplifies the conversion from a serial to parallel job.  In any case,
the equivalent parallel pseudocode logic is this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span><span class="p">)</span> <span class="n">Create</span> <span class="n">database</span> <span class="n">handle</span>
<span class="mi">2</span><span class="p">)</span> <span class="n">Point</span> <span class="n">the</span> <span class="n">handle</span> <span class="n">at</span> <span class="n">wf_Seismogram</span> <span class="ow">or</span> <span class="n">wf_TimeSeries</span> <span class="k">as</span> <span class="n">appropriate</span>
<span class="mi">3</span><span class="p">)</span> <span class="n">Run</span> <span class="n">the</span> <span class="n">Database</span><span class="o">.</span><span class="n">read_distributed_data</span> <span class="n">method</span> <span class="n">to</span> <span class="n">build</span> <span class="n">parallel</span> <span class="n">dataset</span>
<span class="mi">4</span><span class="p">)</span> <span class="n">Run</span> <span class="n">parallel</span> <span class="n">version</span> <span class="n">of</span> <span class="n">each</span> <span class="n">processing</span> <span class="n">function</span>
<span class="mi">5</span><span class="p">)</span> <span class="n">Run</span> <span class="n">Database</span><span class="o">.</span><span class="n">save_distributed_data</span> <span class="n">method</span>
</pre></div>
</div>
<p>A simple perspective on the difference is that the loop for the serial
job becomes is implied in the parallel job.  Spark schedules which
datum is run through which of a set of parallel jobs.</p>
</section>
</section>
<section id="waveform-data-storage">
<h3>Waveform Data Storage<a class="headerlink" href="#waveform-data-storage" title="Link to this heading"></a></h3>
<section id="id3">
<h4>Overview<a class="headerlink" href="#id3" title="Link to this heading"></a></h4>
<p>All seismogram read operations access one of the wf Collections.
The default behavior is to read all key-value pairs in a single document
and insert most of the attributes into the Metadata for one
TimeSeries or Seismogram objects.  Normalized data (see above) are
loaded automatically by default.</p>
<p>Writers are more complicated because they may have to deal with any
newly generated attributes and potentially fundamental changes in the
nature of the waveform we want to index.  <em>e.g.</em>, a stack can become
completely inconsistent with the concept of a station name and may
require creation of a different set of attributes like a point
in space to define what it is.  If the concept matches an existing
schema attribute that existing key should be used.  If not, the user
can and should define their own attribute that will automatically be saved.
The only limitation is that if the key is not defined in the wf schema
the automatic type conversions will not be feasible.  Similarly, NEVER EVER
write a new attribute to an datum’s Metadata if the key is already defined
in the schema.  Doing so will guarantee downstream problems.</p>
<p>Users must also realize that the sample data in Seismogram or TimeSeries objects
can be constructed from <code class="code docutils literal notranslate"><span class="pre">wf</span></code> documents in one of two ways.  First, the sample data
can be stored in the more conventional method of CSS3.0 based systems
as external files.   In this case, we use the same construct as CSS3.0 where
the correct information is defined by three attribures:  <code class="code docutils literal notranslate"><span class="pre">dir</span></code>, <code class="code docutils literal notranslate"><span class="pre">dfile</span></code>, and
<code class="code docutils literal notranslate"><span class="pre">foff</span></code>.   Unlike CSS3.0 MsPASS currently requires external file data to be
stored as native 64 bit floating point numbers.   We force that restriction
for efficiency as the <code class="code docutils literal notranslate"><span class="pre">Seismogram.data</span></code> array and the <code class="code docutils literal notranslate"><span class="pre">TimeSeries.data</span></code>
vector can then be read and written with fread and fwrite respectively from
the raw buffers.  The alternative (second) method for storing sample data
in MsPASS is through a mechanism called <code class="code docutils literal notranslate"><span class="pre">gridfs</span></code> in MongoDB.  When this
method is used the waveform sample data are managed
by file system like handles inside MongoDB.  That process is largely hidden
from the user, but there are two important things to recognize about
these two models for data storage:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The <code class="code docutils literal notranslate"><span class="pre">gridfs</span></code> method is expected to be superior to file storage for
large clusters because it facilitates parallel io operations.  With
files two processes can collide trying access a common file, especially
with a writer.</p></li>
<li><p>A limitation of gridfs is that the sample data are stored in the same
disk area where MongoDB stores it’s other data.  This can be a
limitation for system configurations that do not contain a modern
large virtual file system or any system without a single disk
file system able to store the entire data set and any completed results.</p></li>
</ol>
</div></blockquote>
</section>
<section id="gridfs-storage">
<h4>gridfs storage<a class="headerlink" href="#gridfs-storage" title="Link to this heading"></a></h4>
<p>When data are saved to gridfs, MongoDB will automatically create two
collections it uses to maintain the integrity of the data stored there.
They are called <code class="code docutils literal notranslate"><span class="pre">fs.files</span></code> and <code class="code docutils literal notranslate"><span class="pre">fs.chunks</span></code>.   Any book on MongoDB and
any complete online source will discuss details of gridfs and these
two collections.  A useful example is this <a class="reference external" href="https://www.tutorialspoint.com/mongodb/mongodb_gridfs.htm">tutorial</a>.</p>
<blockquote>
<div></div></blockquote>
<p>You as a user do will not normally need to interact with these collections
directly.   The database readers and writers handle the bookkeeping
for you by maintaining an index in either of the wf collections to
link to the gridfs collections.   Cross-referencing ids and special
attributes are defined in the schema documentation.</p>
</section>
<section id="file-storage">
<h4>File storage<a class="headerlink" href="#file-storage" title="Link to this heading"></a></h4>
<p>The alternative storage model is external files.  We use the same
concepts to manage data in external files as CSS3.0.  Data in file
storage is managed by four attributes:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">dir</span></code> a directory path identifier in a file system.  We assume all
users are familiar with this concept.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">dfile</span></code> the “file name” that defines the leaf of the directory (path)
tree structure.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">foff</span></code> is a byte offset to the start of the data of interest.
Classic earthquake data formats like SAC do not use this concept and
put only one seismogram in each file.  Multiple objects can be stored
in a single file using common dir and dfile fields but different foff
values.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">nbytes</span></code> or <code class="code docutils literal notranslate"><span class="pre">npts</span></code> are attributes closely related to <code class="code docutils literal notranslate"><span class="pre">foff</span></code>.   They
define the size of the block of data that needs to be read from the
position of <code class="code docutils literal notranslate"><span class="pre">foff</span></code>.</p></li>
</ol>
</div></blockquote>
<p>Both TimeSeries and Seismograms use a data array that is a contiguous
memory block.  The default storage mode for external files is a raw
binary memory image saved by writing the memory buffer to the external
file (defined by <code class="code docutils literal notranslate"><span class="pre">dir</span></code> and <code class="code docutils literal notranslate"><span class="pre">dfile</span></code>) using the low level C fwrite function
that is wrapped in the python standard by the <code class="code docutils literal notranslate"><span class="pre">write</span></code> method of
standard file handles described in many tutorials like this <a class="reference external" href="https://docs.python.org/3/tutorial/inputoutput.html).">one</a>.</p>
<blockquote>
<div></div></blockquote>
<p>TimeSeries stores data as vector of binary “double” values, which for
decades now has implied an 8 byte floating point number stored in the IEEE
format.  (Note historically that was not true.   In the early days of
computers there were major differences in binary representations of
real numbers.   We make an assumption in MsPASS that the machines in the
cluster used for processing have the same architecture and a doubles are
idenitical on all machines.)  Similarly, a Seismogram stores data in a
contiguous buffer of memory but the memory block is 3 x <code class="code docutils literal notranslate"><span class="pre">npts</span></code> doubles.
The buffer is order in what numpy calls FORTRAN order meaning the matrix is
stored with the row index fastest (also called column order).  In any case,
key point is that for efficiency the data for a Seismogram is also
read and written using low level binary <code class="code docutils literal notranslate"><span class="pre">read</span></code> and <code class="code docutils literal notranslate"><span class="pre">write</span></code> methods of the
python file handle class.</p>
</section>
<section id="summary">
<h4>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h4>
<p>The main idea you as a user will need to understand is that a single
document in one of the wf collections contains all the information
needed to reconstruct the object (the read operation) that is the
same as that saved there previously (the save operation).  The
name-value pairs of each document stored in a wf collection are either
loaded directly as Metadata or used internally to load other Metadata
attributes or to guide readers for the sample data.   Readers
handle which storage model to use automatically.</p>
<p>Writers create documents in a wf collection that allow you to recreate the
saved data with a reader.  The write process has some complexities
a reader does not have to deal with.   That is, writers have more options
to deal with (notably the storage mode) that control their behavior and
have to handle potential inconsistencies created by a processing
workflow.  The <code class="code docutils literal notranslate"><span class="pre">Schema</span></code> class (described in more detail below) manages
automatically mapping Metadata to database attributes where possible.
To avoid fatal write errors we emphasize the following as a rule:</p>
<blockquote>
<div><dl class="simple">
<dt>Rule 2:</dt><dd><p>Make sure any custom Metadata keys do not match existing schema keys.
If change the meaning or data  type stored with that key,
you can create any range of downstream problems and could abort the
final save of your results.</p>
</dd>
</dl>
</div></blockquote>
</section>
</section>
<section id="elog">
<h3>elog<a class="headerlink" href="#elog" title="Link to this heading"></a></h3>
<p>The elog collection holds log messages that should
automatically be posted and saved in a MsPASS workflow.  The elog
collection saves any entries in ErrorLogger objects that are
contain in both Seismogram and TimeSeries objects.   The
main idea of an ErrorLogger is a mechanism to post errors of any level
of severity to the data with which the error is associated, preserve a
record that can be used by the user to debug the problem, and allow
the entire job to run to completion even if the error made the data
invalid.  More details about this idea can be found in the <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">Data
Objects</span></a> section.</p>
<p>A special case is data killed during processing.   Any datum from a MsPASS
processing module that was killed should contain an elog entry that the
level <code class="code docutils literal notranslate"><span class="pre">Invalid</span></code>.   The sample data in killed Seismogram or TimeSeries data
is not guaranteed to be valid, and may, in fact, be empty.   Hence, killed
data have to be handled specially.   All elog entries from such data will
be preserved in this collection.   In addition, the document for killed
data will contain a dict container with the key “metadata”.   That dict is
an recasting of the Metadata of the datum that was killed.  It is neeed,
in general, to sort out what specific datum to which the error was attached.
The documents in elog for live data contain an <code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> that is a link back
to the wf collection where that waveform was saved.</p>
</section>
<section id="history">
<h3>history<a class="headerlink" href="#history" title="Link to this heading"></a></h3>
<p>An important requirement to create a reproducible result from
data is a mechanism to create a full history that can be used to recreate
a workflow.  The same mechanism provides a way for you to know the sequence
of processing algorithms that have been applied with what tunable parameters
to produce results stored in the database.  The history collection stores this
information.   Most users should never need to interact directly with this
collection so we omit any details of the history collection contents from
this manual.  Users should, however, understand the concepts described
in - link to new document in this manual on ProcessingHistory concepts –</p>
<p>TODO:  Leaving this section for now.  Needs to use the figure used in
our AGU poster.  Main reason to punt for now is to needs to include a
clear description of how the global and object level history interact.
Global is under development at this writing.</p>
</section>
<section id="normalized-collections">
<h3>Normalized collections<a class="headerlink" href="#normalized-collections" title="Link to this heading"></a></h3>
<section id="site-and-channel">
<h4>site and channel<a class="headerlink" href="#site-and-channel" title="Link to this heading"></a></h4>
<p>The <code class="code docutils literal notranslate"><span class="pre">site</span></code> collection is intended as a largely static table
that can be used to
<a class="reference external" href="https://docs.mongodb.com/manual/core/data-model-design/">normalize</a>
a wf collection.   The name is (intentionally) identical to the CSS3.0
site table.   It’s role is similar, but not identical to the CSS3.0
table.  Similarly, <code class="code docutils literal notranslate"><span class="pre">channel</span></code> plays the same role as the <code class="code docutils literal notranslate"><span class="pre">sitechan</span></code>
table in CSS3.0.  They are similar in the sense that <code class="code docutils literal notranslate"><span class="pre">site</span></code> is
used to find the spatial location of a recording instrument.
In the same way <code class="code docutils literal notranslate"><span class="pre">channel</span></code> acts like <code class="code docutils literal notranslate"><span class="pre">sitechan</span></code> in that it is used
to define the orientation of a particular single channel of seismic
data.   Both collections, however, mix in concepts CSS3.0 stores
in a collection of static tables used for maintaining station metadata.
Antelope users will know these as the collection of tables generated
when <a class="reference external" href="https://brtt.com">sd2db</a> is run on a SEED file from an FDSN
data center.  We expand on this below, but the following are useful
summaries for Antelope and obspy users:</p>
<ul class="simple">
<li><p>Antelope user’s should think of the channel collection as nearly identical
to the CSS3.0 sitechan table with response data handled through obspy.</p></li>
<li><p>Obspy users can think of both <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">sitechan</span></code> as a way to
manage the same information obspy handles with their
<a class="reference external" href="https://docs.obspy.org/packages/autogen/obspy.core.inventory.inventory.Inventory.html">Inventory</a>
object.  In fact, channel documents produced from
<a class="reference external" href="https://www.fdsn.org/xml/station/">StationXML</a>
files contain an image of an obspy
<a class="reference external" href="https://docs.obspy.org/packages/autogen/obspy.core.inventory.channel.Channel.htmlobject">Channel</a>
object saved with pickle.</p></li>
</ul>
<p>We emphasize that <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code> support SEED indexed metadata, but
they do not demand it.  We use the <code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> of documents in both
collections as the primary cross-referencing key.  The <code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> keys are
referenced in collections outside of <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code>
(i.e. wf_TimeSeries and wf_Seismogram) with the keys <code class="code docutils literal notranslate"><span class="pre">site_id</span></code> and <code class="code docutils literal notranslate"><span class="pre">chan_id</span></code>
respectively.</p>
<p>Although those <code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> can be thought of as primary keys, we provide
some support for two alternative indexing methods.</p>
<blockquote>
<div><ul class="simple">
<li><p><em>SEED net, sta, chan, loc keys</em>.  Any data obtained from FDSN
data centers like IRIS-DMC distribute data in the SEED
(Standard for the Exchange of Earthquake Data) or miniSEED
format.  MiniSEED data is SEED data with minimal metadata.
The primary keys SEED uses to define a specfic channel of data are
three string attributes: (1) a network code referred to as <code class="code docutils literal notranslate"><span class="pre">net</span></code> in
MsPASS, (2) a station code (<code class="code docutils literal notranslate"><span class="pre">sta</span></code>), (3) a channel (<code class="code docutils literal notranslate"><span class="pre">chan</span></code>), and
a “location” code (<code class="code docutils literal notranslate"><span class="pre">loc</span></code>).   <code class="code docutils literal notranslate"><span class="pre">site</span></code> documents extracted from StationXML
files will always contain <code class="code docutils literal notranslate"><span class="pre">net</span></code>, <code class="code docutils literal notranslate"><span class="pre">sta</span></code>, and <code class="code docutils literal notranslate"><span class="pre">loc</span></code> names while
<code class="code docutils literal notranslate"><span class="pre">channel</span></code> documents add the <code class="code docutils literal notranslate"><span class="pre">chan</span></code> attibute.  For documents generated
from StationXML keys (3 keys for <code class="code docutils literal notranslate"><span class="pre">site</span></code> and 4 for <code class="code docutils literal notranslate"><span class="pre">channel</span></code>) can
be properly viewed as alternate keys to locate documents related to a
particular station (<code class="code docutils literal notranslate"><span class="pre">site</span></code>) or channel (<code class="code docutils literal notranslate"><span class="pre">channel</span></code>).  With SEED data it
is important to realize that those keys are frequently not sufficient
to locate a single document.  All SEED-based data (StationXML) also
use a pair of time range attributes that we call <code class="code docutils literal notranslate"><span class="pre">starttime</span></code> and
<code class="code docutils literal notranslate"><span class="pre">endtime</span></code>.   Both are unix epoch times that define a time span for which
the associated document’s data are valid.   These are used for a whole
range of practical issues in recording of continuous data, but the
key point is any query for a unique document in both the <code class="code docutils literal notranslate"><span class="pre">site</span></code> and
<code class="code docutils literal notranslate"><span class="pre">channel</span></code> collection require a time stamp that needs to be tested
against a time range defined by <code class="code docutils literal notranslate"><span class="pre">starttime</span></code> and <code class="code docutils literal notranslate"><span class="pre">endtime</span></code>.</p></li>
<li><p>We also provide some limited support for a form of spatial query.
The use of a spatial query was a design decision based
on the author’s experiences using CSS3.0’s site table as implemented
in Antelope.   Antelope uses the station name and a time period as a
key to find location information for a waveform.   That model works
well for bulletin preparation but creates a dilemma for processed
waveforms;  the concept of a “station name” is meaningless for many
types of processed waveform.  Two type examples, are a phased array
beam and Common Conversion Point (CCP) stacks of receiver functions.
On the other hand, many such processed waveforms have a space concept
that needs to be preserved.  Hence, the location information in the
collection may relate to some more abstract point like  piercing point
for a CCP stack.   In this mode the <code class="code docutils literal notranslate"><span class="pre">Object_Id</span></code> stored as <code class="code docutils literal notranslate"><span class="pre">site_id</span></code>
or <code class="code docutils literal notranslate"><span class="pre">chan_id</span></code> is the only index. The difference is geospatial queries
in MongoDB can be used as an alternate index.  We note that
geospatial queries can also be used on <code class="code docutils literal notranslate"><span class="pre">site</span></code> and <code class="code docutils literal notranslate"><span class="pre">channel</span></code> collections
created with StationXML files too provided the user constructs the
index with (NEEDS A REFERERENCE HERE - We need a database method for this)</p></li>
</ul>
</div></blockquote>
<p>A spatial query to link anything to a point in the <code class="code docutils literal notranslate"><span class="pre">site</span></code> or <code class="code docutils literal notranslate"><span class="pre">channel</span></code> collection has
two complexities:  (1) all spatial queries require a uncertainty
specification that are data and implementation dependent, and (2)
sometimes, but not always, a vertical position (site_elev) needs to be
defined.  The first is readily solved with the geospatial indexing
capabilities of MongoDB.   Geospatial queries can define a radius of
uncertainty to efficiently find one or more documents linked to a
circle defined relative to a query point.  The size of such a circle
is always a data dependent choice;  a scientist working with free
oscillations of the earth require station coordinates with minimal
precision, while an active source experiment often requires submeter
location precision.   We treat vertical positions differently.  The
common key to define vertical position is <code class="code docutils literal notranslate"><span class="pre">site_elev</span></code> or <code class="code docutils literal notranslate"><span class="pre">chan_elev</span></code>.
How to handle
vertical position is application dependent.  <em>e.g.</em> to look up the
location of an operational GSN station, it may be necessary to
distinguish borehole and vault instruments that are deployed at many
stations.   In contrast, a point defined by piercing points for a CCP
stack would normally be assumed referenced to a common, fixed depth so
site_elev may not even be needed.  We deal with this complexity by a
defining another rule that user’s need to recognize and abide by:</p>
<blockquote>
<div><dl class="simple">
<dt>Rule 3:</dt><dd><p>The site and channel collections should only contain metadata relevant to
the data set.   Used documents are not a problem but waste space.
Missing metadata is a problem as it will always lead to dropped data.
Assembly of a working data set requires linking documents in <code class="code docutils literal notranslate"><span class="pre">site</span></code>
and/or <code class="code docutils literal notranslate"><span class="pre">channel</span></code> to wf_Seismogram documents and channel to wf_TimeSeries
using keys <code class="code docutils literal notranslate"><span class="pre">site_id</span></code> and <code class="code docutils literal notranslate"><span class="pre">chan_id</span></code> respectively.</p>
</dd>
</dl>
</div></blockquote>
<p>MsPASS has some supported functions to add in building the site and channel
collections and building links to wf collections.   The details are best
obtained from the docstrings for functions in <code class="code docutils literal notranslate"><span class="pre">mspasspy.db.database</span></code> and
<code class="code docutils literal notranslate"><span class="pre">mspass.preprocessing.seed</span></code> and tutorials on raw data handling.</p>
<p>As noted earlier <code class="code docutils literal notranslate"><span class="pre">site</span></code> is a near match in concept to the css3.0 table
with the same name, but <code class="code docutils literal notranslate"><span class="pre">channel</span></code> is is more than its closes analog in
css3.0 called sitechan.   The key difference between <code class="code docutils literal notranslate"><span class="pre">channel</span></code> and sitechan
is that <code class="code docutils literal notranslate"><span class="pre">channel</span></code> contains not just orientation information, but <strong>may</strong>
contain all the metadata needed to define the response characteristics of the
channel to which it is linked.  We stress <strong>may</strong> because for a generic
processing system response information must be optional.   Traditional reflection
processing has, at best, only limited response information (e.g. the
sensor corner frequency is an optional parameter in SEGY) and a large fraction of
processing functions have not need for detailed response data.  In contrast,
some common applications like moment tensor inversions and surface wave dispersion
measurements demand detailed response metadata.   We address this problem
by leaning heavily on the existing infrastructure for handling response data
in obspy.   That is, obspy defines a python class they call <code class="code docutils literal notranslate"><span class="pre">Inventory</span></code>.
The <code class="code docutils literal notranslate"><span class="pre">Inventory</span></code> class is a complicated data structure that is best thought of,
in fact, as a image of the data structure defined by an FDSN StationXML file.
Embedded in that mess is the response data, but obspy has build a clean
api to obtain the response information from the <code class="code docutils literal notranslate"><span class="pre">Inventory</span></code>.   In MsPASS
we handle this problem by storing a pickle image of the <code class="code docutils literal notranslate"><span class="pre">Inventory</span></code> object
related to that channel.   (TO DO:   our current implementation may not
be correct on this point.  see discussion)</p>
<p>Finally, we emphasize that if your final processing workflow requires
metadata in <code class="code docutils literal notranslate"><span class="pre">site</span></code> and/or <code class="code docutils literal notranslate"><span class="pre">channel</span></code> you must complete preprocessing to
define linking ids in wf_Seismogram and/or wf_TimeSeries.  Any incomplete
entries will be dropped in final processing.  Conversely, if your workflow
does not require any receiver related Metadata (rare), these collections
do not need to be dealt with at all.</p>
</section>
<section id="source">
<h4>source<a class="headerlink" href="#source" title="Link to this heading"></a></h4>
<p>The source collection has much in common with site, but
has two fundamental differences:  (1) the origin time of each source
needs to be specified, and (2) multiple estimates are frequently
available for the same source.</p>
<p>The origin time issue is a more multifaceted problem that it might at
first appear.  The first is that MongoDB, like ArcGIS, is map-centric
and stock geospatial queries lack a depth attribute, let alone a time
variable.   Hence, associating a waveform to a source position defined
in terms of hypocenter coordinates (<code class="code docutils literal notranslate"><span class="pre">latitude</span></code>, <code class="code docutils literal notranslate"><span class="pre">longitude</span></code>,
<code class="code docutils literal notranslate"><span class="pre">depth</span></code>, and <code class="code docutils literal notranslate"><span class="pre">time</span></code> attributes in <code class="code docutils literal notranslate"><span class="pre">source</span></code>) requires a multistage query that can
potentially be very slow for a large data set.</p>
<p>The other issue that distinguishes origin time is that it’s accuracy
is data dependent.   With earthquakes it is always estimated by an
earthquake location algorithm, while with active source it normally
measured directly.  The complexity with active source data is a
classic case distinguishing “precision” from “accuracy”.   Active
source times relative to the start time of a seismogram may be very
precise but not accurate.  A type example is multichannel data where
time 0 of each seismogram is defined by the shot time, but the
absolute time linked to that shot may be poorly constrained.   We
address this problem in MsPASS through the concept of UTC versus
“Relative” time definined in all MsPASS data objects.  See the <a class="reference internal" href="data_object_design_concepts.html#data-object-design-concepts"><span class="std std-ref">Data
Object section</span></a> on BasicTimeSeries
for more on this topic.</p>
<p>A final point about the source table is the issue of multiple
estimates of the same event.   The CSS3.0 has an elaborate mechanism
for dealing with this issue involving three closely related tables
(relations):  event, origin, assoc, and arrival.   The approach we
take in MsPASS is to treat that issue as somebody else’s problem.
Thus, for the same reason as above we state rule 3 which is very
similar to rule 2:</p>
<blockquote>
<div><dl class="simple">
<dt>Rule 4:</dt><dd><p>The source collection should contain any useful source
positions that define locations in space and time (attributes
<code class="code docutils literal notranslate"><span class="pre">source_lat</span></code>, <code class="code docutils literal notranslate"><span class="pre">source_lon</span></code>, <code class="code docutils literal notranslate"><span class="pre">source_depth</span></code>, and <code class="code docutils literal notranslate"><span class="pre">source_time</span></code>).  Linking
each document in a wf collection to the desired point in the source
collection is a preprocessing step to define a valid dataset.
The link should always be done with by inserting the <code class="code docutils literal notranslate"><span class="pre">ObjectId</span></code> of
the appropriate document in <code class="code docutils literal notranslate"><span class="pre">source</span></code> as in wf_Seismogram or
wf_TimeSeries with the key <code class="code docutils literal notranslate"><span class="pre">source_id</span></code>.</p>
</dd>
</dl>
</div></blockquote>
<p>A first-order limitation this imposes on MsPASS is that it means that
normal behavior should be that there is a one-to-one mapping of a single
<code class="code docutils literal notranslate"><span class="pre">source</span></code> document to a given wf document as defined by the <code class="code docutils literal notranslate"><span class="pre">source_id</span></code> key.
Note MongoDB is flexible enough that it would be possible to support
multiple event location estimates for each wf document but that is not
a feature we have elected to support.  As noted other places we consider the
catalog preparation problem a solved problem with multiple solutions.</p>
<p>A final point about <code class="code docutils literal notranslate"><span class="pre">source</span></code> is that we emphasize normalizing <code class="code docutils literal notranslate"><span class="pre">source</span></code>
by defining <code class="code docutils literal notranslate"><span class="pre">source_id</span></code> values in wf collections should always be thought of
as an (optional) preprocessing step.   If your workflow requires source
information, you must complete the association of records in source to
wf_Seismogram and/or wf_TimeSeries documents before your primary processing.
Any entries not associated will be dropped.</p>
</section>
<section id="history-object-and-history-global">
<h4>history_object and history_global<a class="headerlink" href="#history-object-and-history-global" title="Link to this heading"></a></h4>
<p>An important requirement to create a reproducible result from
data is a mechanism to create a full history that can be used to recreate
a workflow.  The same mechanism provides a way for you to know the sequence
of processing algorithms that have been applied with what tunable parameters
to produce results stored in the database.  The history collections stores this
information.   Most users should never need to interact directly with this
collection so we defer any details of how these are stored and managed to
the reference manual.   The assumption you as a reader need to understand is
that the default behavior of all MsPASS modules is to not preserve history.
The idea is that when you need to retain that information you would rerun
the workflow with history saving enabled for each processing step.
Examples where this might be needed are preparing a final dataset to link to
a publication or as an archive you expect to need to work with at a later date.</p>
</section>
</section>
</section>
<section id="id4">
<h2>Summary<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<p>The details above may obscure a few critical points about what the
database in MsPASS does for you and what you must keep in mind to use
is correctly.</p>
<ul class="simple">
<li><p>All parallel workflows should normally be driven by data assembled into
the wf_TimeSeries and/or wf_Seismogram collections.  Subsets (or all) of
one of these collections define a parallel dataset that is the
required input for any parallel job.</p></li>
<li><p>The Database api simplifies the processing of reading and writing.
We abstract the always complex process of reading and writing to <code class="code docutils literal notranslate"><span class="pre">save</span></code> and
<code class="code docutils literal notranslate"><span class="pre">read</span></code> methods of the python class Database.  See the reference manual
for details.</p></li>
<li><p>Assembling the wf_Seismogram and/or wf_TimeSeries collection should
always be viewed as a preprocessing step to build a clean dataset.  That
model is essential for efficiency because all the complexity of real
data problems cannot be anticipated and are best treated as a special
problem you as a user are responsible for solving.</p></li>
<li><p>Assembling the metadata stored in <code class="code docutils literal notranslate"><span class="pre">site</span></code>, <code class="code docutils literal notranslate"><span class="pre">channel</span></code>, and/or <code class="code docutils literal notranslate"><span class="pre">source</span></code>
is also always treated as a preprocessing problem.   Linking of these
normalized collections to wf_Seismogram and/or wf_TimeSeries is
required if the associated metadata is needed in your workflow.</p></li>
</ul>
<section id="preprocessing-import-collections">
<h3>Preprocessing/Import collections<a class="headerlink" href="#preprocessing-import-collections" title="Link to this heading"></a></h3>
<section id="id5">
<h4>Overview<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<p>We use the MongoDB database to manage waveform data import.  Waveform data
import should always be understood as another component of preprocessing
needed to assemble a working data set.   The reason we are dogmatic on that
principle is that our TimeSeries and Seismogram containers were designed to
be totally generic, while every single data format we know of has
implicit assumptions about the nature of the data.   For example,
has intrinsic assumptions the data are multichannel, seismic-reflection data and
SEED was designed for archive of permanent observatory data.
We discuss import methods currently available in MsPASS in separate
sections below.</p>
</section>
<section id="seed-and-miniseed">
<h4>SEED and MiniSEED<a class="headerlink" href="#seed-and-miniseed" title="Link to this heading"></a></h4>
<p>The Standard for the Exchange of Earthquake Data (SEED) format is the primary
format used by global data centers in seismology.   It has also become
a common format for raw data handling from portable earthquake recording
instruments supplied by the IRIS-PASSCAL program.   The most complete
support for data import in MsPASS is based on SEED and/or so called
miniSEED (miniseed) data.  For those unfamiliar with these terms miniseed
is a subset of SEED data that contains only the minimal metadata required
to define a set of data contained in package of data.  (We say “package”
instead of “file” because miniseed can and has been used as a network
transfer format because the data bundled into a serial string of packets.
For more details about SEED and miniseed can be found
<a class="reference external" href="https://ds.iris.edu/ds/nodes/dmc/data/formats/seed/">here</a> ).</p>
<p>Python modules to handle the import of SEED data are packages found
under <code class="code docutils literal notranslate"><span class="pre">mspasspy.preprocessing.seed</span></code>.   Our current implementation depends
upon obspy’s miniseed reader that imposes some restrictions.
A fundamental scalability problem in the current version of obspy’s reader
is it makes what we might call the SAC model of data management.  That is,
SAC and obspy both work best if the data are fragmented loaded with one
file per Trace object (equivalent in concept to mspasspy.ccore.TimeSeries).
That model produces a serious scalability problem on large data sets, especially if
they are stored on large virtual disk arrays common today in HPC centers.
The authors have seen example where simply deleting a data set with the
order of a million files can take days to complete on such a system.
Thus that model is completely at odd with the goal of building a high performance
parallel system.</p>
<p>To address this problem our current implementation to import miniseed data
uses a compromise solution where we concatenate logically related miniseed
files into larger files of data.  Type examples are: (1) “event gathers”, which
means a file of all data related to particular earthquake (event) and (2)
“receiver gathers” where data are grouped by station.   As a somewhat extreme
example, a year of USArray data for teleseismic earthquakes is known to
define of the order of 10^7 files per year if stored using the obspy/sac model.
(The entire teleseismic data set approaches 10^9 waveform segments.)
When merged into event files the number reduces to the order of 1000 per year.
That is known to eliminate the file name management problem at the cost of
needing to work with potentially very large files that can create memory problems.
That problem is particularly acute at the present because of a fundamental
problem with obspy’s reader miniseed reader; when given a large file their
reader will try to eat the whole file and convert the data to a potentially
large list of Trace objects bundled into a Stream container.  We plan to
eventually implement a foff index as used in CSS3.0’s wfdisc table, but
that idea is not currently supported.  (For those familiar with raw data
handling <code class="code docutils literal notranslate"><span class="pre">foff</span></code> in css3.0 implementation is used as a argument to the low
level, C function fseek to position the read pointer to a particular
position in a file containing multiple waveform segments.  A more efficent
reader would also need to store the number of bytes to load to know the
range of data defining data to be uncoded to produce a single Trace/TimeSeries
object.)</p>
<p>Our current code in the module <code class="code docutils literal notranslate"><span class="pre">mspasspy.preprocessing.seed.ensembles</span></code>
imports data through a two step procedure:</p>
<ol class="arabic">
<li><p>Run the following function on each seed file that is a bundle of
multiple channels of data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">dbsave_seed_ensemble_file</span><span class="p">(</span><span class="n">db</span><span class="p">,</span><span class="n">file</span><span class="p">,</span><span class="n">gather_type</span><span class="o">=</span><span class="s2">&quot;event&quot;</span><span class="p">,</span><span class="n">keys</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</pre></div>
</div>
<p>where <code class="code docutils literal notranslate"><span class="pre">file</span></code> is assumed to be a miniseed file and <code class="code docutils literal notranslate"><span class="pre">db</span></code> is a <code class="code docutils literal notranslate"><span class="pre">Database</span></code>
object, which is our database handle class.  The <code class="code docutils literal notranslate"><span class="pre">dbsave_seed_ensemble_file</span></code>
function builds only an index of the given file and writes the index to
a special collection called <code class="code docutils literal notranslate"><span class="pre">wf_miniseed</span></code>.</p>
</li>
<li><p>The same data can be loaded into memory as a MsPASS <a class="reference internal" href="../python_api/mspasspy.ccore.html#mspasspy.ccore.seismic.TimeSeriesEnsemble" title="mspasspy.ccore.seismic.TimeSeriesEnsemble"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeriesEnsemble</span></code></a>
object using the related function with this signature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_one_ensemble</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span>
           <span class="n">create_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">jobname</span><span class="o">=</span><span class="s1">&#39;Default job&#39;</span><span class="p">,</span>
           <span class="n">jobid</span><span class="o">=</span><span class="s1">&#39;99999&#39;</span><span class="p">,</span>
           <span class="n">algid</span><span class="o">=</span><span class="s1">&#39;99999&#39;</span><span class="p">,</span>
           <span class="n">ensemble_mdkeys</span><span class="o">=</span><span class="p">[],</span>
                       <span class="n">apply_calib</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</pre></div>
</div>
<p>where <code class="code docutils literal notranslate"><span class="pre">doc</span></code> is a document retrieved from the wf_miniseed collection.
For example, the following shows how an entire dataset of miniseed files indexed
previously with dbsave_seed_ensemble_file can be read sequentially:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">mspasspy.db.client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mspasspy.db.database</span><span class="w"> </span><span class="kn">import</span> <span class="n">Database</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mspasspy.preprocessing.seed.ensembles</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_one_ensemble</span>


<span class="n">dbname</span><span class="o">=</span><span class="s2">&quot;mydatabase&quot;</span>   <span class="c1"># set to the name of your MongoDB database</span>
<span class="n">client</span><span class="o">=</span><span class="n">Client</span><span class="p">()</span>
<span class="n">db</span><span class="o">=</span><span class="n">Database</span><span class="p">(</span><span class="n">client</span><span class="p">,</span><span class="n">dbname</span><span class="p">)</span>
<span class="n">dbwf</span><span class="o">=</span><span class="n">db</span><span class="o">.</span><span class="n">wf_miniseed</span>
<span class="n">curs</span><span class="o">=</span><span class="n">dbwf</span><span class="o">.</span><span class="n">find</span><span class="p">()</span>   <span class="c1"># insert a query dict in the find function to limit number</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">curs</span><span class="p">:</span>
  <span class="n">ensemble</span><span class="o">=</span><span class="n">load_one_ensemble</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
  <span class="c1"># Save these as TimeSeries objects</span>
  <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">member</span><span class="p">:</span>
    <span class="n">db</span><span class="o">.</span><span class="n">save_data</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>The above would produce a bare bones set of documents in the wf_TimeSeries
collection.   For some processing like noise correlation studies that may
be enough.   For any event-based processing the data will need to be
linked to the <code class="code docutils literal notranslate"><span class="pre">channel</span></code> and <code class="code docutils literal notranslate"><span class="pre">source</span></code> collections.   Current capability is
limited to ensemble processing and is best understood by examining the
sphynx generated documentation for the following functions:  <em>link_source_collection,
load_hypocenter_data_by_id, load_hypoceter_data_by_time, load_site_data</em>, and
<code class="code docutils literal notranslate"><span class="pre">load_channel_data</span></code>.   In addition, see our tutorial section for a detailed
example of how to use these functions.</p>
</section>
</section>
</section>
<section id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Link to this heading"></a></h2>
<section id="customizing-the-schema">
<h3>Customizing the schema<a class="headerlink" href="#customizing-the-schema" title="Link to this heading"></a></h3>
<p>THIS NEEDS TO BE WRITTEN</p>
</section>
<section id="importing-data-formats-other-than-miniseed">
<h3>Importing Data Formats other than miniSEED<a class="headerlink" href="#importing-data-formats-other-than-miniseed" title="Link to this heading"></a></h3>
<p>Obspy’s generic file reader supports a long list of formats described
<a class="reference external" href="https://docs.obspy.org/packages/autogen/obspy.core.stream.read.html">here</a>.
Any of these formats are readily imported into MsPASS, but would require
writing a custom reader.  Our miniseed reader in <code class="code docutils literal notranslate"><span class="pre">mspasspy.preprocessing.seed</span></code>
provides a model to do this.  One version of such a custom algorithm could
be summarized in the following common steps:</p>
<ol class="arabic simple">
<li><p>Run the obspy read function on a file.  It will return a Stream container
with one or more Trace objects.</p></li>
<li><p>Run the mspass Stream2TimeSeriesEnsemble function found in
<code class="code docutils literal notranslate"><span class="pre">mspasspy.util.converter</span></code>.</p></li>
<li><p>Run the loop as above containing <code class="code docutils literal notranslate"><span class="pre">db.save(d)</span></code> on the output of
Stream2TimeSeriesEnsemble</p></li>
</ol>
<p>If you need to import a format not on that list, the problem is much harder.
Our general recommendation is to replace the functionality of obspy’s
reader with a custom python read function designed to crack that particular
format.  One could either convert the weird format data to an obspy Stream
object so it was plug compatible in obspy or convert the data directly to
TimeSeries or TimeSeriesEnemble objects with the mspass ccore api.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="CRUD_operations.html" class="btn btn-neutral float-right" title="CRUD Operations in MsPASS" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2021, Ian Wang.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>