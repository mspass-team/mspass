{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Metadata, MetadataDefinitions, and AntelopePf\n",
    "\n",
    "## Metadata\n",
    "\n",
    "MsPASS makes extensive use of a C++ object we call Metadata.  A Metadata object is one of a many options we could have used to define the generalization of the idea of a header familar to many seismologists.  Headers are used, for example, in SAC and all seismic reflection packages we are aware of.   A \"header\", however, can be thought of as a implementation detail for a more general concept:  fetching parameters with a name-value pair relationship.  We use Metadata in preference to a python dict because the implementation is cleaner at the C++ level.  It is also, in principle, faster since the same methods visible through python wrapper are accesible in C++ code.   The purpose of this tutorial is to give users familiarity of this core class used for a wide variety of purposes.  \n",
    "\n",
    "This tutorial first teaches how to utilize the Metadata object in python.  When that is understood, we move to the AntelopePf, which is a child of Metadata with expanded capabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tell python what a Metadata object is.  We use the following common python incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mspasspy.ccore'; 'mspasspy' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb5e075e47cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmspasspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mccore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mspasspy.ccore'; 'mspasspy' is not a package"
     ]
    }
   ],
   "source": [
    "from mspasspy.ccore import Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create an empty Metadata container in the standard python way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "md=Metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metadata container is more flexible, but in MsPASS to provide a cleaner mapping to MongoDB we restrict the contents to the four lowest common demoninator types native to most computer languages: \n",
    "* real numbers, which are always promoted in MsPASS to double (64 bit floats)\n",
    "* integers, which are always promoted to 64 bit signed ints\n",
    "* character strings, which are handled in C++ as string objects and wrapped to python strings.  i.e. the conversion between C++ and python strings should be seamless.\n",
    "* booleans - meaning values that are \"True\" or \"False\" in python.   \n",
    "\n",
    "There are putters can create each of these core types by a clear name convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.put_double(\"real_example\",10.45)\n",
    "md.put_long(\"int_example\",42)\n",
    "md.put_string(\"foo\",\"bar\")  # the classic programmer idiom\n",
    "md.put_bool(\"bool_example\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also \"overloaded\" versions of the same that can depend on the python interpreters rules for assigning type from a literal.   These four lines do nearly the same thing as the previous four lines of python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.put(\"overreal\",99.45)\n",
    "md.put(\"overint\",2)\n",
    "md.put(\"overstr\",\"foobar\")\n",
    "md.put(\"overbool\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C++ code could use operator<< to dump the contents of md, but we currently have no equivalent in python.  For now you need to use getters.  This small example shows getters to pull a subset of the 8 entries currently stored in md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real example = 99.45\n",
      "int example= 42\n",
      "string example= bar\n",
      "boolean example= True\n"
     ]
    }
   ],
   "source": [
    "x=md.get_double('overreal')\n",
    "i=md.get_long('int_example')\n",
    "s=md.get_string('foo')\n",
    "b=md.get_bool('bool_example')\n",
    "print(\"real example =\",x)\n",
    "print(\"int example=\",i)\n",
    "print(\"string example=\",s)\n",
    "print(\"boolean example=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In interactive scripts or for testing it is often helpful to know what data are defined.  For that purpose we provide the keys method used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bool_example', 'overstr', 'foo', 'real_example', 'overbool', 'overint', 'int_example', 'overreal'}\n"
     ]
    }
   ],
   "source": [
    "keys=md.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetadataDefinitions\n",
    "\n",
    "A thorny problem in real data analysis with header attributes, which is what Metadata generalizes, is that the type of a parameter must match what is stored.  For example, if we tried to fetch the parameter \"foo\" from md as an integer or real value, the result would make no sense.    The Metadata class will throw a C++ exception that your python code may need to handle.   The current wrappers for Metadata cast the exception message to a stock python RuntimeError. Hence, if you have a section of code where a key-value pair may not be defined or is subject to a type mismatch, you should use a construct like the the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError('Error in Metadata get method.   Type mismatch in attem to get data with key=foo\\nboost::any bad_any_cast wrote this message:  \\nboost::bad_any_cast: failed conversion using boost::any_cast\\nTrying to convert to data of type=float\\nActual entry has type=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >\\n',)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s=md.get_string(\"foo\")  # this will work\n",
    "    x=md.get_double(\"foo\")  # this will throw an exception we need to handle\n",
    "except Exception as e: \n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a best practice catcher that uses the base class Exception and the built in repr function to convert the RuntimeError to a more readable form.  It is remains a bit ugly because of mismatch is handling the newline (\\n) character in ipython, but the gist of the error message should be clear.\n",
    "\n",
    "The general problem of data types with attributes like those we extract from Metadata has two end members in the computing world:  (1) python, which is completely agnostic about type, and (2) strongly typed languages like C/C++ and FORTRAN.  Since MsPASS is a hybrid with C++ implementation of some compute intensive, core algorithms and python for higher level processing this clash in concept has to be handled. An additional constraint comes from the use of MongoDB or any database engine.  Chaos will reign if a name key is used for attributes of different types; a problem far too easy to create even accidentally with python. Since the main purpose of MsPASS is to provide a framework for handling long-running, compute intensive data processing jobs, we have to enforce some type restrictions on attributes to avoid mysterious downstream behaviour and bugs that are difficult to impossible to find.  We do this through a core mspass object we call MetadataDefinitions.   More about the philosphy and design concepts of MetadataDefinitions can be found in the User's Manual (hyperlink to proper page).   Here we focus on how MetadataDefinitions should be used in a workflow.\n",
    "\n",
    "Any MsPASS job using any of the ccore data objects should near the top of the job script contain this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore import MetadataDefinitions\n",
    "mdef=MetadataDefinitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The import line, of course, could be mixed with other import commands at the top of your processing script.  The point here is the call to the default constructor for MetadataDefinitions().   That step initiates a read of a configuration file that defines the MsPASS default attribute namespace.    The full details of the default namespace can be found in tables in the User's Manual (hyperlink).  \n",
    "\n",
    "First, let's look at the full set of keys for the default namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U11', 'U12', 'U13', 'U21', 'U22', 'U23', 'U31', 'U32', 'U33', 'calib', 'chan', 'chanid', 'delta', 'dfile', 'dir', 'foff', 'gridfs_idstr', 'hang', 'iphase', 'loc', 'mb', 'ms', 'net', 'npts', 'oid_site', 'oid_source', 'orid', 'phase', 'sampling_rate', 'site_elev', 'site_id', 'site_lat', 'site_lon', 'source_depth', 'source_id', 'source_lat', 'source_lon', 'source_time', 'sta', 'starttime', 'storage_mode', 't0_shift', 'time_standard', 'vang', 'wfid_string']\n"
     ]
    }
   ],
   "source": [
    "mdkeys=mdef.keys()\n",
    "print(mdkeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the type of any key through the type method.  This script prints the full table of types for all defined keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U11  has type= MDtype.Double\n",
      "U12  has type= MDtype.Double\n",
      "U13  has type= MDtype.Double\n",
      "U21  has type= MDtype.Double\n",
      "U22  has type= MDtype.Double\n",
      "U23  has type= MDtype.Double\n",
      "U31  has type= MDtype.Double\n",
      "U32  has type= MDtype.Double\n",
      "U33  has type= MDtype.Double\n",
      "calib  has type= MDtype.Double\n",
      "chan  has type= MDtype.String\n",
      "chanid  has type= MDtype.Int64\n",
      "delta  has type= MDtype.Double\n",
      "dfile  has type= MDtype.String\n",
      "dir  has type= MDtype.String\n",
      "foff  has type= MDtype.Int64\n",
      "gridfs_idstr  has type= MDtype.String\n",
      "hang  has type= MDtype.Double\n",
      "iphase  has type= MDtype.String\n",
      "loc  has type= MDtype.String\n",
      "mb  has type= MDtype.Double\n",
      "ms  has type= MDtype.Double\n",
      "net  has type= MDtype.String\n",
      "npts  has type= MDtype.Int64\n",
      "oid_site  has type= MDtype.String\n",
      "oid_source  has type= MDtype.String\n",
      "orid  has type= MDtype.Int64\n",
      "phase  has type= MDtype.String\n",
      "sampling_rate  has type= MDtype.Double\n",
      "site_elev  has type= MDtype.Double\n",
      "site_id  has type= MDtype.Int64\n",
      "site_lat  has type= MDtype.Double\n",
      "site_lon  has type= MDtype.Double\n",
      "source_depth  has type= MDtype.Double\n",
      "source_id  has type= MDtype.Int64\n",
      "source_lat  has type= MDtype.Double\n",
      "source_lon  has type= MDtype.Double\n",
      "source_time  has type= MDtype.Double\n",
      "sta  has type= MDtype.String\n",
      "starttime  has type= MDtype.Double\n",
      "storage_mode  has type= MDtype.String\n",
      "t0_shift  has type= MDtype.Double\n",
      "time_standard  has type= MDtype.String\n",
      "vang  has type= MDtype.Double\n",
      "wfid_string  has type= MDtype.String\n"
     ]
    }
   ],
   "source": [
    "for k in mdkeys:\n",
    "    t=mdef.type(k)\n",
    "    print(k,' has type=',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing a script if you aren't sure what a key represents you can use the concept method.  Here, for example, we ask for a brief description of what the attribute \"vang\" should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inclination from +up (in degree) of a seismometer component - vertical angle\n"
     ]
    }
   ],
   "source": [
    "s=mdef.concept('vang')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a mechanism for MsPASS to legacy packages like SAC, the MetadataDefinitions object has a generic aliases mechanism.   For example, if we wanted to know alternative names that can be handled automatically for the attribute called \"source_lat\", we can issue this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVLA', 'origin.lat']\n"
     ]
    }
   ],
   "source": [
    "print(mdef.aliases('source_lat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we can use the SAC name, EVLA, or the Antelope name, origin.lat, for an alternative to source_lat and it will be handled automatically by database readers and writers.\n",
    "\n",
    "There are a collection of other useful methods defined in for MetadataObject for dealing with aliases.  The following script illustrates their use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVLA is a valid alias\n",
      "The unique key for EVLA for database records is  ('source_lat', MDtype.Double)\n",
      "The keyword source_lat has one or more aliases defined\n",
      "Valid aliases are ['EVLA', 'origin.lat']\n"
     ]
    }
   ],
   "source": [
    "if(mdef.is_alias('EVLA')):\n",
    "    print('EVLA is a valid alias')\n",
    "    ukey=mdef.unique_name('EVLA')\n",
    "    print('The unique key for EVLA for database records is ',ukey)\n",
    "if(mdef.has_alias('source_lat')):\n",
    "    print('The keyword source_lat has one or more aliases defined')\n",
    "    print('Valid aliases are',mdef.aliases('source_lat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of aliases is an important component of MsPASS to simplify utilizing legacy software as part of a workflow.   A typical example would be some specialized program that was built around the SAC data format or an anticipated development in MsPASS of running SAC from a MsPASS workflow.   To support that type of application MsPASS has two methods in *MetadataDefinitions* called *apply_aliases* and the inverse called *clear_aliases*.   The following small script demonstrates a small example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MetadataDefinitions has what could be called advanced methods for handling a feature of MongoDB we found useful called \"normalization\".   (See https://docs.mongodb.com/manual/core/data-model-design/ for the concepts of normalization in MongoDB)   The key point is that some attributes like receiver coordinates and response information are better stored in a single place and found by a cross-reference mechanism rather than being duplicated many times or risk mistakes from incorrect associations.   Methods available to support this feature are:\n",
    "\n",
    "1.  *is_ normalized* - test whether an attribute is expected to be normalized\n",
    "2.  *collection* - return the MongoDB collection name where the master of an attribute should be located.\n",
    "3.  *readonly, writeable* - test for whether an attribute is marked readonly.   Normalized data are normally marked readonly* (immutable) because they should be set only on construction and never altered by a processing workflow.   \n",
    "4.  *set_readonly, set_writeable* - backdoor methods to set (set_readonly) or override locks (set_writeable) on an attribute.   These functions should not be used unless essential.   \n",
    "\n",
    "Finally, there are two methods for manually defining new attributes not defined in the master namespace.\n",
    "\n",
    "1. *add* - define a new attribute with type and concept properties\n",
    "2. *add_alias* - define a new alias for an existing attribute. \n",
    "\n",
    "These also should be used with caution as it is preferable for custom applications to edit the master list used to construct MetadataDefinitions objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial metadata with MsPASS names\n",
      "Value with key= site_lon  is  55.0\n",
      "Value with key= starttime  is  0.0\n",
      "Value with key= source_lat  is  10.0\n",
      "Value with key= npts  is  100\n",
      "Value with key= delta  is  1.0\n",
      "Value with key= source_lon  is  -75.0\n",
      "Value with key= source_depth  is  10.0\n",
      "Value with key= site_lat  is  45.0\n",
      "Metadata after apply_aliases\n",
      "Value with key= EVLO  is  -75.0\n",
      "Value with key= starttime  is  0.0\n",
      "Value with key= npts  is  100\n",
      "Value with key= EVDP  is  10.0\n",
      "Value with key= delta  is  1.0\n",
      "Value with key= EVLA  is  10.0\n",
      "Value with key= STLO  is  55.0\n",
      "Value with key= STLA  is  45.0\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.ccore import CoreSeismogram\n",
    "def printmd(md):\n",
    "    keys=md.keys()\n",
    "    for k in keys:\n",
    "        print(\"Value with key=\",k,\" is \",md[k])\n",
    "\n",
    "# We create an seismogram object to demonstrate these are used for data objects\n",
    "sacmd=CoreSeismogram(100)\n",
    "sacmd.put_double(\"source_lat\",10.0)\n",
    "sacmd.put_double(\"source_lon\",-75.0)\n",
    "sacmd.put_double(\"source_depth\",10.0)\n",
    "sacmd.put_double(\"site_lat\",45.0)\n",
    "sacmd.put_double(\"site_lon\",55.0)\n",
    "print(\"Initial metadata with MsPASS names\")\n",
    "printmd(sacmd)\n",
    "# This creates the list of aliases to apply - the names are the alias names\n",
    "# Note STEL is not actually defined above - illustrates handling of unset aliases\n",
    "aliaslist=[\"EVLA\",\"EVLO\",\"EVDP\",\"STLA\",\"STLO\",\"STEL\"]\n",
    "mdef.apply_aliases(sacmd,aliaslist)\n",
    "print(\"Metadata after apply_aliases\")\n",
    "printmd(sacmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that apply_aliases silently skipped the inconsistency that *site_elev* is not defined in sacmd.  That was an intentional design because we expect global alias lists (e.g. applying a set of fixed aliases for sac, obspy, or Antelope) to be the norm.  \n",
    "\n",
    "Also notice that three additional Metadata attributes appeared when the printmd function was called:  starttime(real), npts (integer), and delta (real).   These are three attributes wired into private variables in the mspass data objects.   The C++ api guarantees these Metadata name-value pairs are consistent with internal values.  They appear in this output because the printmd lists all defined Metadata fields.  Note finally printmd uses the alternative access method for Metadata fields using the key in the same syntax as a python dict.   For purely pythonic interactions that approach is appropriate because python just returns the right type.  In contrast calling a method like get_double that is dogmatic about type will generate an exception if the type does not match.  Which you should use will depend on context.\n",
    "\n",
    "The *clear_aliases* method of *MetadataDefiniions* will restore all entries to the standard value as illustrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdef.clear_aliases(sacmd)\n",
    "printmd(sacmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *apply_aliases* and *clear_aliases* methods provide a generic support for working with different namespaces.  The cost of applying them is not large, but not tiny either.  For efficiency avoid unnecessary alias definitions by grouping processes using a common namespace when possible.   On the other hand, when a block of processing steps requiring aliases are finished you should immediately call *clear_aliases* to avoid downstream errors.  We emphasize *clear_aliases* resets ALL metadata defined as an alias by the MetadataDefinitions object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AntelopePf\n",
    "\n",
    "The Metadata object is an implementation of the core idea of fetching an attribute defined by a name-value pair, where the name is what is commonly called a key and the value is the data we want to associate with the key.   As we just saw this maps exactly into the concept of a header that has been proven a useful concept since the earliest days of seismic data processing pioneered by the oil and gas industry in the 1960s.  \n",
    "\n",
    "The *AntelopePf* is an implementation of a different, but related problem in all data processing.  Almost any algorithm has at least one tunable parameter that needs to be defined.  In general, the more generic an algorithm is the more parameters will be needed to define the way it should behave.  In the early days of data processing this problem was often solved by creating special format \"input files\" that a program read at startup.   Computer scientists realized decades ago that this was a generic problem and thus had generic solutions.   Hundreds of solutions to this problem exist with configuration files of various formats.  Today the most common is probably xml. We elected to no use xml for our initial development of mspass for two reasons:\n",
    "1.  xml is not a human readable format, but a language for robots (computers).  It is very hard for a human being to create a valid xml file by entering the data manually.  We needed a format that was easy for a human to construct.\n",
    "2.  Many seismologists utilize BRTT's \"parameter files\", because of the generous license agreement BRTT provides for U.S. scientists.   Furthermore, both of primary authors of MsPASS were familiar with parameter files and we had an open source implementation we could build on from Pavlis's plane wave migration code.\n",
    "\n",
    "We thus adopted the \"parameter files\" syntax to implement an extension of Metadata we call an *AntelopePf*. The *AntelopePf* is a child of *Metadata* so the same methods introduced for *Metadata* can be used for an *AntelopePf*.   The use, however, is more than a little subtle and is best understood from an example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a concrete example of the kind of complex parameter file that *AntelopePf* was designed to handle.  The following is the default configuration for a new deconvolution routine in MsPASS we call CNR3CDecon (for Colored Noise 3C (Three-component) Deconvolution):\n",
    "```\n",
    "########################################################################\n",
    "operator_nfft 4096\n",
    "#damping_factor 1000.0\n",
    "damping_factor 1.0\n",
    "snr_regularization_floor 2.0\n",
    "target_sample_interval 0.05\n",
    "deconvolution_data_window_start -2.0\n",
    "deconvolution_data_window_end 30.0\n",
    "time_bandwidth_product 4.5\n",
    "number_tapers 8\n",
    "shaping_wavelet_dt 0.05\n",
    "shaping_wavelet_type ricker\n",
    "shaping_wavelet_frequency 1.0\n",
    "shaping_wavelet_frequency_for_inverse 0.5\n",
    "noise_window_start -30.0\n",
    "noise_window_end -5.0\n",
    "\n",
    "taper_type cosine\n",
    "CosineTaper &Arr{\n",
    "  data_taper &Arr{\n",
    "    front0 -2.0\n",
    "    front1 -1.0\n",
    "    tail1 27.0\n",
    "    tail0 29.5\n",
    "  }\n",
    "  wavelet_taper &Arr{\n",
    "   front0 -0.75\n",
    "   front1 -0.25\n",
    "   tail1 2.5\n",
    "   tail0 3.0\n",
    "  }\n",
    "}\n",
    "LinearTaper &Arr{\n",
    "  data_taper &Arr{\n",
    "    front0 -2.0\n",
    "    front1 -1.0\n",
    "    tail1 27.0\n",
    "    tail0 29.5\n",
    "  }\n",
    "  wavelet_taper &Arr{\n",
    "   front0 -0.75\n",
    "   front1 -0.25\n",
    "   tail1 2.5\n",
    "   tail0 3.0\n",
    "  }\n",
    "}\n",
    "########################################################################\n",
    "\n",
    "```\n",
    "We have supplied a copy of the data above in a file called data/test.pf.  You should then be able to load this file with the following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore import AntelopePf\n",
    "pf=AntelopePf('data/test.pf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters outside the curly brackets and the \"&Arr\" tags are handled by Metadat methods - they are simple name value pairs.  Here are a couple examples.  You can extend these to test your knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple parameter operator_nfft has this value: 4096\n",
      "Simple parameter damping_factor has this value: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple parameter operator_nfft has this value:\",pf.get_long(\"operator_nfft\"))\n",
    "print(\"Simple parameter damping_factor has this value:\",pf.get_double(\"damping_factor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AntelopePf* extends *Metadata* with two primary methods:   *get_branch* and *get_tbl*.  This little code fragment illustrates the *get_branch* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata keys for CosineTaper branch  (an empty list) set()\n",
      "Metadata keys found for wavelet_taper branch:  {'front0', 'tail0', 'tail1', 'front1'}\n"
     ]
    }
   ],
   "source": [
    "pfb1=pf.get_branch('CosineTaper')\n",
    "# Note the example pf has no simple name-value pairs under the CosineTaper tag.  \n",
    "# This illustrates that is o\n",
    "keys=pfb1.keys()\n",
    "print('Metadata keys for CosineTaper branch  (an empty list)',keys)\n",
    "pfb2=pfb1.get_branch('wavelet_taper')\n",
    "keys=pfb2.keys()\n",
    "print('Metadata keys found for wavelet_taper branch: ',keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular example is a little unusual in that the result of the first get_branch call has simple name-value pairs, but only two branches.  It also doesn't demonstrate have data that can be fetched with the other *AntelopePf* extension called *get_tbl*.   To see, here is the default parameter file for the Antelope contrib program export_to_mspass that can be used to take a data set defined by an Antelope database and import it to MsPASS.\n",
    "```\n",
    "required &Tbl{\n",
    "dt delta real\n",
    "origin.depth source_depth real\n",
    "origin.lat source_lat real\n",
    "origin.lon source_lon real\n",
    "origin.time source_time real\n",
    "site.lat site_lat real\n",
    "site.lon site_lon real\n",
    "site.elev site_elev real\n",
    "nsamp npts int\n",
    "sta sta string\n",
    "evid source_id int\n",
    "U11 U11 real\n",
    "U12 U12 real\n",
    "U13 U13 real\n",
    "U21 U21 real\n",
    "U22 U22 real\n",
    "U23 U23 real\n",
    "U31 U31 real\n",
    "U32 U32 real\n",
    "U33 U33 real\n",
    "}\n",
    "optional &Tbl{\n",
    "origin.mb mb real\n",
    "origin.ms ms real\n",
    "arrival.iphase iphase string\n",
    "assoc.phase phase string\n",
    "orid orid int\n",
    "}\n",
    "```\n",
    "The above data are contained in another file data/test2.pf.   You should be able to load it by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['origin.mb mb real', 'origin.ms ms real', 'arrival.iphase iphase string', 'assoc.phase phase string', 'orid orid int']\n"
     ]
    }
   ],
   "source": [
    "pf2=AntelopePf('data/test2.pf')\n",
    "tbllist=pf2.get_tbl('optional')\n",
    "print(tbllist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates the *get_tbl* method returns the data between the \"optional &Tbl{\" and the \"}\" at the end of the data file as a python list of strings - one list element per line.   This can be used by any program where the input can be defined as a sequence of lines.  This example uses a format where token 1 is the antelope database attribute name that is to be fetch, token 2 is the name that is to be assigned for the export file, and token 3 is the name used to define the type of the data expected (Antelope's database has type constraints for the same reasons we noted above).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
