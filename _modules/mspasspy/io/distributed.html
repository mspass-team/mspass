<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mspasspy.io.distributed &mdash; MsPASS 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=f6245a2f"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
        <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />  

  <style>
    .wy-nav-content { max-width: 1600px; }
  </style>

  
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            MsPASS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
    
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/quick_start.html">Getting Started in a Nutshell</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/run_mspass_with_docker.html">Run MsPASS with Docker</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/deploy_mspass_with_docker_compose.html">Deploy MsPASS with Docker Compose</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/deploy_mspass_on_HPC.html">Deploying MsPASS on an HPC cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/getting_started_overview.html">MsPASS Virtual Cluster Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started/deploy_mspass_with_conda_and_coiled.html">Deploy MsPASS with Conda and Coiled</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/introduction.html">Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Management</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/database_concepts.html">Database Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/CRUD_operations.html">CRUD Operations in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/mongodb_and_mspass.html">Using MongoDB with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/importing_tabular_data.html">Importing Tabular Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Seismic Data Objects</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/data_object_design_concepts.html">Data Object Design Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/numpy_scipy_interface.html">Using numpy/scipy with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/obspy_interface.html">Using ObsPy with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/time_standard_constraints.html">Time Standard Constraints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/processing_history_concepts.html">Processing History Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/continuous_data.html">Continuous Data Handling with MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/schema_choices.html">What database schema should I use?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Processing</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/importing_data.html">Importing Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/handling_errors.html">Handling Errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/data_editing.html">Data Editing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/header_math.html">Header (Metadata) Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/graphics.html">Graphics in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/signal_to_noise.html">Signal to Noise Ratio Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/adapting_algorithms.html">Adapting an Existing Algorithm to MsPASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">System Tuning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/parallel_processing.html">Parallel Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/memory_management.html">Memory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/io.html">I/O in MsPASS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/parallel_io.html">Parallel IO in MsPASS</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/FAQ.html">Frequency Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_manual/development_strategies.html">How do I develop a new workflow from scratch?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../python_api/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cxx_api/index.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mspass_schema/mspass_schema.html">MsPASS Schema</a></li>
</ul>

    <a href= "../../../genindex.html">Index</a>
  
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MsPASS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">mspasspy.io.distributed</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mspasspy.io.distributed</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">json</span>


<span class="kn">from</span> <span class="nn">mspasspy.db.database</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Database</span><span class="p">,</span>
    <span class="n">md2doc</span><span class="p">,</span>
    <span class="n">doc2md</span><span class="p">,</span>
    <span class="n">elog2doc</span><span class="p">,</span>
    <span class="n">history2doc</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.normalize</span> <span class="kn">import</span> <span class="n">BasicMatcher</span>

<span class="c1"># name collision here requires this alias</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.normalize</span> <span class="kn">import</span> <span class="n">normalize</span> <span class="k">as</span> <span class="n">normalize_function</span>
<span class="kn">from</span> <span class="nn">mspasspy.util.Undertaker</span> <span class="kn">import</span> <span class="n">Undertaker</span>
<span class="kn">from</span> <span class="nn">mspasspy.db.client</span> <span class="kn">import</span> <span class="n">DBClient</span>


<span class="kn">from</span> <span class="nn">mspasspy.ccore.utility</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ErrorLogger</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">dask</span>
<span class="kn">import</span> <span class="nn">pyspark</span>


<div class="viewcode-block" id="read_ensemble_parallel"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.read_ensemble_parallel">[docs]</a><span class="k">def</span> <span class="nf">read_ensemble_parallel</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;wf_TimeSeries&quot;</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;promiscuous&quot;</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">load_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sort_clause</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">aws_access_key_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Special function used in read_distributed_data to handle ensembles.</span>

<span class="sd">    Ensembles need to be read via a cursor which is not serializable.</span>
<span class="sd">    Here we query a Database class, which is serializable, and</span>
<span class="sd">    call it&#39;s read_data method to construct an ensemble that it returns.</span>
<span class="sd">    Defined as a function instead of using a lambda due largely to the</span>
<span class="sd">    complexity of the argument list passed to  read_data.</span>

<span class="sd">    Arguments are all passed directly from values set within</span>
<span class="sd">    read_distributed_data.  See that function for parameter descriptions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">sort_clause</span><span class="p">:</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">collection</span><span class="p">]</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">sort_clause</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cursor</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">collection</span><span class="p">]</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">ensemble</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">read_data</span><span class="p">(</span>
        <span class="n">cursor</span><span class="p">,</span>
        <span class="n">collection</span><span class="o">=</span><span class="n">collection</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
        <span class="n">load_history</span><span class="o">=</span><span class="n">load_history</span><span class="p">,</span>
        <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
        <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
        <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">aws_access_key_id</span><span class="p">,</span>
        <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">aws_secret_access_key</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">kill_me</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">ensemble</span><span class="o">.</span><span class="n">member</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">live</span><span class="p">:</span>
            <span class="n">kill_me</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">break</span>
    <span class="k">if</span> <span class="n">kill_me</span><span class="p">:</span>
        <span class="n">ensemble</span><span class="o">.</span><span class="n">kill</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ensemble</span></div>


<div class="viewcode-block" id="post2metadata"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.post2metadata">[docs]</a><span class="k">def</span> <span class="nf">post2metadata</span><span class="p">(</span><span class="n">mspass_object</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posts content of key-value pair container doc to Metadata container</span>
<span class="sd">    of mspass_object.  Operation only requires that doc be iterable</span>
<span class="sd">    over keys and both doc and mspass_object act like associative</span>
<span class="sd">    arrays.   In MsPASS that means doc can be either a dict or a</span>
<span class="sd">    Metadata container.  Note the contents of doc[k] will overwrite</span>
<span class="sd">    any existing content defined by mspass_object[k].  It returns the edited</span>
<span class="sd">    copy of mspass_object to mesh with map operator usage in</span>
<span class="sd">    read_distributed_data.   Note that Metadata &quot;is a&quot; fundamental component</span>
<span class="sd">    of all atomic data. Ensemble objects contain a special instance of</span>
<span class="sd">    Metadata we normally refer to as &quot;ensemble metadata&quot;.   When handling</span>
<span class="sd">    ensembles the content of doc are pushed to the ensemble metadata and</span>
<span class="sd">    member attributes are not altered by this function.</span>

<span class="sd">    This function is the default for the read_distributed_data</span>
<span class="sd">    argument container_merge_function which provides the means to push</span>
<span class="sd">    a list of Metadata or dict containers held in a conguent bag/rdd</span>
<span class="sd">    with a set of seismic data objects.   Because of that usage it has</span>
<span class="sd">    no safeties for efficiency.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="n">mspass_object</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">mspass_object</span></div>


<div class="viewcode-block" id="read_distributed_data"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.read_distributed_data">[docs]</a><span class="k">def</span> <span class="nf">read_distributed_data</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">db</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">query</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scratchfile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;wf_TimeSeries&quot;</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;promiscuous&quot;</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize_ensemble</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">load_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="s2">&quot;dask&quot;</span><span class="p">,</span>
    <span class="n">npartitions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">spark_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sort_clause</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">container_merge_function</span><span class="o">=</span><span class="n">post2metadata</span><span class="p">,</span>
    <span class="n">container_to_merge</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">aws_access_key_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel data reader for seismic data objects.</span>

<span class="sd">    In MsPASS seismic data objects need to be loaded into a Spark RDD or</span>
<span class="sd">    Dask bag for parallel processing. This function abstracts that</span>
<span class="sd">    process parallelizing the read operation where it can do so.</span>
<span class="sd">    In MsPASS all data objects are created by constructors driven by</span>
<span class="sd">    one or more documents stored in a MongoDB database collection</span>
<span class="sd">    we will refer to here as a &quot;waveform collection&quot;.  Atomic data</span>
<span class="sd">    are built from single documents that may or may not be extended</span>
<span class="sd">    by &quot;normalization&quot; (option defined by normalize parameter).</span>
<span class="sd">    The constructors can get the sample data associated with a</span>
<span class="sd">    waveform document by a variety of storage methods that</span>
<span class="sd">    are abstracted to be &quot;under the hood&quot;.  That is relevant to understanding</span>
<span class="sd">    this function because an absolutely required input is a handle to</span>
<span class="sd">    the waveform db collection OR an image of it in another format.</span>
<span class="sd">    Furthermore, the other absolute is that the output is ALWAYS a</span>
<span class="sd">    parallel container;  a spark RDD or a Dask bag.   This reader</span>
<span class="sd">    is the standard way to load a dataset into one of these parallel</span>
<span class="sd">    containers.</span>

<span class="sd">    A complexity arises because of two properties of any database</span>
<span class="sd">    including MongoDB.  First, queries to any database are almost always</span>
<span class="sd">    very slow compared to processor speed.  Initiating a workflow with</span>
<span class="sd">    a series of queries is possible, but we have found it ill advised</span>
<span class="sd">    as it can represent a throttle on speed for many workflows where the</span>
<span class="sd">    time for the query is large compared to the processing time for a</span>
<span class="sd">    single object.   Second, all database engines, in contrast, can</span>
<span class="sd">    work linearly through a table (collection in MongoDB) very fast</span>
<span class="sd">    because they cache blocks of records send from the server to the client.</span>
<span class="sd">    In MongoDB that means a &quot;cursor&quot; returned by a &quot;find&quot; operation</span>
<span class="sd">    can be iterated very fast compared to one-by-one queries of the</span>
<span class="sd">    same data.  Why all that is relevant is that arg0 if this function is</span>
<span class="sd">    required to be one of three things to work well within these constraints:</span>
<span class="sd">        1. An instance of a mspass `Database` handle</span>
<span class="sd">           (:class:`mspasspy.db.database.Database`).  Default</span>
<span class="sd">           with this input is to read an entire collection.  Use the</span>
<span class="sd">           query parameter to limit input.  This mode also implicitly</span>
<span class="sd">           implies the result will be a bag/RDD of atomic data.  Note</span>
<span class="sd">           that for efficiency when run in this mode the first step</span>
<span class="sd">           the algorithm does is load the entire set of documents</span>
<span class="sd">           defined by query (or the default of all documents) into a</span>
<span class="sd">           pandas DataFrame.  If that approach causes memory issues</span>
<span class="sd">           use the `scratchfile` option to buffer the large table into</span>
<span class="sd">           a scratch file and then construct a dask or spark DataFrame</span>
<span class="sd">           that are designed as containers that can overflow memory.</span>
<span class="sd">        2. One of several implementations of a &quot;Dataframe&quot;, that are</span>
<span class="sd">           an image or a substitute for a MongoDB waveform collection.</span>
<span class="sd">           A Dataframe, for example, is a natural output from any</span>
<span class="sd">           sql (or, for seismologists, an Antelope) database.   This</span>
<span class="sd">           reader supports input through a pandas Dataframe, a Dask</span>
<span class="sd">           Dataframe, or a pyspark Dataframe.  THE KEY POINT about</span>
<span class="sd">           dataframe input, however, is that the attribute names</span>
<span class="sd">           must match schema constraints on the MongoDB database</span>
<span class="sd">           that is required as an auxiliary input when reading</span>
<span class="sd">           directly from a dataframe.   Note also that Dataframe input</span>
<span class="sd">           also only makes sense for Atomic data with each tuple</span>
<span class="sd">           mapping to one atomic data object to be created by the reader.</span>
<span class="sd">        3. Ensembles represent a fundamentally different problem in this</span>
<span class="sd">           context.  An ensemble, by definition, is a group of atomic data</span>
<span class="sd">           with an optional set of common Metadata.  The data model for this</span>
<span class="sd">           function for ensembles is it needs to return a RDD/bag</span>
<span class="sd">           containing a dataset organized into ensembles.  The approach used</span>
<span class="sd">           here is that a third type of input for arg0 (data) is a list</span>
<span class="sd">           of python dict containers that are ASSUMED to be a set of</span>
<span class="sd">           queries that defined the grouping of the ensembles.</span>
<span class="sd">           For example, a data set you want to process as a set of</span>
<span class="sd">           common source gathers (ensmebles) might be created</span>
<span class="sd">           using a list something like this:</span>
<span class="sd">           [{&quot;source_id&quot; : ObjectId(&#39;64b917ce9aa746564e8ecbfd&#39;)},</span>
<span class="sd">            {&quot;source_id&quot; : ObjectId(&#39;64b917d69aa746564e8ecbfe&#39;)},</span>
<span class="sd">            ... ]</span>

<span class="sd">    This function is more-or-less three algorithms that are run for</span>
<span class="sd">    each of the three cases above.   In the same order as above they are:</span>
<span class="sd">        1. With a Database input the function first iterates through the entire</span>
<span class="sd">           set of records defined for specified collection</span>
<span class="sd">           (passed via collection argument) constrained by the</span>
<span class="sd">           (optional) query argument.  Note that processing is run</span>
<span class="sd">           serial but fast because it working through a cursor is</span>
<span class="sd">           optimized in MongoDB and the proceessing is little more than</span>
<span class="sd">           reformatting the data.</span>
<span class="sd">        2. The dataframe is passed through a map operator</span>
<span class="sd">           (dask or spark depending on the setting of the scheduler argument)</span>
<span class="sd">           that constructs the output bag/RDD in parallel.  The atomic</span>
<span class="sd">           operation is calls to db.read_data, but the output is a bag/RDD</span>
<span class="sd">           of atomic mspass data objects.   That means reads will be</span>
<span class="sd">           parallel with one reader per worker.</span>
<span class="sd">        3. Ensembles are read in parallel with granularity defined by</span>
<span class="sd">           a partitioning of the list of queries set by the npartitions</span>
<span class="sd">           parameter.  Parallelism is achieved by calling the function</span>
<span class="sd">           internal to this function called `read_ensemble_parallel`</span>
<span class="sd">           in a map operator.   That function queries the database</span>
<span class="sd">           using the query derived form arg0 of this function and uses</span>
<span class="sd">           the return to call the `Database.read_data` method.</span>

<span class="sd">    Reading all types in a parallel context have a more subtle complexity</span>
<span class="sd">    that arises in a number of situations.  That is, many algorithms</span>
<span class="sd">    are driven by external lists of attributes with one item in the list</span>
<span class="sd">    for each datum to be constructed and posted to the parallel container</span>
<span class="sd">    output by this function.   An example is a list of arrival times</span>
<span class="sd">    used to create a dataset of waveform segments windowed relative to</span>
<span class="sd">    the list of arrival times from continuous data or a dataset built</span>
<span class="sd">    from longer time windows (e.g. extracting P wave windows from hour</span>
<span class="sd">    long segments created for teleseismic data.).  That requires a special</span>
<span class="sd">    type of matching that is very inefficient (i.e. slow) to implement</span>
<span class="sd">    with MongoDB (It requires a query for every datum.)   One-to-one</span>
<span class="sd">    matching attributes can handled by this function with proper</span>
<span class="sd">    use of the two attributes `container_to_merge` and</span>
<span class="sd">    `container_merge_function`.   `container_to_merge` must be either</span>
<span class="sd">    a dask bag or pyspark RDD depending on the setting of the</span>
<span class="sd">    argument `spark_context` (i.e. if `spark_context` is defined the</span>
<span class="sd">    function requires the container be an RDD while a dask bag is the default.)</span>
<span class="sd">    The related argument, `container_merge_function`, is an optional function</span>
<span class="sd">    to use to merge the content of the components of `container_to_merge`.</span>
<span class="sd">    The default assumes `container_to_merge` is a bag/RDD of `Metadata`</span>
<span class="sd">    or python dict containers that are to be copied (overwriting) the</span>
<span class="sd">    Metadata container of each result bag/RDD component.  Note that for</span>
<span class="sd">    atomic data that means the Metadata container for each</span>
<span class="sd">    `TimeSeries` or `Seismogram` constructed by the reader while for</span>
<span class="sd">    ensembles the attributes are posted to the ensemble&#39;s `Metadata` container.</span>
<span class="sd">    A user supplied function to replace the default must have two arguments</span>
<span class="sd">    where arg0 is the seismic data to receive the edits defined by</span>
<span class="sd">    `container_to_merge` while arg1 should contain the comparable component</span>
<span class="sd">    of `container_to_merge`.   Note this capability is not at all limited to</span>
<span class="sd">    `Metadata`.  This function can contain anything that provides input</span>
<span class="sd">    for applying an algorithm that alters the datum given a set of parameters</span>
<span class="sd">    passed through the `container_to_merge`.</span>

<span class="sd">    Normalization with normalizing collections like source, site, and channel</span>
<span class="sd">    are possible through the normalize argument.   Normalizers using</span>
<span class="sd">    data cached to memory can be used but are likely better handled</span>
<span class="sd">    after a bag/rdd is created with this function via one or more</span>
<span class="sd">    map calls following this reader.  Database-driven normalizers</span>
<span class="sd">    are (likely) best done through this function to reduce unnecessary</span>
<span class="sd">    serialization of the database handle essential to this function</span>
<span class="sd">    (i.e. the handle is already in the workspace of this function).</span>
<span class="sd">    Avoid the form of normalize used prior to version 2.0 that allowed</span>
<span class="sd">    the use of a list of collection names.   It was retained for</span>
<span class="sd">    backward compatibility but is slow.</span>

<span class="sd">    A final complexity users need to be aware of is how this reader handles</span>
<span class="sd">    any errors that happen during construction of all the objects in the</span>
<span class="sd">    output bag/rdd.  All errors that create invalid data objects produce</span>
<span class="sd">    what we call &quot;abortions&quot; in the User Manual and docstring for the</span>
<span class="sd">    :class:`mspasspy.util.Undertaker.Undertaker` class.   Invalid, atomic data</span>
<span class="sd">    will be the same type as the other bag/rdd components but will have</span>
<span class="sd">    the following properties that can be used to distinguish them:</span>
<span class="sd">        1.  They will have the Metadata field &quot;is_live&quot; set False.</span>
<span class="sd">        2.  The data object itself will have the interal attribute &quot;live&quot;</span>
<span class="sd">            set False.</span>
<span class="sd">        3.  The Metadata field with key &quot;is_abortion&quot; will be defined and set True.</span>
<span class="sd">        4.  The sample array will be zero length (datum.npts==0)</span>

<span class="sd">    Ensembles are still ensembles but they may contain dead data with</span>
<span class="sd">    the properties of atomic data noted above EXCEPT that the &quot;is_live&quot;.</span>
<span class="sd">    attribute will not be set - that is used only inside this function.</span>
<span class="sd">    An ensemble return will be marked dead only if all its members are found</span>
<span class="sd">    to be marked dead.</span>

<span class="sd">    :param data: variable type arguement used to drive construction as</span>
<span class="sd">      described above.  See above for how this argument drives the</span>
<span class="sd">      functions behavor.  Note when set as a Database handle the cursor</span>
<span class="sd">      argument must be set.  Otherwise it is ignored.</span>
<span class="sd">    :type data: :class:`mspasspy.db.database.Database` or :class:`pandas.DataFrame`</span>
<span class="sd">    or :class:`dask.dataframe.core.DataFrame` or :class:`pyspark.sql.dataframe.DataFrame`</span>
<span class="sd">    for atomic data.  List of python dicts defining queries to read a</span>
<span class="sd">    dataset of ensembles.</span>

<span class="sd">    :param db:  Database handle for loading data.   Required input if</span>
<span class="sd">      reading from a dataframe or with ensemble reading via list of queries.</span>
<span class="sd">      Ignored if the &quot;data&quot; parameter is a Database handle.</span>
<span class="sd">    :type db:  :class:`mspasspy.db.Database`.  Can be None (default)</span>
<span class="sd">      only if the data parameter contains the database handle.  Other uses</span>
<span class="sd">      require this argument to be set.</span>

<span class="sd">    :param query:  optional query to apply to input collection when using</span>
<span class="sd">      a :class:`mspasspy.db.Database` as input.  Ignored for dataframe or</span>
<span class="sd">      a list input.  Default is None which means no query is used.</span>
<span class="sd">    :type query:  python dict defining a valid MongoDB query.</span>

<span class="sd">    :param scratchfile: This argument is referenced only when input is</span>
<span class="sd">      drive by a :class:`mspasspy.db.Database` handle.  For very large</span>
<span class="sd">      datasets loading the entire set of documents that define the</span>
<span class="sd">      dataset into memory can be an issue on a system with smaller</span>
<span class="sd">      &quot;RAM&quot; memory available.  This optional argument makes this function</span>
<span class="sd">      scalable to the largest conceivable seismic data sets.  When defined</span>
<span class="sd">      the documents retreived from the database are reformatted and pushed to</span>
<span class="sd">      a scratch file with the name defined by this argument.  The contents</span>
<span class="sd">      of the file are then reloaded into a dask or spark DataFrame that</span>
<span class="sd">      allow the same data to be handled within a more limited memory footprint.</span>
<span class="sd">      Note use of this feature is rare and should never be necessary in an</span>
<span class="sd">      HPC or cloud cluster.   The default us None which means this that</span>
<span class="sd">      database input is loaded directly into memory to initiate construction</span>
<span class="sd">      of the parallel container output.</span>
<span class="sd">    :type scratchfile:  str</span>

<span class="sd">    :param collection:  waveform collection name for reading.  Default is</span>
<span class="sd">      &quot;wf_TimeSeries&quot;.</span>
<span class="sd">    :type collection: string</span>

<span class="sd">    :param mode: reading mode that controls how the function interacts with</span>
<span class="sd">      the schema definition for the data type.   Must be one of</span>
<span class="sd">      [&#39;promiscuous&#39;,&#39;cautious&#39;,&#39;pedantic&#39;].   See user&#39;s manual for a</span>
<span class="sd">      detailed description of what the modes mean.  Default is &#39;promiscuous&#39;</span>
<span class="sd">      which turns off all schema checks and loads all attributes defined for</span>
<span class="sd">      each object read.</span>
<span class="sd">    :type mode: :class:`str`</span>

<span class="sd">    :param normalize: List of normalizers.   This parameter is passed</span>
<span class="sd">      directly to the `Database.read_data` method internally.  See the</span>
<span class="sd">      docstring for that method for how this parameter is handled.</span>
<span class="sd">      For atomic data each component is used with the `normalize`</span>
<span class="sd">      function to apply one or more normalization operations to each</span>
<span class="sd">      datum.   For ensembles, the same operation is done in a loop</span>
<span class="sd">      over all ensembles members (i.e. the member objects are atomic</span>
<span class="sd">      and normalized in a loop.).  Use `normalize_ensemble` to</span>
<span class="sd">      set values in the ensemble Metadata container.</span>
<span class="sd">    :type normalize: must be a python list of subclasses of</span>
<span class="sd">      the abstract class :class:`mspasspy.db.normalize.BasicMatcher`</span>
<span class="sd">      that can be used as the normalization operator in the</span>
<span class="sd">      `normalize` function.</span>

<span class="sd">     param normalize_ensemble:  This parameter should be used to</span>
<span class="sd">       apply normalization to ensemble Metadata (attributes common to</span>
<span class="sd">       the entire ensemble.) It will be ignored if reading</span>
<span class="sd">       atomic data.  Otherwise it behaves like normalize and is</span>
<span class="sd">       assumed to a list of subclasses of :class:`BasicMatcher` objects.</span>
<span class="sd">       If using this option you must also specify a valid value for</span>
<span class="sd">       the `container_to_merge` argument.  The reason is that currently</span>
<span class="sd">       the only efficient way to post any Metadata components to</span>
<span class="sd">       an ensemble&#39;s Metadata container is via the algorithm used by</span>
<span class="sd">       if the `container_to_merge` option is used.  This feature was</span>
<span class="sd">       designed with ids in mind where the ids would link to a collection</span>
<span class="sd">       that are contain defining properties for what the ensemble is.</span>
<span class="sd">       For example, if the ensemble is a &quot;common-source gather&quot;</span>
<span class="sd">       the `container_to_merge` could be a bag/RDD of ObjectIds defining</span>
<span class="sd">       the `source_id` attribute.  Then the normalize_ensemble list</span>
<span class="sd">       could contain an instance of :class:`mspasspy.db.normalize.ObjectIdMatcher`</span>
<span class="sd">       created to match and load source data.</span>
<span class="sd">     :type normalize_ensemble:  a :class:`list` of :class:`BasicMatcher`.</span>
<span class="sd">       :class:`BasicMatchers` are applied sequentialy with the</span>
<span class="sd">       `normalize` function with the list of attributes loaded defined</span>
<span class="sd">       by the instance.</span>

<span class="sd">    :param load_history: boolean (True or False) switch used to enable or</span>
<span class="sd">      disable object level history mechanism.   When set True each datum</span>
<span class="sd">      will be tagged with its origin id that defines the leaf nodes of a</span>
<span class="sd">      history G-tree.  See the User&#39;s manual for additional details of this</span>
<span class="sd">      feature.  Default is False.</span>

<span class="sd">    :param exclude_keys: Sometimes it is helpful to remove one or more</span>
<span class="sd">      attributes stored in the database from the data&#39;s Metadata (header)</span>
<span class="sd">      so they will not cause problems in downstream processing.</span>
<span class="sd">    :type exclude_keys: a :class:`list` of :class:`str`</span>

<span class="sd">    :param scheduler: Set the format of the parallel container to define the</span>
<span class="sd">      dataset.   Must be either &quot;spark&quot; or &quot;dask&quot; or the job will abort</span>
<span class="sd">      immediately with a ValueError exception</span>
<span class="sd">    :type scheduler: :class:`str`</span>

<span class="sd">    :param spark_context: If using spark this argument is required.  Spark</span>
<span class="sd">      defines the concept of a &quot;context&quot; that is a global control object that</span>
<span class="sd">      manages schduling.  See online Spark documentation for details on</span>
<span class="sd">      this concept.</span>
<span class="sd">    :type spark_context: :class:`pyspark.SparkContext`</span>

<span class="sd">    :param npartitions: The number of desired partitions for Dask or the number</span>
<span class="sd">      of slices for Spark. By default Dask will use 100 and Spark will determine</span>
<span class="sd">      it automatically based on the cluster.  If using this parameter and</span>
<span class="sd">      a container_to_merge make sure the number used here matches the partitioning</span>
<span class="sd">      of container_to_merge.  If specified and container_to_merge</span>
<span class="sd">      is defined this function will test them for consistency and throw a</span>
<span class="sd">      ValueError exception if they don&#39;t match.  If not set (i.e. left</span>
<span class="sd">      default of None) that test is not done and the function assumes</span>
<span class="sd">      container_to_merge also uses default partitioning.</span>
<span class="sd">    :type npartitions: :class:`int`</span>

<span class="sd">    :param data_tag:  The definition of a dataset can become ambiguous</span>
<span class="sd">      when partially processed data are saved within a workflow.   A common</span>
<span class="sd">      example would be windowing long time blocks of data to shorter time</span>
<span class="sd">      windows around a particular seismic phase and saving the windowed data.</span>
<span class="sd">      The windowed data can be difficult to distinguish from the original</span>
<span class="sd">      with standard queries.  For this reason we make extensive use of &quot;tags&quot;</span>
<span class="sd">      for save and read operations to improve the efficiency and simplify</span>
<span class="sd">      read operations.   Default turns this off by setting the tag null (None).</span>
<span class="sd">    :type data_tag: :class:`str`</span>

<span class="sd">    :param sort_clause:  When reading ensembles it is sometimes helpful to</span>
<span class="sd">      apply a sort clause to each database query.  The type example is</span>
<span class="sd">      reading continuous data where there it is necessary to sort the</span>
<span class="sd">      data into channels in time order.  If defined this should be a clause</span>
<span class="sd">      that can be inserted in the MongoDB sort method commonly applied in</span>
<span class="sd">      a line like this:  `cursor=db.wf_miniseed.find(query).sort(sort_clause)`.</span>
<span class="sd">      This argument is tested for existence only when reading ensembles</span>
<span class="sd">      (implied with list of dict input).</span>
<span class="sd">    :type sort_clause:  if None (default) no sorting is invoked when reading</span>
<span class="sd">      ensembles.   Other wise should be a python list of tuples</span>
<span class="sd">      defining a sort order.</span>
<span class="sd">      e.g. [(&quot;sta&quot;,pymongo.ASCENDING),()&quot;time&quot;,pymongo.ASCENDING)]</span>

<span class="sd">    :param container_to_merge:  bag/RDD containing data packaged with</span>
<span class="sd">      one item per datum this reader is asked to read.   See above for</span>
<span class="sd">      details and examples of how this feature can be used.  Default is</span>
<span class="sd">      None which turns off this option.</span>
<span class="sd">    :type container_to_merge:  dask bag or pyspark RDD.   The number of</span>
<span class="sd">      partitions in the input must match the explicit</span>
<span class="sd">      (i.e. set with `npartitions`) or implict (defaulted) number of</span>
<span class="sd">      partitions.</span>

<span class="sd">    :param container_merge_function:  function that defines what to do</span>
<span class="sd">      with components of the `container_to_merge` if it is defined.</span>
<span class="sd">      Default is a Metadata merge function, which is defined internally</span>
<span class="sd">      in this module.  That function assumes `container_to_merge`</span>
<span class="sd">      is a bag/RDD of either `Metadata` or python dict containers that</span>
<span class="sd">      define key-value pairs to be posted to the output.  (i.e. it</span>
<span class="sd">      act like the Metadata += operator.)  A custom function can be</span>
<span class="sd">      used here.  A custom function must have only two arguments</span>
<span class="sd">      with arg0 the target seismic datum (component the reader is</span>
<span class="sd">      creating) and arg1 defining attributes to use to edit the</span>
<span class="sd">      datum being created.   Note the default only alters Metadata but</span>
<span class="sd">      that is not at all a restriction.  The function must simply return</span>
<span class="sd">      a (normally modified) copy of the component it receives as arg0.</span>

<span class="sd">    :param aws_access_key_id: A part of the credentials to authenticate the user</span>
<span class="sd">    :param aws_secret_access_key: A part of the credentials to authenticate the user</span>
<span class="sd">    :return: container defining the parallel dataset.  A spark `RDD` if scheduler</span>
<span class="sd">      is &quot;Spark&quot; and a dask &#39;bag&#39; if scheduler is &quot;dask&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This is a base error message that is an initialization for</span>
    <span class="c1"># any throw error.  We first two type checking of arg0</span>
    <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;read_distributed_data:  &quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">scheduler</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;dask&quot;</span><span class="p">,</span> <span class="s2">&quot;spark&quot;</span><span class="p">]:</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Unsupported value for scheduler=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scheduler</span><span class="p">)</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be either &#39;dask&#39; or &#39;spark&#39;&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">ensemble_mode</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;arg0 is a list, but component </span><span class="si">{}</span><span class="s2"> has illegal type=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">i</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;list must contain only python dict defining queries that define each ensemble to be loaded&quot;</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sort_clause</span><span class="p">:</span>
            <span class="c1"># TODO - conflicting examples of the type of this clause</span>
            <span class="c1"># may have a hidden bug in TimeIntervalReader as it has usage</span>
            <span class="c1"># differnt from this restricteion</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sort_clause</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">str</span><span class="p">]):</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;sort_clause argument is invalid</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be either a list or a single string&quot;</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sort_clause</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sort_clause</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;sort_clause value = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sort_clause</span><span class="p">)</span>
                        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot; is invalid input for MongoDB&quot;</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">Database</span><span class="p">):</span>
        <span class="n">ensemble_mode</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">dataframe_input</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">data</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span>
        <span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">dask</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="n">ensemble_mode</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">dataframe_input</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">Database</span><span class="p">):</span>
            <span class="n">db</span> <span class="o">=</span> <span class="n">db</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">db</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type=</span><span class="si">{}</span><span class="s2"> for db argument - required with dataframe input&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">db</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Usage error. An instance of Database class is required for db argument when input is a dataframe&quot;</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type=</span><span class="si">{}</span><span class="s2"> for arg0</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be a Dataframe (pandas, spark, or dask), Database, or a list of query dictionaries&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalize</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">nrm</span> <span class="ow">in</span> <span class="n">normalize</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nrm</span><span class="p">,</span> <span class="n">BasicMatcher</span><span class="p">):</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="s2">&quot;Illegal type=</span><span class="si">{}</span><span class="s2"> for component </span><span class="si">{}</span><span class="s2"> of normalize list</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="nb">type</span><span class="p">(</span><span class="n">nrm</span><span class="p">),</span> <span class="n">i</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be subclass of BasicMatcher to allow use in normalize function&quot;</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type for normalize argument = </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be a python list of implementations of BasicMatcher&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">container_to_merge</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">container_to_merge</span><span class="p">,</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">RDD</span><span class="p">):</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;container_to_merge must define a pyspark RDD with scheduler==spark&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="n">container_partitions</span> <span class="o">=</span> <span class="n">container_to_merge</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">container_to_merge</span><span class="p">,</span> <span class="n">dask</span><span class="o">.</span><span class="n">bag</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Bag</span><span class="p">):</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;container_to_merge must define a dask bag with scheduler==dask&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="n">container_partitions</span> <span class="o">=</span> <span class="n">container_to_merge</span><span class="o">.</span><span class="n">npartitions</span>
        <span class="k">if</span> <span class="n">npartitions</span><span class="p">:</span>
            <span class="c1"># This error handler only works if npartitions is set in the</span>
            <span class="c1"># arg list.  Can&#39;t test the data container here as it doesn&#39;t</span>
            <span class="c1"># exist yet and putting it inside the logic below would be awkward</span>
            <span class="c1"># and could slow execution</span>
            <span class="k">if</span> <span class="n">container_partitions</span> <span class="o">!=</span> <span class="n">npartitions</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;container_to_merge number of partitions=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">container_partitions</span>
                <span class="p">)</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
                    <span class="s2">&quot;must match value of npartitions passed to function=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">npartitions</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">container_merge_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;normalize_ensemble option requires specifying a bag/RDD passed via container_to_merge argument</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Received a (default) None value for container_to_merge argument&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalize_ensemble</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">nrm</span> <span class="ow">in</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nrm</span><span class="p">,</span> <span class="n">BasicMatcher</span><span class="p">):</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type=</span><span class="si">{}</span><span class="s2"> for component </span><span class="si">{}</span><span class="s2"> of normalize_ensemble list</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="nb">type</span><span class="p">(</span><span class="n">nrm</span><span class="p">),</span> <span class="n">i</span>
                    <span class="p">)</span>
                    <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be subclass of BasicMatcher to allow use in normalize function&quot;</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type for normalize_ensemble argument = </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">type</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be a python list of implementations of BasicMatcher&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="c1"># This has a fundamentally different algorithm for handling</span>
    <span class="c1"># ensembles than atomic data.  Ensembles are driven by a list of</span>
    <span class="c1"># queries while atomic data are driven by a dataframe.</span>
    <span class="c1"># the dataframe is necessary in this context because MongoDB</span>
    <span class="c1"># cursors can not be serialized while Database can.</span>
    <span class="k">if</span> <span class="n">ensemble_mode</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalize_ensemble</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">nrm</span> <span class="ow">in</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nrm</span><span class="p">,</span> <span class="n">BasicMatcher</span><span class="p">):</span>
                        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type=</span><span class="si">{}</span><span class="s2"> for component </span><span class="si">{}</span><span class="s2"> of normalize list</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="nb">type</span><span class="p">(</span><span class="n">nrm</span><span class="p">),</span> <span class="n">i</span>
                        <span class="p">)</span>
                        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be subclass of BasicMatcher to allow use in normalize function&quot;</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Illegal type for normallze_ensemble argument = </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="nb">type</span><span class="p">(</span><span class="n">normalize_ensemble</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be a python list of implementations of BasicMatcher&quot;</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
            <span class="c1"># note this works only because parallelize treats a None as default</span>
            <span class="c1"># and we use None as our default too - could break with version change</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">spark_context</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numSlices</span><span class="o">=</span><span class="n">npartitions</span><span class="p">)</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">read_ensemble_parallel</span><span class="p">(</span>
                    <span class="n">q</span><span class="p">,</span>
                    <span class="n">db</span><span class="p">,</span>
                    <span class="n">collection</span><span class="o">=</span><span class="n">collection</span><span class="p">,</span>
                    <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                    <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
                    <span class="n">load_history</span><span class="o">=</span><span class="n">load_history</span><span class="p">,</span>
                    <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
                    <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                    <span class="n">sort_clause</span><span class="o">=</span><span class="n">sort_clause</span><span class="p">,</span>
                    <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">aws_access_key_id</span><span class="p">,</span>
                    <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">aws_secret_access_key</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># note same maintenance issue as with parallelize above</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">bag</span><span class="o">.</span><span class="n">from_sequence</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">npartitions</span><span class="p">)</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">read_ensemble_parallel</span><span class="p">,</span>
                <span class="n">db</span><span class="p">,</span>
                <span class="n">collection</span><span class="o">=</span><span class="n">collection</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
                <span class="n">load_history</span><span class="o">=</span><span class="n">load_history</span><span class="p">,</span>
                <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
                <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                <span class="n">sort_clause</span><span class="o">=</span><span class="n">sort_clause</span><span class="p">,</span>
                <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">aws_access_key_id</span><span class="p">,</span>
                <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">aws_secret_access_key</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Logic here gets a bit complex to handle the multiple inputs</span>
        <span class="c1"># possible for atomic data.  That is, multiple dataframe</span>
        <span class="c1"># implementations and a direct database reading mechanism.</span>
        <span class="c1"># The database instance is assumed to converted to a bag/rdd of docs</span>
        <span class="c1"># above.  We use a set of internal variables to control the</span>
        <span class="c1"># input block used.</span>
        <span class="c1"># TODO:  clean up symbol - plist is an awful name</span>
        <span class="k">if</span> <span class="n">dataframe_input</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
                <span class="n">plist</span> <span class="o">=</span> <span class="n">spark_context</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span>
                    <span class="n">data</span><span class="o">.</span><span class="n">to_dict</span><span class="p">(</span><span class="s2">&quot;records&quot;</span><span class="p">),</span> <span class="n">numSlices</span><span class="o">=</span><span class="n">npartitions</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Seems we ahve to convert a pandas df to a dask</span>
                <span class="c1"># df to have access to the &quot;to_bag&quot; method of dask</span>
                <span class="c1"># DataFrame.   It may be better to write a small</span>
                <span class="c1"># converter run with a map operator row by row</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">):</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">dataframe</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">npartitions</span><span class="p">)</span>
                <span class="c1"># format arg s essential as default is tuple</span>
                <span class="n">plist</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_bag</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s2">&quot;dict&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># logic above should guarantee data is a Database</span>
            <span class="c1"># object that can be queried and used to generate the bag/rdd</span>
            <span class="c1"># needed to read data in parallel immediately after this block</span>
            <span class="k">if</span> <span class="n">query</span><span class="p">:</span>
                <span class="n">fullquery</span> <span class="o">=</span> <span class="n">query</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fullquery</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">data_tag</span><span class="p">:</span>
                <span class="n">fullquery</span><span class="p">[</span><span class="s2">&quot;data_tag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_tag</span>
            <span class="n">cursor</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">collection</span><span class="p">]</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="n">fullquery</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">scratchfile</span><span class="p">:</span>
                <span class="c1"># here we write the documents all to a scratch file in</span>
                <span class="c1"># json and immediately read them back to create a bag or RDD</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">scratchfile</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">outfile</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
                        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">outfile</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
                    <span class="c1"># the only way I could find to load json data in pyspark</span>
                    <span class="c1"># is to use an intermediate dataframe.   This should</span>
                    <span class="c1"># still parallelize, but will probably be slower</span>
                    <span class="c1"># if there is a more direct solution should be done here.</span>
                    <span class="n">plist</span> <span class="o">=</span> <span class="n">spark_context</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">scratchfile</span><span class="p">)</span>
                    <span class="c1"># this is wrong above also don&#39;t see how to do partitions</span>
                    <span class="c1"># this section is broken until I (glp) can get help</span>
                    <span class="c1"># plist = plist.map(to_dict,&quot;records&quot;)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">plist</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">bag</span><span class="o">.</span><span class="n">read_text</span><span class="p">(</span><span class="n">scratchfile</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">)</span>
                <span class="c1"># Intentionally omit error handler here.  Assume</span>
                <span class="c1"># system will throw an error if file open files or write files</span>
                <span class="c1"># that will be sufficient for user to understand the problem.</span>
                <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">scratchfile</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">doclist</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
                    <span class="n">doclist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
                    <span class="n">plist</span> <span class="o">=</span> <span class="n">spark_context</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">doclist</span><span class="p">,</span> <span class="n">numSlices</span><span class="o">=</span><span class="n">npartitions</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">plist</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">bag</span><span class="o">.</span><span class="n">from_sequence</span><span class="p">(</span><span class="n">doclist</span><span class="p">,</span> <span class="n">npartitions</span><span class="o">=</span><span class="n">npartitions</span><span class="p">)</span>
                <span class="k">del</span> <span class="n">doclist</span>

        <span class="c1"># Earlier logic make list a bag/rdd of docs - above converts dataframe to same</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="n">db</span><span class="o">.</span><span class="n">read_data</span><span class="p">(</span>
                    <span class="n">doc</span><span class="p">,</span>
                    <span class="n">collection</span><span class="o">=</span><span class="n">collection</span><span class="p">,</span>
                    <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
                    <span class="n">load_history</span><span class="o">=</span><span class="n">load_history</span><span class="p">,</span>
                    <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
                    <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                    <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">aws_access_key_id</span><span class="p">,</span>
                    <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">aws_secret_access_key</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">db</span><span class="o">.</span><span class="n">read_data</span><span class="p">,</span>
                <span class="n">collection</span><span class="o">=</span><span class="n">collection</span><span class="p">,</span>
                <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">,</span>
                <span class="n">load_history</span><span class="o">=</span><span class="n">load_history</span><span class="p">,</span>
                <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
                <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                <span class="n">aws_access_key_id</span><span class="o">=</span><span class="n">aws_access_key_id</span><span class="p">,</span>
                <span class="n">aws_secret_access_key</span><span class="o">=</span><span class="n">aws_secret_access_key</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="c1"># all cases create plist.   Run the container_merge_function function</span>
    <span class="c1"># if requested</span>
    <span class="k">if</span> <span class="n">container_to_merge</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span><span class="n">container_to_merge</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">container_merge_function</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plist</span> <span class="o">=</span> <span class="n">dask</span><span class="o">.</span><span class="n">bag</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">container_merge_function</span><span class="p">,</span> <span class="n">plist</span><span class="p">,</span> <span class="n">container_to_merge</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">nrm</span> <span class="ow">in</span> <span class="n">normalize_ensemble</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
                <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">normalize_function</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">nrm</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">plist</span> <span class="o">=</span> <span class="n">plist</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">normalize_function</span><span class="p">,</span> <span class="n">nrm</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">plist</span></div>


<span class="k">def</span> <span class="nf">_partitioned_save_wfdoc</span><span class="p">(</span>
    <span class="n">partition_iterator</span><span class="p">,</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;wf_TimeSeries&quot;</span><span class="p">,</span>
    <span class="n">dbname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal function used to save the core wf document data for atomic mspass</span>
<span class="sd">    data objects.</span>

<span class="sd">    This function is intended for internal use only as a component of</span>
<span class="sd">    the function write_distributed_data.   It uses a bulk write</span>
<span class="sd">    method (pymongo&#39;s insert_many method) to reduce database</span>
<span class="sd">    transaction delays.</span>

<span class="sd">    This function has a strong dependency on a method for handling</span>
<span class="sd">    dead data within `write_distributed_data`.   That is, each document</span>
<span class="sd">    it processes is ASSUMED (there are no error handlers since</span>
<span class="sd">    this is an internal function) to have a boolean value set with the</span>
<span class="sd">    key &quot;llve&quot;.   When True that document is pushed to MongoDB.  When</span>
<span class="sd">    False the contents of the document are ignored.  The return of this</span>
<span class="sd">    function is a list of MongoDB `ObjectId`s of live data inserted</span>
<span class="sd">    with a None in the list for any datum marked dead (&quot;live&quot;:False).</span>
<span class="sd">    This causes the size of the return list to be the same as the</span>
<span class="sd">    input iterator length (normally partition size, but usually</span>
<span class="sd">    truncated at the last partition).   dask and pyspark handle the</span>
<span class="sd">    partitioned list and concatenate the sequence of lists into a single</span>
<span class="sd">    long list.   Counting None values in that list provides a way to</span>
<span class="sd">    cross-check the number of data killed in a workflow.</span>

<span class="sd">    The function has a feature not used at present, but was preserved here</span>
<span class="sd">    as an idea.  At present there is a workaround to provide a</span>
<span class="sd">    mechanism to serialize a mspasspy.db.Database class.   When the</span>
<span class="sd">    function is called with default parameters it assumes the db</span>
<span class="sd">    argument will serialize in dask/spark.   If db is set to None</span>
<span class="sd">    each call to the function will construct a</span>
<span class="sd">    :class:`mspasspy.db.Database` object for call to the function.</span>
<span class="sd">    Because it is done by partition the cost scales by the number of</span>
<span class="sd">    partitions not the number of data items to use that algorithm.</span>

<span class="sd">    :param partition_iterator :  This parameter is an iterator</span>
<span class="sd">      that is expected to contain a series of python dict containers</span>
<span class="sd">      defining the translation of data Metadata to the format needed</span>
<span class="sd">      by pymong (a python dict).  It could be used as a regular function</span>
<span class="sd">      with a python list, but normal use with dask map_partition or</span>
<span class="sd">      pyspark&#39;s mapPartition send the function an iterable that can</span>
<span class="sd">      only be traversed ONCE.   In that situation, the iterator value</span>
<span class="sd">      is a component of the bag/RDD that is being handled and the entire</span>
<span class="sd">      range received in a single call is defined by the partition size of</span>
<span class="sd">      the bag/RDD.</span>
<span class="sd">    :type partition_iterator:  iterable container of python dict</span>
<span class="sd">      translated from seismic data Metadata.</span>

<span class="sd">    :param db:   Should normally be a database handle that is to be used to</span>
<span class="sd">    save the documents stored in partition_iterator .  Set to None if you want to</span>
<span class="sd">    have the function use the feature of creating a new handle for</span>
<span class="sd">    each instance to avoid serialization.</span>
<span class="sd">    :type db:  :class:`mspasspy.db.Database` or None.   Type is not tested</span>
<span class="sd">    for efficiency so the function would likely abort with ambiguous messages</span>
<span class="sd">    when used in a parallel workflow.</span>

<span class="sd">    :param collection:  wf collection name. Default &quot;wf_TimeSeries&quot;.</span>

<span class="sd">    :param dbname:  database name to save data into.  This parameter is</span>
<span class="sd">    referenced ONLY if db is set to None.</span>
<span class="sd">    :type dbname:  string</span>

<span class="sd">    :return:  a python list of ObjectIds of the inserted documents</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This makes the function more bombproof in the event a database</span>
    <span class="c1"># handle can&#39;t be serialized - db should normally be defined</span>
    <span class="k">if</span> <span class="n">db</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dbclient</span> <span class="o">=</span> <span class="n">DBClient</span><span class="p">()</span>
        <span class="c1"># needs to throw an exception of both db and dbname are none</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">dbclient</span><span class="o">.</span><span class="n">get_database</span><span class="p">(</span><span class="n">dbname</span><span class="p">)</span>
    <span class="n">dbcol</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">collection</span><span class="p">]</span>
    <span class="c1"># test for the existence of any dead data.  Handle that case specially</span>
    <span class="c1"># Very important to realize the algorithm is complicated by the fact that</span>
    <span class="c1"># partition_iterator can normally only be traversed once.  Hence</span>
    <span class="c1"># we copy documents to a new container (cleaned doclit) and define the pattern of</span>
    <span class="c1"># anty dead data with the boolean list lifelist</span>
    <span class="n">has_bodies</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">docarray</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">partition_iterator</span><span class="p">:</span>
        <span class="c1"># clear the wfid if it exists or mongo may overwrite</span>
        <span class="k">if</span> <span class="s2">&quot;_id&quot;</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
            <span class="n">doc</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_id&quot;</span><span class="p">)</span>
        <span class="n">docarray</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;live&quot;</span><span class="p">]:</span>
            <span class="n">has_bodies</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">has_bodies</span><span class="p">:</span>
        <span class="n">lifelist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">cleaned_doclist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docarray</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;live&quot;</span><span class="p">]:</span>
                <span class="n">cleaned_doclist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
                <span class="n">lifelist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lifelist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cleaned_doclist</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">wfids_inserted</span> <span class="o">=</span> <span class="n">dbcol</span><span class="o">.</span><span class="n">insert_many</span><span class="p">(</span><span class="n">cleaned_doclist</span><span class="p">)</span><span class="o">.</span><span class="n">inserted_ids</span>
            <span class="n">wfids</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">ii</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lifelist</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">lifelist</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                    <span class="n">wfids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wfids_inserted</span><span class="p">[</span><span class="n">ii</span><span class="p">])</span>
                    <span class="n">ii</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">wfids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">wfids</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">docarray</span><span class="p">)):</span>
                <span class="n">wfids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># this case is much simpler</span>
        <span class="c1"># note plural ids.  insert_one uses &quot;inserted_id&quot;.</span>
        <span class="c1"># proper english usage but potentially confusing - beware</span>
        <span class="n">wfids</span> <span class="o">=</span> <span class="n">dbcol</span><span class="o">.</span><span class="n">insert_many</span><span class="p">(</span><span class="n">docarray</span><span class="p">)</span><span class="o">.</span><span class="n">inserted_ids</span>

    <span class="k">return</span> <span class="n">wfids</span>


<div class="viewcode-block" id="pyspark_mappartition_interface"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.pyspark_mappartition_interface">[docs]</a><span class="k">class</span> <span class="nc">pyspark_mappartition_interface</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interface class required for pyspark mapPartition.</span>

<span class="sd">    This class is a workaround for a limitation of pyspark&#39;s api for</span>
<span class="sd">    mapPartition that is a legacy of its scala foundation.   mapPartition</span>
<span class="sd">    only accepts a function name as arg0 and has no provision for</span>
<span class="sd">    any optional arguments to the function.   An alternative solution</span>
<span class="sd">    found in web sources was &quot;currying&quot; and it might have been possible</span>
<span class="sd">    to do this with just a function wrapper with fixed kwarg values</span>
<span class="sd">    defined with in the `write_distributed_data` function.   In this case,</span>
<span class="sd">    however, because `write_distributed_data` handles an entire bag/rdd</span>
<span class="sd">    as input a class can be created at the initialization stage of that</span>
<span class="sd">    function.  Otherwise the overhead of a wrapper would likely be smaller.</span>

<span class="sd">    Perhaps the feature most useful to make this a class is the</span>
<span class="sd">    safety valve if db is a None.  (see below)</span>

<span class="sd">    The method `partioned_save_wfdoc` is just an alias for</span>
<span class="sd">    the file scope function `_partitioned_save_wfdoc` with the parameters</span>
<span class="sd">    for that function defined by self content.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">db</span><span class="p">,</span>
        <span class="n">collection</span><span class="p">,</span>
        <span class="n">dbname</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Constructor - called in write_distributed_data.</span>

<span class="sd">        :param db:  database handle to use for saving wf documents with</span>
<span class="sd">          by partition.  If passed a None for this value the constructor</span>
<span class="sd">          will attempt to create a default connection to MongoDB and</span>
<span class="sd">          use the dbname argument to fetch the database to use.</span>

<span class="sd">        :collection:  name of MongoDB collection where documents are to be</span>
<span class="sd">          written.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">db</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dbname</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;pyspark_mappartion_interface constructor:  invalid parameter combination</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Both db (arg0) and dbname (arg2) values are None.  One or the other must be defined&quot;</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
            <span class="n">dbclient</span> <span class="o">=</span> <span class="n">DBClient</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">dbclient</span><span class="o">.</span><span class="n">get_database</span><span class="p">(</span><span class="n">dbname</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="n">db</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collection</span> <span class="o">=</span> <span class="n">collection</span>

<div class="viewcode-block" id="pyspark_mappartition_interface.partitioned_save_wfdoc"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.pyspark_mappartition_interface.partitioned_save_wfdoc">[docs]</a>    <span class="k">def</span> <span class="nf">partitioned_save_wfdoc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Method used as a wrapper to pass on to pyspark&#39;s mapPartitions</span>
<span class="sd">        operator.  iterator is assumed to be the iterator passed to</span>
<span class="sd">        the function per partition by mapPartitions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">wfidlist</span> <span class="o">=</span> <span class="n">_partitioned_save_wfdoc</span><span class="p">(</span>
            <span class="n">iterator</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">,</span> <span class="n">collection</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collection</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">wfidlist</span></div></div>


<div class="viewcode-block" id="post_error_log"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.post_error_log">[docs]</a><span class="k">def</span> <span class="nf">post_error_log</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">other_elog</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">elog_key</span><span class="o">=</span><span class="s2">&quot;error_log&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Posts error log data as a subdocument to arg0 datum (symbol d).</span>

<span class="sd">    write_distributed_data has a &quot;post_elog&quot; boolean.  By default</span>
<span class="sd">    elog entries for any atomic seismic datum will be posted</span>
<span class="sd">    one-at-a-time with insert_one calls to the &quot;elog&quot; collection.</span>
<span class="sd">    If a workflow expects a large number of error log entries that</span>
<span class="sd">    high database traffic can be a bottleneck.  This function is</span>
<span class="sd">    used within `write_distributed_data` to avoid that by posting</span>
<span class="sd">    the same data to subdocuments attached to wf documents saved</span>
<span class="sd">    with live data.</span>

<span class="sd">    Note dead data are never handled by this mechanism.  They always</span>
<span class="sd">    end up in either the abortions or cemetery collections.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">elog</span> <span class="o">=</span> <span class="n">ErrorLogger</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">elog</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">elog</span>
    <span class="k">elif</span> <span class="n">other_elog</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">other_elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">elog</span> <span class="o">+=</span> <span class="n">other_elog</span>
    <span class="k">if</span> <span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">elogdoc</span> <span class="o">=</span> <span class="n">elog2doc</span><span class="p">(</span><span class="n">elog</span><span class="p">)</span>
        <span class="n">doc</span><span class="p">[</span><span class="n">elog_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">elogdoc</span>
    <span class="k">return</span> <span class="n">doc</span></div>


<span class="k">def</span> <span class="nf">_save_ensemble_wfdocs</span><span class="p">(</span>
    <span class="n">ensemble_data</span><span class="p">,</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">save_schema</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">,</span>
    <span class="n">undertaker</span><span class="p">,</span>
    <span class="n">normalizing_collections</span><span class="p">,</span>
    <span class="n">cremate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">post_elog</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">save_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">post_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">history_key</span><span class="o">=</span><span class="s2">&quot;history_data&quot;</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Internal function that saves wf documents for all live members of</span>
<span class="sd">    ensemkble objects.</span>


<span class="sd">    Ensembles have a built in efficiency in database transactions</span>
<span class="sd">    not possible with atomic data.  That is, bulk inserts defined</span>
<span class="sd">    by the number of live members in the ensemble are a natural way to</span>
<span class="sd">    save the documents extracted from ensemble member Metadata.</span>

<span class="sd">    Enembles have a complextity in handling data marked dead.  First, if</span>
<span class="sd">    an ensemble itself is marked dead the function assumes earlier logic</span>
<span class="sd">    marked all the members dead.   That assumption is true in this context</span>
<span class="sd">    as this is an internal function, but be cautious if this code is</span>
<span class="sd">    reused elsewhere.   For normal use, the Undertaker is called</span>
<span class="sd">    in a way that separates the living and the dead</span>
<span class="sd">    (a application of the best python joke ever:  bring_out_your_dead).</span>
<span class="sd">    The dead are buried by default.  If cremate is set true dead members</span>
<span class="sd">    are vaporized leaving no trace in the database.</span>

<span class="sd">    The booleans post_elog and post_history can impact performance of</span>
<span class="sd">    this function.   Default is the fastest mode where post_elog is</span>
<span class="sd">    set True and save_history is off (False).  When post_elog is set</span>
<span class="sd">    False, any error log entries will will cause error log data to</span>
<span class="sd">    be saved to the &quot;elog&quot; collection one document at a time.</span>
<span class="sd">    Similarly, if save_history is set True and the history feature is</span>
<span class="sd">    enabled every datum will generate a call to save a document in the</span>
<span class="sd">    &quot;history&quot; collection.  The idea is one would not do that unless the</span>
<span class="sd">    dataset is fairly small or other steps are a bigger throttle on the</span>
<span class="sd">    throughput.  For large data sets with history, one should also set</span>
<span class="sd">    post_history True.  Then the history data will be posted as a</span>
<span class="sd">    subdocument with the wf documents saved by this function for</span>
<span class="sd">    each live ensemble member.</span>

<span class="sd">    :param ensemble_data:   ensemble object containing data to</span>
<span class="sd">    be saved.</span>
<span class="sd">    :type ensemble_data:  assumed to be a TimeSeriesEsnemble or</span>
<span class="sd">    SeismgoramEnsembles.  Because this is an internal function there</span>
<span class="sd">    are no safeties to test that assumption.</span>
<span class="sd">    :param db:  Database handle for all database saves</span>
<span class="sd">    :param save_schema:  schema object - passed directly to md2doc.  See</span>
<span class="sd">    that function&#39;s docstring for description.</span>
<span class="sd">    :param exclude_keys:  list of metadata keys to discard when translating</span>
<span class="sd">      metadata to python dict (document).  Sent verbatim to md2doc.</span>
<span class="sd">    :param mode:  one of &quot;promiscuous&quot;, &quot;cautious&quot;, or &quot;pedantic&quot; used to</span>
<span class="sd">      define handling of mismatches between schema definitions defined by</span>
<span class="sd">      the save_schema argument and Metadata.  See Database docstring and</span>
<span class="sd">      User&#39;s manual for description of this common argument.</span>
<span class="sd">    :poram undertaker:  Instance of :class:`mspasspy.util.Undertaker`</span>
<span class="sd">      to handle dead data (see above)</span>
<span class="sd">    :param normalizing_collections: see docstring for `write_distributed_data`.</span>
<span class="sd">    :param cremate:  tells Undertaker how to handle dead data (see above)</span>
<span class="sd">    :param post_elog:  see above</span>
<span class="sd">    :param save_history:  see above</span>
<span class="sd">    :param post_history:  see above</span>
<span class="sd">    :history_key:  name to use for posting history data if post_history is</span>
<span class="sd">    set True.  Ignored if False.,</span>
<span class="sd">    :param data_tag:  Most data saved with `write_distributed_data` should</span>
<span class="sd">    use a data tag string to define the state of processing of that data.</span>
<span class="sd">    Default is None, but normal use should set it as an appropriate string</span>
<span class="sd">    defining what this dataset is.  D</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">cremate</span><span class="p">:</span>
        <span class="n">ensemble_data</span> <span class="o">=</span> <span class="n">undertaker</span><span class="o">.</span><span class="n">cremate</span><span class="p">(</span><span class="n">ensemble_data</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ensemble_data</span><span class="p">,</span> <span class="n">bodies</span> <span class="o">=</span> <span class="n">undertaker</span><span class="o">.</span><span class="n">bring_out_your_dead</span><span class="p">(</span><span class="n">ensemble_data</span><span class="p">)</span>
        <span class="n">undertaker</span><span class="o">.</span><span class="n">bury</span><span class="p">(</span><span class="n">bodies</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">bodies</span>

    <span class="c1"># Need to handle empty ensembles.  Undertaker removes dead bodies</span>
    <span class="c1"># for both bury and cremate from ensembles so we can end up</span>
    <span class="c1"># with an empty ensemble.  Normal return will be an empty list</span>
    <span class="c1"># for this case</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ensemble_data</span><span class="o">.</span><span class="n">member</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ensemble_data</span><span class="o">.</span><span class="n">dead</span><span class="p">():</span>
        <span class="n">wfids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">doclist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># we don&#39;t have to test for dead data in the loop below above removes</span>
        <span class="c1"># them so we just use md2doc.  We do, however, have to handle</span>
        <span class="c1"># md2doc failure signaled with aok False</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">ensemble_data</span><span class="o">.</span><span class="n">member</span><span class="p">:</span>
            <span class="n">doc</span><span class="p">,</span> <span class="n">aok</span><span class="p">,</span> <span class="n">elog</span> <span class="o">=</span> <span class="n">md2doc</span><span class="p">(</span>
                <span class="n">d</span><span class="p">,</span>
                <span class="n">save_schema</span><span class="o">=</span><span class="n">save_schema</span><span class="p">,</span>
                <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                <span class="n">normalizing_collections</span><span class="o">=</span><span class="n">normalizing_collections</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">aok</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">data_tag</span><span class="p">:</span>
                    <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;data_tag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_tag</span>
                <span class="k">if</span> <span class="s2">&quot;_id&quot;</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
                    <span class="n">doc</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_id&quot;</span><span class="p">)</span>
                <span class="c1"># Handle the error log if it is not empty</span>
                <span class="c1"># Either post it to the doc or push the entry to the database</span>
                <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">doc</span> <span class="o">=</span> <span class="n">post_error_log</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">other_elog</span><span class="o">=</span><span class="n">elog</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">d</span><span class="o">.</span><span class="n">elog</span> <span class="o">+=</span> <span class="n">elog</span>
                    <span class="n">elog_id</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_elog</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                    <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;elog_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elog_id</span>
                <span class="k">if</span> <span class="n">save_history</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">post_history</span><span class="p">:</span>
                        <span class="c1"># is_empty is part of ProcessingHistory</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="n">d</span><span class="o">.</span><span class="n">is_empty</span><span class="p">():</span>
                            <span class="n">doc</span> <span class="o">=</span> <span class="n">history2doc</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                            <span class="n">doc</span><span class="p">[</span><span class="n">history_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">history_id</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_history</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                        <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;history_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">history_id</span>
                <span class="k">if</span> <span class="n">data_tag</span><span class="p">:</span>
                    <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;data_tag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_tag</span>
                <span class="n">doclist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">d</span><span class="o">.</span><span class="n">elog</span> <span class="o">+=</span> <span class="n">elog</span>
                <span class="n">d</span><span class="o">.</span><span class="n">kill</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">cremate</span><span class="p">:</span>
                    <span class="n">d</span> <span class="o">=</span> <span class="n">undertaker</span><span class="o">.</span><span class="n">cremate</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">d</span> <span class="o">=</span> <span class="n">undertaker</span><span class="o">.</span><span class="n">bury</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

        <span class="c1"># weird trick to get waveform collection name - borrowed from</span>
        <span class="c1"># :class:`mspasspy.db.Database` save_data method code</span>
        <span class="n">wf_collection_name</span> <span class="o">=</span> <span class="n">save_schema</span><span class="o">.</span><span class="n">collection</span><span class="p">(</span><span class="s2">&quot;_id&quot;</span><span class="p">)</span>
        <span class="c1"># note plural ids.  insert_one uses &quot;inserted_id&quot;.</span>
        <span class="c1"># proper english usage but potentially confusing - beware</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">doclist</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">wfids</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">wf_collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert_many</span><span class="p">(</span><span class="n">doclist</span><span class="p">)</span><span class="o">.</span><span class="n">inserted_ids</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">wfids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="n">wfids</span>


<span class="k">def</span> <span class="nf">_atomic_extract_wf_document</span><span class="p">(</span>
    <span class="n">d</span><span class="p">,</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">save_schema</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">,</span>
    <span class="n">normalizing_collections</span><span class="p">,</span>
    <span class="n">post_elog</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">elog_key</span><span class="o">=</span><span class="s2">&quot;error_log&quot;</span><span class="p">,</span>
    <span class="n">post_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">save_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">history_key</span><span class="o">=</span><span class="s2">&quot;history_data&quot;</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">undertaker</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cremate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an internal function used in a map operator to</span>
<span class="sd">    extract the Metadata contents of atomic MsPASS seismic</span>
<span class="sd">    data objects returning an edited version of the contents</span>
<span class="sd">    as a python dictionary that write_distributed_data later passes</span>
<span class="sd">    to MongoDB to be saved as wf documents.</span>

<span class="sd">    This function does only half of the steps in the related function</span>
<span class="sd">    _save_ensemble_wfdocs.  That is it only does the metadata extraction</span>
<span class="sd">    and handling of dead data.  It leaves saving the wf documents to</span>
<span class="sd">    a the partitioned save function.</span>

<span class="sd">    If the datum received is marked dead it is handled by the</span>
<span class="sd">    instance of :class:`mspasspy.util.Undertaker` passed with the</span>
<span class="sd">    undertaker argument.  If cremate is set True the returned contents</span>
<span class="sd">    will be minimal.  All dead data will have the attribute &quot;live&quot;</span>
<span class="sd">    set to a boolean False.  All live data, will have tha value set True.</span>

<span class="sd">    Error log and history data handling are as described in the</span>
<span class="sd">    docstring for `write_distributed_data` with which this function is</span>
<span class="sd">    intimately linked.  Similarly all the argument descriptions can</span>
<span class="sd">    be found in that docstring.</span>

<span class="sd">    :return:   python dict translation of Metadata container of input</span>
<span class="sd">    datum d.   Note the return always has a boolean value associated</span>
<span class="sd">    with the key &quot;live&quot;.  That value is critical downstream from this</span>
<span class="sd">    function in the partition-based writer to assure the contents of</span>
<span class="sd">    dead data are not store in a wf collection.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">undertaker</span><span class="p">:</span>
        <span class="n">stedronsky</span> <span class="o">=</span> <span class="n">undertaker</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">stedronsky</span> <span class="o">=</span> <span class="n">Undertaker</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>

    <span class="n">doc</span><span class="p">,</span> <span class="n">aok</span><span class="p">,</span> <span class="n">elog_md2doc</span> <span class="o">=</span> <span class="n">md2doc</span><span class="p">(</span>
        <span class="n">d</span><span class="p">,</span>
        <span class="n">save_schema</span><span class="o">=</span><span class="n">save_schema</span><span class="p">,</span>
        <span class="n">exclude_keys</span><span class="o">=</span><span class="n">exclude_keys</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="n">normalizing_collections</span><span class="o">=</span><span class="n">normalizing_collections</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># cremate or bury dead data.</span>
    <span class="c1"># both return an edited data object reduced to ashes or a skeleton</span>
    <span class="c1"># doc and elog contents are handled separately.  When cremate is</span>
    <span class="c1"># true nothing will be saved in the database.  Default will</span>
    <span class="c1"># bury the body leaving a cemetery record.</span>

    <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">dead</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="n">aok</span><span class="p">):</span>
        <span class="c1"># this posts any elog content to error of d so bury will</span>
        <span class="c1"># save it</span>
        <span class="n">d</span><span class="o">.</span><span class="n">elog</span> <span class="o">+=</span> <span class="n">elog_md2doc</span>
        <span class="n">d</span><span class="o">.</span><span class="n">kill</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">cremate</span><span class="p">:</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">stedronsky</span><span class="o">.</span><span class="n">cremate</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">stedronsky</span><span class="o">.</span><span class="n">bury</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="c1"># make sure this is set as it is used in logic below</span>
        <span class="c1"># we use this instead of d to handle case with aok False</span>
        <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;live&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="c1"># d.kill()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;live&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">post_elog</span><span class="p">:</span>
            <span class="n">doc</span> <span class="o">=</span> <span class="n">post_error_log</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">doc</span><span class="p">,</span> <span class="n">other_elog</span><span class="o">=</span><span class="n">elog_md2doc</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">elog_md2doc</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">d</span><span class="o">.</span><span class="n">elog</span> <span class="o">+=</span> <span class="n">elog_md2doc</span>  <span class="c1"># does nothing if rhs is empty</span>
                <span class="n">elog_id</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_elog</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;elog_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elog_id</span>
        <span class="k">if</span> <span class="n">save_history</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">post_history</span><span class="p">:</span>
                <span class="c1"># is_empty is part of ProcessingHistory</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">d</span><span class="o">.</span><span class="n">is_empty</span><span class="p">():</span>
                    <span class="n">doc</span> <span class="o">=</span> <span class="n">history2doc</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                    <span class="n">doc</span><span class="p">[</span><span class="n">history_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">doc</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">history_id</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_history</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
                <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;history_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">history_id</span>
        <span class="k">if</span> <span class="n">data_tag</span><span class="p">:</span>
            <span class="n">doc</span><span class="p">[</span><span class="s2">&quot;data_tag&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_tag</span>
    <span class="k">return</span> <span class="n">doc</span>


<div class="viewcode-block" id="write_distributed_data"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.write_distributed_data">[docs]</a><span class="k">def</span> <span class="nf">write_distributed_data</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">data_are_atomic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;promiscuous&quot;</span><span class="p">,</span>
    <span class="n">storage_mode</span><span class="o">=</span><span class="s2">&quot;gridfs&quot;</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">file_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;wf_TimeSeries&quot;</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">post_elog</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">save_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">post_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">cremate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">normalizing_collections</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="s2">&quot;site&quot;</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">],</span>
    <span class="n">alg_name</span><span class="o">=</span><span class="s2">&quot;write_distributed_data&quot;</span><span class="p">,</span>
    <span class="n">alg_id</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel save function for termination of a processing script.</span>

<span class="sd">    Saving data from a parallel container (i.e. bag/rdd) is a different</span>
<span class="sd">    problem from a serial writer.  Bottlenecks are likely with</span>
<span class="sd">    communication delays talking to the MonboDB server and</span>
<span class="sd">    complexities of file based io.   Further, there are efficiency issues </span>
<span class="sd">    in the need to reduce transactions that cause delays with the MongoDB </span>
<span class="sd">    server.   This function tries to address these issues with two </span>
<span class="sd">    approaches:</span>
<span class="sd">        1.   Since v2 it uses single function that handles writing the </span>
<span class="sd">             sample data for a datum.   For atomic data that means a </span>
<span class="sd">             single array but for ensembles it is the combination of all</span>
<span class="sd">             arrays in the ensemble &quot;member&quot; containers.   The writing </span>
<span class="sd">             functions are passed through a map operator</span>
<span class="sd">             so writing is a parallelized per worker.  Note the function </span>
<span class="sd">             also abstracts how the data are written with different </span>
<span class="sd">             things done depending on the &quot;storage_mode&quot; attribute </span>
<span class="sd">             that can optionally be defined for each atomic object.</span>
<span class="sd">        2.   After the sample data are saved we use a MongoDB </span>
<span class="sd">             &quot;update_many&quot; operator with the &quot;many&quot; defined by the </span>
<span class="sd">             partition size.  That reduces database transaction delays </span>
<span class="sd">             by 1/object_per_partition.  For ensembles the partitioning </span>
<span class="sd">             is natural with bulk writes controlled by the number of </span>
<span class="sd">             ensemble members. </span>

<span class="sd">    The function also handles data marked dead in a standardized way </span>
<span class="sd">    though the use of the :class:`mspasspy.util.Undertaker` now </span>
<span class="sd">    defined within the Database class handle.   The default will </span>
<span class="sd">    call the `bury` method on all dead data which leaves a document </span>
<span class="sd">    containing the datum&#39;s Metadata and and error log messages in </span>
<span class="sd">    a collection called &quot;cemetery&quot;.   If the `cremate` argument is </span>
<span class="sd">    set True dead data will be vaporized and no trace of them will </span>
<span class="sd">    appear in output.  </span>

<span class="sd">    To further reduce database traffic the function has two </span>
<span class="sd">    (boolean) options called `post_elog` and `post_history`.  </span>
<span class="sd">    When set True the elog and/or object-level history data will </span>
<span class="sd">    be posted as subdocuments in the output collection instead of </span>
<span class="sd">    the normal (at least as defined by the Database handle) way </span>
<span class="sd">    these data are saved (In Database the error log is saved to the </span>
<span class="sd">    &quot;elog&quot; collection and the history data is saved to &quot;history&quot;.)</span>
<span class="sd">    Note post_history is ignored unless the related `save_history`</span>
<span class="sd">    argument is changed from the default False ot True.</span>

<span class="sd">    A peculiarity that is a consequence of python&#39;s &quot;duck typing&quot; is that </span>
<span class="sd">    the user must give the writer some hints about the type of data objects </span>
<span class="sd">    it is expected to handle.  Rather than specify a type argument, </span>
<span class="sd">    the type is inferred from two arguments that are necessary anyway:</span>
<span class="sd">    (1)  the `collection` argument value, and (2) the boolean </span>
<span class="sd">    `data_are_atomic`.   The idea is that `collection` is used to </span>
<span class="sd">    determine of the writer is handling TimeSeries or Seismogram </span>
<span class="sd">    objects (&quot;wf_TimeSeries&quot; or &quot;wf_Seismogram&quot; values respectively)\</span>
<span class="sd">    and the boolean is used, as the name implies, to infer if the </span>
<span class="sd">    data are atomic or ensembles.  </span>

<span class="sd">    This function should only be used as the terminal step </span>
<span class="sd">    of a parallel workflow (i.e. a chain of map/reduce operators).</span>
<span class="sd">    This function will ALWAYS initiate a lazy computation on such a chain of </span>
<span class="sd">    operators because it calls the &quot;compute&quot; method for dask and the </span>
<span class="sd">    &quot;collect&quot; method of spark before returning.  It then always returns </span>
<span class="sd">    a list of ObjectIds of  live, saved data.   The function is dogmatic </span>
<span class="sd">    about that because the return can never be a bag/RDD of the the </span>
<span class="sd">    data.</span>

<span class="sd">    If your workflow requires an intermediate save (i.e. saving data </span>
<span class="sd">    in an intermediate step within a chain of map/reduce opeators)</span>
<span class="sd">    the best approach at present is to use the `save_data` method of </span>
<span class="sd">    the `Database` class in a variant of the following (dask) example</span>
<span class="sd">    that also illustrates how this function is used as the terminator </span>
<span class="sd">    of a chain of map-reduce operators.</span>

<span class="sd">    ```</span>
<span class="sd">       mybag = read_distributed_data(db,collection=&#39;wf_TimeSeries&#39;)</span>
<span class="sd">       mybag = mybag.map(detrend)   # example</span>
<span class="sd">       # intermediate save</span>
<span class="sd">       mybag = mybag.map(db.save_data,collection=&quot;wf_TimeSeries&quot;)</span>
<span class="sd">       # more processing - trivial example</span>
<span class="sd">       mybag = mybag.map(filter,&#39;lowpass&#39;,freq=1.0)</span>
<span class="sd">       # termination with this function</span>
<span class="sd">       wfids = write_distributed_data(mybag,db,collection=&#39;wf_TimeSeries&#39;)</span>
<span class="sd">    ```   </span>

<span class="sd">    The `storage_mode` argument is a constant that defines how the </span>
<span class="sd">    SAMPLE DATA are to be stored.  Currently this can be &quot;file&quot; or </span>
<span class="sd">    &quot;gridfs&quot;, but be aware future evolution may extend the options.  </span>
<span class="sd">    &quot;gridfs&quot; is the default as the only complexity it has is a speed </span>
<span class="sd">    throttle by requiring the sample data to move through MongoDB and </span>
<span class="sd">    the potential to overflow the file system where the database is stored. </span>
<span class="sd">    (See User&#39;s Manual for more on this topic.).   Most users, however, </span>
<span class="sd">    likely want to use the &quot;file&quot; option for that parameter.  There are, </span>
<span class="sd">    however, some caveats in that use that users MUST be aware of before</span>
<span class="sd">    using that option with large data sets.   Since V2 of MsPASS </span>
<span class="sd">    the file save process was made more robust by allowing a chain of </span>
<span class="sd">    options for how the actual file name where data is stored is set.  </span>
<span class="sd">    The algorithm used here is a private method in </span>
<span class="sd">    :class:`mspasspy.db.Database` called `_save_sample_data_to_file`. </span>
<span class="sd">    When used here that that function is passed a None type for dir and </span>
<span class="sd">    dfile.   The EXPECTED use is that you as a user should set the </span>
<span class="sd">    dir and dfile attribute for EVERY datum in the bag/RDD this function is </span>
<span class="sd">    asked to handle.  That allows each atomic datum to define what the </span>
<span class="sd">    file destination is.  For ensembles normal behavior is to require the </span>
<span class="sd">    entire ensemble content to be saved in one file defined by the dir </span>
<span class="sd">    and dfile values in the ensemble&#39;s Metadata container.  </span>
<span class="sd">    THE WARNING is that to be robust the file writer will alway default </span>
<span class="sd">    a value for dir and dfile.  The default dir is the run directory. </span>
<span class="sd">    The default dfile (if not set) is a unique name created by a </span>
<span class="sd">    uuid generator.  Care must be taken in file writes to make sure </span>
<span class="sd">    you don&#39;t create huge numbers of files that overflow directories or </span>
<span class="sd">    similar file system errors that are all to easy to do with large </span>
<span class="sd">    data set saves.  See the User Manual for examples of how to set </span>
<span class="sd">    output file names for a large data set.</span>

<span class="sd">    :param data: parallel container of data to be written</span>
<span class="sd">    :type data: :class:`dask.bag.Bag` or :class:`pyspark.RDD`.</span>

<span class="sd">    :param db: database handle to manage data saved by this function.</span>
<span class="sd">    :type db: :class:`mspasspy.db.database.Database`.</span>

<span class="sd">    :param mode: This parameter defines how attributes defined with</span>
<span class="sd">        key-value pairs in MongoDB documents are to be handled for writes.</span>
<span class="sd">        By &quot;to be handled&quot; we mean how strongly to enforce name and type</span>
<span class="sd">        specification in the schema for the type of object being constructed.</span>
<span class="sd">        Options are [&#39;promiscuous&#39;,&#39;cautious&#39;,&#39;pedantic&#39;] with &#39;promiscuous&#39;</span>
<span class="sd">        being the default.  See the User&#39;s manual for more details on</span>
<span class="sd">        the concepts and how to use this option.</span>
<span class="sd">    :type mode: :class:`str`</span>

<span class="sd">    :param storage_mode: Must be either &quot;gridfs&quot; or &quot;file.  When set to</span>
<span class="sd">        &quot;gridfs&quot; the waveform data are stored internally and managed by</span>
<span class="sd">        MongoDB.  If set to &quot;file&quot; the data will be stored in a file system.</span>
<span class="sd">        File names are derived from attributes with the tags &quot;dir&quot; and </span>
<span class="sd">        &quot;dfile&quot; in the standard way.   Any datum for which dir or dfile </span>
<span class="sd">        aren&#39;t defined will default to the behaviour of the Database </span>
<span class="sd">        class method `save_data`.  See the docstring for details but the </span>
<span class="sd">        concept is it will always be bombproof even if not ideal.</span>
<span class="sd">    :type storage_mode: :class:`str` </span>

<span class="sd">    :param scheduler:  name of parallel scheduler being used by this writer. </span>
<span class="sd">      MsPASS currently support pyspark and dask.  If arg0 is an RDD </span>
<span class="sd">      scheduler must be &quot;spark&quot; and arg0 defines dask bag schduler must </span>
<span class="sd">      be &quot;dask&quot;.   The function will raise a ValueError exception of </span>
<span class="sd">      scheduler and the type of arg0 are not consistent or if the </span>
<span class="sd">      value of scheduler is illegal.  Note with spark the context is </span>
<span class="sd">      not required because of how this algorithm is structured.</span>
<span class="sd">    :type scheduler:  string  Must be either &quot;dask&quot; or &quot;spark&quot;.  Default </span>
<span class="sd">      is None which is is equivalent to the value of &quot;dask&quot;.</span>

<span class="sd">    :param file_format: the format of the file. This can be one of the</span>
<span class="sd">        `supported formats &lt;https://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.write.html#supported-formats&gt;`__</span>
<span class="sd">        of ObsPy writer. The default the python None which the method</span>
<span class="sd">        assumes means to store the data in its raw binary form.  The default</span>
<span class="sd">        should normally be used for efficiency.  Alternate formats are</span>
<span class="sd">        primarily a simple export mechanism.  See the User&#39;s manual for</span>
<span class="sd">        more details on data export.  Used only for &quot;file&quot; storage mode.</span>
<span class="sd">    :type file_format: :class:`str`</span>

<span class="sd">    :param overwrite:  If true gridfs data linked to the original</span>
<span class="sd">        waveform will be replaced by the sample data from this save.</span>
<span class="sd">        Default is false, and should be the normal use.  This option</span>
<span class="sd">        should never be used after a reduce operator as the parents</span>
<span class="sd">        are not tracked and the space advantage is likely minimal for</span>
<span class="sd">        the confusion it would cause.   This is most useful for light, stable</span>
<span class="sd">        preprocessing with a set of map operators to regularize a data</span>
<span class="sd">        set before more extensive processing.  It can only be used when</span>
<span class="sd">        storage_mode is set to gridfs.</span>
<span class="sd">    :type overwrite:  boolean</span>

<span class="sd">    :param collectiion:   name of wf collection where the documents </span>
<span class="sd">       derived from the data are to be saved.  Standard values are </span>
<span class="sd">       &quot;wf_TimeSeries&quot; and &quot;wf_Seismogram&quot; for which a schema is defined in </span>
<span class="sd">       MsPASS.   Normal use should specify one or the other.   The default is </span>
<span class="sd">       &quot;wf_TimeSeries&quot;  but normal usage should specify this argument </span>
<span class="sd">       explicitly for clarity in reuse. </span>
<span class="sd">    :type collection:  :class:`str`</span>

<span class="sd">    :param exclude_keys: Metadata can often become contaminated with</span>
<span class="sd">        attributes that are no longer needed or a mismatch with the data.</span>
<span class="sd">        A type example is the bundle algorithm takes three TimeSeries</span>
<span class="sd">        objects and produces a single Seismogram from them.  That process</span>
<span class="sd">        can, and usually does, leave things like seed channel names and</span>
<span class="sd">        orientation attributes (hang and vang) from one of the components</span>
<span class="sd">        as extraneous baggage.   Use this of keys to prevent such attributes</span>
<span class="sd">        from being written to the output documents.  Not if the data being</span>
<span class="sd">        saved lack these keys nothing happens so it is safer, albeit slower,</span>
<span class="sd">        to have the list be as large as necessary to eliminate any potential</span>
<span class="sd">        debris.</span>
<span class="sd">    :type exclude_keys: a :class:`list` of :class:`str`</span>

<span class="sd">    :param data_tag: a user specified &quot;data_tag&quot; key.  See above and</span>
<span class="sd">        User&#39;s manual for guidance on how the use of this option.</span>
<span class="sd">    :type data_tag: :class:`str`</span>

<span class="sd">    :param post_elog:   boolean controlling how error log messages are </span>
<span class="sd">       handled.  When False (default) error log messages get posted in </span>
<span class="sd">       single transactions with MongoDB to the &quot;elog&quot; collection.   </span>
<span class="sd">       When set True error log entries will be posted to as subdocuments to </span>
<span class="sd">       the wf collection entry for each datum.   Setting post_elog True </span>
<span class="sd">       is most useful if you anticipate a run will generate a large number of </span>
<span class="sd">       error that could throttle processing with a large number of </span>
<span class="sd">       one-at-a-time document saves.  For normal use with small number of </span>
<span class="sd">       errors it is easier to review error issue by inspecting the elog</span>
<span class="sd">       collection than having to query the larger wf collection.</span>

<span class="sd">     :param save_history:  When set True (default is False) write will </span>
<span class="sd">       save any object-level history data saved within the input data objects.</span>
<span class="sd">       The related boolean (described below) called post_history controls </span>
<span class="sd">       how such data is saved if this option is enable.  Note post_history </span>
<span class="sd">       is ignored unless save_history is True.</span>

<span class="sd">     :param post_history:  boolean similar to post_elog for handling </span>
<span class="sd">       object-level history data.  It is, however, only handled if the </span>
<span class="sd">       related boolean &quot;save_history&quot; is set True. When post_history is </span>
<span class="sd">       set True the history data will be saved as a subdocument in the wf</span>
<span class="sd">       document saved for each live, atomic datum (note for ensembles </span>
<span class="sd">       that means all live members).   When False each atomic datum </span>
<span class="sd">       will generate a insert_one transaction with MongoDB and save the </span>
<span class="sd">       history data in  the &quot;history&quot; collection.  It then sets the </span>
<span class="sd">       attribute with key &quot;history_id&quot; to the ObjectId of the saved </span>
<span class="sd">       document.  The default for this argument is True to avoid </span>
<span class="sd">       accidentally throttling workflows on large data sets.  The default </span>
<span class="sd">       for save_history is False so overall default behavior is to drop </span>
<span class="sd">       any history data. </span>

<span class="sd">     :param cremate:  boolean controlling handling of dead data.  </span>
<span class="sd">       When True dead data will be passed to the `cremate` </span>
<span class="sd">       method of :class:`mspasspy.util.Undertaker` which leaves only </span>
<span class="sd">       ashes to nothing in the return.   When False (default) the </span>
<span class="sd">       `bury` method will be called instead which saves a skeleton </span>
<span class="sd">       (error log and Metadata content) of the results in the &quot;cemetery&quot; </span>
<span class="sd">       collection.</span>

<span class="sd">     :param normalizing_collections:  list of collection names dogmatically treated</span>
<span class="sd">          as normalizing collection names.  The keywords in the list are used </span>
<span class="sd">          to always (i.e. for all modes) erase any attribute with a key name </span>
<span class="sd">          of the form `collection_attribute where `collection` is one of the collection </span>
<span class="sd">          names in this list and attribute is any string.  Attribute names with the &quot;_&quot; </span>
<span class="sd">          separator are saved unless the collection field matches one one of the </span>
<span class="sd">          strings (e.g. &quot;channel_vang&quot; will be erased before saving to the </span>
<span class="sd">          wf collection while &quot;foo_bar&quot; will not be erased.)  This list should </span>
<span class="sd">          ONLY be changed if a different schema than the default mspass schema </span>
<span class="sd">          is used and different names are used for normalizing collections.  </span>
<span class="sd">          (e.g. if one added a &quot;shot&quot; collection to the schema the list would need </span>
<span class="sd">          to be changed to at least add &quot;shot&quot;.)</span>
<span class="sd">     :type normalizing_collection:  list if strings defining collection names.</span>

<span class="sd">     :param alg_name:  do not change</span>
<span class="sd">     :param alg_id:  algorithm id for object-level history.  Normally</span>
<span class="sd">       assigned by global history manager.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We don&#39;t do type check on the data argument assuming dask or</span>
    <span class="c1"># spark will throw errors that make the mistake clear.</span>
    <span class="c1"># Too awkward to use an isinstance test so for now at least we don&#39;t</span>
    <span class="c1"># test the type of data</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">Database</span><span class="p">):</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;write_distributed_data:  required arg1 (db) must be an instance of mspasspy.db.Database</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Type of arg1 received is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">db</span><span class="p">)))</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">storage_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;file&quot;</span><span class="p">,</span> <span class="s2">&quot;gridfs&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;write_distributed_data:  Unsupported storage_mode=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">storage_mode</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;promiscuous&quot;</span><span class="p">,</span> <span class="s2">&quot;cautious&quot;</span><span class="p">,</span> <span class="s2">&quot;pedantic&quot;</span><span class="p">]:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;write_distributed_data:  Illegal value of mode=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="p">(</span>
            <span class="s2">&quot;Must be one one of the following:  promiscuous, cautious, or pedantic&quot;</span>
        <span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scheduler</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;dask&quot;</span><span class="p">,</span> <span class="s2">&quot;spark&quot;</span><span class="p">]:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;write_distributed_data:  Illegal value of scheduler=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">scheduler</span>
            <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Must be either dask or spark&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="s2">&quot;dask&quot;</span>
    <span class="c1"># This use of the collection name to establish the schema is</span>
    <span class="c1"># a bit fragile as it depends upon the mspass schema naming</span>
    <span class="c1"># convention.  Once tried using take(1) and probing the content of</span>
    <span class="c1"># the container but that has bad memory consequences at least for pyspark</span>
    <span class="k">if</span> <span class="n">collection</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">collection</span> <span class="o">==</span> <span class="s2">&quot;wf_TimeSeries&quot;</span><span class="p">:</span>
        <span class="n">save_schema</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">metadata_schema</span><span class="o">.</span><span class="n">TimeSeries</span>
    <span class="k">elif</span> <span class="n">collection</span> <span class="o">==</span> <span class="s2">&quot;wf_Seismogram&quot;</span><span class="p">:</span>
        <span class="n">save_schema</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">metadata_schema</span><span class="o">.</span><span class="n">Seismogram</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;write_distributed_data:   Illegal value of collection=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">collection</span>
        <span class="p">)</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;Currently must be either wf_TimeSeries, wf_Seismogram, or default that implies wf_TimeSeries&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">overwrite</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">storage_mode</span> <span class="o">!=</span> <span class="s2">&quot;gridfs&quot;</span><span class="p">:</span>
            <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;write_distributed_data:  overwrite mode is set True with storage_mode=</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">storage_mode</span>
            <span class="p">)</span>
            <span class="n">message</span> <span class="o">+=</span> <span class="s2">&quot;overwrite is only allowed with gridfs storage_mode&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

    <span class="n">stedronsky</span> <span class="o">=</span> <span class="n">Undertaker</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">scheduler</span> <span class="o">==</span> <span class="s2">&quot;spark&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">data_are_atomic</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_sample_data</span><span class="p">(</span>
                    <span class="n">d</span><span class="p">,</span>
                    <span class="n">storage_mode</span><span class="o">=</span><span class="n">storage_mode</span><span class="p">,</span>
                    <span class="nb">dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dfile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="nb">format</span><span class="o">=</span><span class="n">file_format</span><span class="p">,</span>
                    <span class="n">overwrite</span><span class="o">=</span><span class="n">overwrite</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">pyspark_interface</span> <span class="o">=</span> <span class="n">pyspark_mappartition_interface</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">collection</span><span class="p">)</span>
            <span class="c1"># With atomic data dead in this implementation we handle</span>
            <span class="c1"># any dead datum with the map operators that saves the</span>
            <span class="c1"># wf documents.   Dead data return a None instead of an id</span>
            <span class="c1"># by default and leave a body in the cemetery collection</span>
            <span class="c1"># unless cremate is set true</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">_atomic_extract_wf_document</span><span class="p">(</span>
                    <span class="n">d</span><span class="p">,</span>
                    <span class="n">db</span><span class="p">,</span>
                    <span class="n">save_schema</span><span class="p">,</span>
                    <span class="n">exclude_keys</span><span class="p">,</span>
                    <span class="n">mode</span><span class="p">,</span>
                    <span class="n">normalizing_collections</span><span class="p">,</span>
                    <span class="n">post_elog</span><span class="o">=</span><span class="n">post_elog</span><span class="p">,</span>
                    <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">,</span>
                    <span class="n">post_history</span><span class="o">=</span><span class="n">post_history</span><span class="p">,</span>
                    <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                    <span class="n">undertaker</span><span class="o">=</span><span class="n">stedronsky</span><span class="p">,</span>
                    <span class="n">cremate</span><span class="o">=</span><span class="n">cremate</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">pyspark_interface</span><span class="o">.</span><span class="n">partitioned_save_wfdoc</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># This step adds some minor overhead, but it can reduce</span>
            <span class="c1"># memory use at a small cost.  Ensembles are particularly</span>
            <span class="c1"># prone to memory problems so for now view this as worth doing</span>
            <span class="c1"># Note _save_ensemble_wfdocs is assumed to handle the bodies</span>
            <span class="c1"># cleanly when cremate is False (note when true dead members</span>
            <span class="c1"># are vaporized with no trace).  The default runs bury which</span>
            <span class="c1"># will create some overhead if there are a lot of bodies to handle</span>
            <span class="c1"># in both cases the ensemble has the bodies removed by the undertaker</span>
            <span class="k">if</span> <span class="n">cremate</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">stedronsky</span><span class="o">.</span><span class="n">cremate</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">stedronsky</span><span class="o">.</span><span class="n">bury</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">))</span>
            <span class="c1"># Note with ensembles we delay saving sample data until the undertaker</span>
            <span class="c1"># has taken care of the dead - different from atomic data.</span>
            <span class="c1"># Works for atomic data data as it does nothing if the datum is</span>
            <span class="c1"># marked dead.  Similarly if an entire ensemble is marked dead this</span>
            <span class="c1"># will do nothing.</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">db</span><span class="o">.</span><span class="n">_save_sample_data</span><span class="p">(</span>
                    <span class="n">d</span><span class="p">,</span>
                    <span class="n">storage_mode</span><span class="o">=</span><span class="n">storage_mode</span><span class="p">,</span>
                    <span class="nb">dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">dfile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="nb">format</span><span class="o">=</span><span class="n">file_format</span><span class="p">,</span>
                    <span class="n">overwrite</span><span class="o">=</span><span class="n">overwrite</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="n">_save_ensemble_wfdocs</span><span class="p">(</span>
                    <span class="n">d</span><span class="p">,</span>
                    <span class="n">db</span><span class="p">,</span>
                    <span class="n">save_schema</span><span class="p">,</span>
                    <span class="n">exclude_keys</span><span class="p">,</span>
                    <span class="n">mode</span><span class="p">,</span>
                    <span class="n">stedronsky</span><span class="p">,</span>
                    <span class="n">normalizing_collections</span><span class="p">,</span>
                    <span class="n">cremate</span><span class="o">=</span><span class="n">cremate</span><span class="p">,</span>
                    <span class="n">post_elog</span><span class="o">=</span><span class="n">post_elog</span><span class="p">,</span>
                    <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">,</span>
                    <span class="n">post_history</span><span class="o">=</span><span class="n">post_history</span><span class="p">,</span>
                    <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">data_are_atomic</span><span class="p">:</span>
            <span class="c1"># See comment at top of spark section  - this code is exactly</span>
            <span class="c1"># the same by in the dask dialect</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">db</span><span class="o">.</span><span class="n">_save_sample_data</span><span class="p">,</span>
                <span class="n">storage_mode</span><span class="o">=</span><span class="n">storage_mode</span><span class="p">,</span>
                <span class="nb">dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">dfile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="nb">format</span><span class="o">=</span><span class="n">file_format</span><span class="p">,</span>
                <span class="n">overwrite</span><span class="o">=</span><span class="n">overwrite</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">_atomic_extract_wf_document</span><span class="p">,</span>
                <span class="n">db</span><span class="p">,</span>
                <span class="n">save_schema</span><span class="p">,</span>
                <span class="n">exclude_keys</span><span class="p">,</span>
                <span class="n">mode</span><span class="p">,</span>
                <span class="n">normalizing_collections</span><span class="p">,</span>
                <span class="n">post_elog</span><span class="o">=</span><span class="n">post_elog</span><span class="p">,</span>
                <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">,</span>
                <span class="n">post_history</span><span class="o">=</span><span class="n">post_history</span><span class="p">,</span>
                <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
                <span class="n">undertaker</span><span class="o">=</span><span class="n">stedronsky</span><span class="p">,</span>
                <span class="n">cremate</span><span class="o">=</span><span class="n">cremate</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map_partitions</span><span class="p">(</span>
                <span class="n">_partitioned_save_wfdoc</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">collection</span><span class="o">=</span><span class="n">collection</span>
            <span class="p">)</span>
            <span class="c1"># necessary here or the map_partition function will fail</span>
            <span class="c1"># because it will receive a DAG structure instead of a list</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># This step adds some minor overhead, but it can reduce</span>
            <span class="c1"># memory use at a small cost.  Ensembles are particularly</span>
            <span class="c1"># prone to memory problems so for now view this as worth doing</span>
            <span class="c1"># Note _save_ensemble_wfdocs is assumed to handle the bodies</span>
            <span class="c1"># cleanly when cremate is False (note when true dead members</span>
            <span class="c1"># are vaporized with no trace)  Default is bury which will</span>
            <span class="c1"># cause some overhead if the number of bodies is large.</span>
            <span class="c1"># in both cases the ensemble has the bodies removed by the undertaker</span>
            <span class="k">if</span> <span class="n">cremate</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">stedronsky</span><span class="o">.</span><span class="n">cremate</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">stedronsky</span><span class="o">.</span><span class="n">bury</span><span class="p">,</span> <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">)</span>
            <span class="c1"># important comment about this next line in the spark section</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">db</span><span class="o">.</span><span class="n">_save_sample_data</span><span class="p">,</span>
                <span class="n">storage_mode</span><span class="o">=</span><span class="n">storage_mode</span><span class="p">,</span>
                <span class="nb">dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">dfile</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="nb">format</span><span class="o">=</span><span class="n">file_format</span><span class="p">,</span>
                <span class="n">overwrite</span><span class="o">=</span><span class="n">overwrite</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">_save_ensemble_wfdocs</span><span class="p">,</span>
                <span class="n">db</span><span class="p">,</span>
                <span class="n">save_schema</span><span class="p">,</span>
                <span class="n">exclude_keys</span><span class="p">,</span>
                <span class="n">mode</span><span class="p">,</span>
                <span class="n">stedronsky</span><span class="p">,</span>
                <span class="n">normalizing_collections</span><span class="p">,</span>
                <span class="n">cremate</span><span class="o">=</span><span class="n">cremate</span><span class="p">,</span>
                <span class="n">post_elog</span><span class="o">=</span><span class="n">post_elog</span><span class="p">,</span>
                <span class="n">save_history</span><span class="o">=</span><span class="n">save_history</span><span class="p">,</span>
                <span class="n">post_history</span><span class="o">=</span><span class="n">post_history</span><span class="p">,</span>
                <span class="n">data_tag</span><span class="o">=</span><span class="n">data_tag</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">data</span></div>


<div class="viewcode-block" id="read_to_dataframe"><a class="viewcode-back" href="../../../python_api/mspasspy.io.html#mspasspy.io.distributed.read_to_dataframe">[docs]</a><span class="k">def</span> <span class="nf">read_to_dataframe</span><span class="p">(</span>
    <span class="n">db</span><span class="p">,</span>
    <span class="n">cursor</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;promiscuous&quot;</span><span class="p">,</span>
    <span class="n">normalize</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">load_history</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">exclude_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">data_tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">alg_name</span><span class="o">=</span><span class="s2">&quot;read_to_dataframe&quot;</span><span class="p">,</span>
    <span class="n">alg_id</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
    <span class="n">define_as_raw</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">retrieve_history_record</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Read the documents defined by a MongoDB cursor into a panda DataFrame.</span>

<span class="sd">    The data stucture called a panda DataFrame is heavily used by</span>
<span class="sd">    many python user&#39;s.  This is convenience function for users wanting to</span>
<span class="sd">    use that api to do pure metadata operations.</span>

<span class="sd">    Be warned this function originated as a prototype where we experimented</span>
<span class="sd">    with using a dask or pyspark DataFrame as an intermediatry for parallel</span>
<span class="sd">    readers.   We developed an alternative algorithm that made the</span>
<span class="sd">    baggage of the intermediary unnecessary.   The warning is the</span>
<span class="sd">    function is not mainstream and may be prone to issues.</span>

<span class="sd">    :param db: the database from which the data are to be read.</span>
<span class="sd">    :type db: :class:`mspasspy.db.database.Database`.</span>
<span class="sd">    :param object_id: MongoDB object id of the wf document to be constructed from</span>
<span class="sd">        data defined in the database.  The object id is guaranteed unique and provides</span>
<span class="sd">        a unique link to a unique document or nothing.   In the later case the</span>
<span class="sd">        function will return a None.</span>
<span class="sd">    :type cursor: :class:`pymongo.cursor.CursorType`</span>
<span class="sd">    :param mode: reading mode that controls how the function interacts with</span>
<span class="sd">        the schema definition for the data type.   Must be one of</span>
<span class="sd">        [&#39;promiscuous&#39;,&#39;cautious&#39;,&#39;pedantic&#39;].   See user&#39;s manual for a</span>
<span class="sd">        detailed description of what the modes mean.  Default is &#39;promiscuous&#39;</span>
<span class="sd">        which turns off all schema checks and loads all attributes defined for</span>
<span class="sd">        each object read.</span>
<span class="sd">    :type mode: :class:`str`</span>
<span class="sd">    :param normalize: list of collections that are to used for data</span>
<span class="sd">        normalization. (see User&#39;s manual and MongoDB documentation for</span>
<span class="sd">        details on this concept)  Briefly normalization means common</span>
<span class="sd">        metadata like source and receiver geometry are defined in separate</span>
<span class="sd">        smaller collections that are linked through this mechanism</span>
<span class="sd">        during reads. Default uses no normalization.</span>
<span class="sd">    :type normalize: a :class:`list` of :class:`str`</span>
<span class="sd">    :param load_history: boolean (True or False) switch used to enable or</span>
<span class="sd">        disable object level history mechanism.   When set True each datum</span>
<span class="sd">        will be tagged with its origin id that defines the leaf nodes of a</span>
<span class="sd">        history G-tree.  See the User&#39;s manual for additional details of this</span>
<span class="sd">        feature.  Default is False.</span>
<span class="sd">    :param exclude_keys: Sometimes it is helpful to remove one or more</span>
<span class="sd">        attributes stored in the database from the data&#39;s Metadata (header)</span>
<span class="sd">        so they will not cause problems in downstream processing.</span>
<span class="sd">    :type exclude_keys: a :class:`list` of :class:`str`</span>
<span class="sd">    :param collection:  Specify an alternate collection name to</span>
<span class="sd">        use for reading the data.  The default sets the collection name</span>
<span class="sd">        based on the data type and automatically loads the correct schema.</span>
<span class="sd">        The collection listed must be defined in the schema and satisfy</span>
<span class="sd">        the expectations of the reader.  This is an advanced option that</span>
<span class="sd">        is indended only to simplify extensions to the reader.</span>
<span class="sd">    :param data_tag:  The definition of a dataset can become ambiguous</span>
<span class="sd">        when partially processed data are saved within a workflow.   A common</span>
<span class="sd">        example would be windowing long time blocks of data to shorter time</span>
<span class="sd">        windows around a particular seismic phase and saving the windowed data.</span>
<span class="sd">        The windowed data can be difficult to distinguish from the original</span>
<span class="sd">        with standard queries.  For this reason we make extensive use of &quot;tags&quot;</span>
<span class="sd">        for save and read operations to improve the efficiency and simplify</span>
<span class="sd">        read operations.   Default turns this off by setting the tag null (None).</span>
<span class="sd">    :type data_tag: :class:`str`</span>
<span class="sd">    :param alg_name: alg_name is the name the func we are gonna save while preserving the history.</span>
<span class="sd">    :type alg_name: :class:`str`</span>
<span class="sd">    :param alg_id: alg_id is a unique id to record the usage of func while preserving the history.</span>
<span class="sd">    :type alg_id: :class:`bson.objectid.ObjectId`</span>
<span class="sd">    :param define_as_raw: a boolean control whether we would like to set_as_origin when loading processing history</span>
<span class="sd">    :type define_as_raw: :class:`bool`</span>
<span class="sd">    :param retrieve_history_record: a boolean control whether we would like to load processing history</span>
<span class="sd">    :type retrieve_history_record: :class:`bool`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">cursor</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">name</span>
    <span class="n">dbschema</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">database_schema</span>
    <span class="n">mdschema</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">metadata_schema</span>
    <span class="n">this_elog</span> <span class="o">=</span> <span class="n">ErrorLogger</span><span class="p">()</span>
    <span class="n">md_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
        <span class="c1"># Use the databhase module function doc2md that standardizes</span>
        <span class="c1"># handling of schema constraints and exlcude_keys</span>

        <span class="n">md</span><span class="p">,</span> <span class="n">aok</span><span class="p">,</span> <span class="n">elog</span> <span class="o">=</span> <span class="n">doc2md</span><span class="p">(</span>
            <span class="n">doc</span><span class="p">,</span>
            <span class="n">dbschema</span><span class="p">,</span>
            <span class="n">mdschema</span><span class="p">,</span>
            <span class="n">collection</span><span class="p">,</span>
            <span class="n">exclude_keys</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">aok</span><span class="p">:</span>
            <span class="n">md_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">md</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">this_elog</span> <span class="o">+=</span> <span class="n">elog</span>

    <span class="k">if</span> <span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;WARNING(read_to_dataframe): &quot;</span><span class="p">,</span>
            <span class="n">elog</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
            <span class="s2">&quot; errors were handled during dataframe construction&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;All data associated with these errors were dropped.   Error log entries from doc2md follow&quot;</span>
        <span class="p">)</span>
        <span class="n">errorlist</span> <span class="o">=</span> <span class="n">elog</span><span class="o">.</span><span class="n">get_error_log</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">errorlist</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">entry</span><span class="o">.</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">entry</span><span class="o">.</span><span class="n">message</span><span class="p">,</span> <span class="n">entry</span><span class="o">.</span><span class="n">badness</span><span class="p">)</span>
    <span class="c1"># convert the metadata list to a dataframe</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">json_normalize</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">cur</span><span class="p">:</span> <span class="n">cur</span><span class="o">.</span><span class="n">todict</span><span class="p">(),</span> <span class="n">md_list</span><span class="p">))</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2021, Ian Wang.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>